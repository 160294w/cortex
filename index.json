[{"body":" The blocks storage is an experimental Cortex storage engine based on Prometheus TSDB: it stores each tenant\u0026rsquo;s time series into their own TSDB which write out their series to a on-disk Block (defaults to 2h block range periods). Each Block is composed by chunk files - containing the timestamp-value pairs for multiple series - and an index, which indexes metric names and labels to time series in the chunk files.\nThe supported backends for the blocks storage are:\n Amazon S3 Google Cloud Storage Microsoft Azure Storage Local Filesystem (single node only) Internally, this storage engine is based on Thanos, but no Thanos knowledge is required in order to run it.\nThe rest of the document assumes you have read the Cortex architecture documentation.\nHow it works When the blocks storage is used, each ingester creates a per-tenant TSDB and ships the TSDB Blocks - which by default are cut every 2 hours - to the long-term storage.\nQueriers periodically iterate over the storage bucket to discover recently uploaded Blocks and - for each Block - download a subset of the block index - called \u0026ldquo;index header\u0026rdquo; - which is kept in memory and used to provide fast lookups.\nThe write path Ingesters receive incoming samples from the distributors. Each push request belongs to a tenant, and the ingester append the received samples to the specific per-tenant TSDB. The received samples are both kept in-memory and written to a write-ahead log (WAL) stored on the local disk and used to recover the in-memory series in case the ingester abruptly terminates. The per-tenant TSDB is lazily created in each ingester upon the first push request is received for that tenant.\nThe in-memory samples are periodically flushed to disk - and the WAL truncated - when a new TSDB Block is cut, which by default occurs every 2 hours. Each new Block cut is then uploaded to the long-term storage and kept in the ingester for some more time, in order to give queriers enough time to discover the new Block from the storage and download its index header.\nIn order to effectively use the WAL and being able to recover the in-memory series upon ingester abruptly termination, the WAL needs to be stored to a persistent local disk which can survive in the event of an ingester failure (ie. AWS EBS volume or GCP persistent disk when running in the cloud). For example, if you\u0026rsquo;re running the Cortex cluster in Kubernetes, you may use a StatefulSet with a persistent volume claim for the ingesters.\nThe read path Queriers - at startup - iterate over the entire storage bucket to discover all tenants Blocks and - for each of them - download the index header. During this initial synchronization phase, a querier is not ready to handle incoming queries yet and its /ready readiness probe endpoint will fail.\nQueriers also periodically re-iterate over the storage bucket to discover newly uploaded Blocks (by the ingesters) and find out Blocks deleted in the meanwhile, as effect of an optional retention policy.\nThe blocks chunks and the entire index is never fully downloaded by the queriers. In the read path, a querier lookups the series label names and values using the in-memory index header and then download the required segments of the index and chunks for the matching series directly from the long-term storage using byte-range requests.\nThe index header is also stored to the local disk, in order to avoid to re-download it on subsequent restarts of a querier. For this reason, it\u0026rsquo;s recommended - but not required - to run the querier with a persistent local disk. For example, if you\u0026rsquo;re running the Cortex cluster in Kubernetes, you may use a StatefulSet with a persistent volume claim for the queriers.\nSeries sharding and replication The series sharding and replication doesn\u0026rsquo;t change based on the storage engine, so the general overview provided by the \u0026ldquo;Cortex architecture\u0026rdquo; documentation applies to the blocks storage as well.\nIt\u0026rsquo;s important to note that - differently than the chunks storage - time series are effectively written N times to the long-term storage, where N is the replication factor (typically 3). This may lead to a storage utilization N times more than the chunks storage, but is actually mitigated by the compactor service (see \u0026ldquo;vertical compaction\u0026rdquo;).\nCompactor The compactor is an optional - but highly recommended - service which compacts multiple Blocks of a given tenant into a single optimized larger Block. The compactor has two main benefits:\n Vertically compact Blocks uploaded by all ingesters for the same time range Horizontally compact Blocks with small time ranges into a single larger Block The vertical compaction compacts all the Blocks of a tenant uploaded by any ingester for the same Block range period (defaults to 2 hours) into a single Block, de-duplicating samples that are originally written to N Blocks as effect of the replication.\nThe horizontal compaction triggers after the vertical compaction and compacts several Blocks belonging to adjacent small range periods (2 hours) into a single larger Block. Despite the total block chunks size doesn\u0026rsquo;t change after this compaction, it may have a significative impact on the reduction of the index size and its index header kept in memory by queriers.\nThe compactor is stateless.\nCompactor sharding The compactor optionally supports sharding. When sharding is enabled, multiple compactor instances can coordinate to split the workload and shard blocks by tenant. All the blocks of a tenant are processed by a single compactor instance at any given time, but compaction for different tenants may simultaneously run on different compactor instances.\nWhenever the pool of compactors increase or decrease (ie. following up a scale up/down), tenants are resharded across the available compactor instances without any manual intervention. Compactors coordinate via the Cortex hash ring.\nCompactor HTTP endpoints GET /compactor_ring\nDisplays the status of the compactors ring, including the tokens owned by each compactor and an option to remove (forget) instances from the ring. Configuration The general configuration documentation also applied to a Cortex cluster running the blocks storage, with few differences:\n storage_config tsdb_config compactor_config storage_config The storage_config configures the storage engine.\nstorage:# The storage engine to use. Use \u0026#34;tsdb\u0026#34; for the blocks storage.# CLI flag: -store.engineengine:tsdb tsdb_config The tsdb_config configures the experimental blocks storage.\ntsdb:# Local directory to store TSDBs in the ingesters.# CLI flag: -experimental.tsdb.dir[dir:\u0026lt;string\u0026gt;|default=\u0026#34;tsdb\u0026#34;]# TSDB blocks range period.# CLI flag: -experimental.tsdb.block-ranges-period[block_ranges_period:\u0026lt;listofduration\u0026gt;|default=2h0m0s]# TSDB blocks retention in the ingester before a block is removed. This should# be larger than the block_ranges_period and large enough to give queriers# enough time to discover newly uploaded blocks.# CLI flag: -experimental.tsdb.retention-period[retention_period:\u0026lt;duration\u0026gt;|default=6h0m0s]# How frequently the TSDB blocks are scanned and new ones are shipped to the# storage. 0 means shipping is disabled.# CLI flag: -experimental.tsdb.ship-interval[ship_interval:\u0026lt;duration\u0026gt;|default=1m0s]# Maximum number of tenants concurrently shipping blocks to the storage.# CLI flag: -experimental.tsdb.ship-concurrency[ship_concurrency:\u0026lt;int\u0026gt;|default=10]# Backend storage to use. Supported backends are: s3, gcs, azure, filesystem.# CLI flag: -experimental.tsdb.backend[backend:\u0026lt;string\u0026gt;|default=\u0026#34;s3\u0026#34;]bucket_store:# Directory to store synchronized TSDB index headers.# CLI flag: -experimental.tsdb.bucket-store.sync-dir[sync_dir:\u0026lt;string\u0026gt;|default=\u0026#34;tsdb-sync\u0026#34;]# How frequently scan the bucket to look for changes (new blocks shipped by# ingesters and blocks removed by retention or compaction). 0 disables it.# CLI flag: -experimental.tsdb.bucket-store.sync-interval[sync_interval:\u0026lt;duration\u0026gt;|default=5m0s]# Size in bytes of in-memory index cache used to speed up blocks index# lookups (shared between all tenants).# CLI flag: -experimental.tsdb.bucket-store.index-cache-size-bytes[index_cache_size_bytes:\u0026lt;int\u0026gt;|default=1073741824]# Max size - in bytes - of a per-tenant chunk pool, used to reduce memory# allocations.# CLI flag: -experimental.tsdb.bucket-store.max-chunk-pool-bytes[max_chunk_pool_bytes:\u0026lt;int\u0026gt;|default=2147483648]# Max number of samples per query when loading series from the long-term# storage. 0 disables the limit.# CLI flag: -experimental.tsdb.bucket-store.max-sample-count[max_sample_count:\u0026lt;int\u0026gt;|default=0]# Max number of concurrent queries to execute against the long-term storage# on a per-tenant basis.# CLI flag: -experimental.tsdb.bucket-store.max-concurrent[max_concurrent:\u0026lt;int\u0026gt;|default=20]# Maximum number of concurrent tenants synching blocks.# CLI flag: -experimental.tsdb.bucket-store.tenant-sync-concurrency[tenant_sync_concurrency:\u0026lt;int\u0026gt;|default=10]# Maximum number of concurrent blocks synching per tenant.# CLI flag: -experimental.tsdb.bucket-store.block-sync-concurrency[block_sync_concurrency:\u0026lt;int\u0026gt;|default=20]# Number of Go routines to use when syncing block meta files from object# storage per tenant.# CLI flag: -experimental.tsdb.bucket-store.meta-sync-concurrency[meta_sync_concurrency:\u0026lt;int\u0026gt;|default=20]# Whether the bucket store should use the binary index header. If false, it# uses the JSON index header.# CLI flag: -experimental.tsdb.bucket-store.binary-index-header-enabled[binary_index_header_enabled:\u0026lt;boolean\u0026gt;|default=true]# Minimum age of a block before it\u0026#39;s being read. Set it to safe value (e.g# 30m) if your object storage is eventually consistent. GCS and S3 are# (roughly) strongly consistent.# CLI flag: -experimental.tsdb.bucket-store.consistency-delay[consistency_delay:\u0026lt;duration\u0026gt;|default=0s]# How frequently does Cortex try to compact TSDB head. Block is only created# if data covers smallest block range. Must be greater than 0 and max 5# minutes.# CLI flag: -experimental.tsdb.head-compaction-interval[head_compaction_interval:\u0026lt;duration\u0026gt;|default=1m0s]# Maximum number of tenants concurrently compacting TSDB head into a new block# CLI flag: -experimental.tsdb.head-compaction-concurrency[head_compaction_concurrency:\u0026lt;int\u0026gt;|default=5]# The number of shards of series to use in TSDB (must be a power of 2).# Reducing this will decrease memory footprint, but can negatively impact# performance.# CLI flag: -experimental.tsdb.stripe-size[stripe_size:\u0026lt;int\u0026gt;|default=16384]# limit the number of concurrently opening TSDB\u0026#39;s on startup# CLI flag: -experimental.tsdb.max-tsdb-opening-concurrency-on-startup[max_tsdb_opening_concurrency_on_startup:\u0026lt;int\u0026gt;|default=10]s3:# S3 endpoint without schema# CLI flag: -experimental.tsdb.s3.endpoint[endpoint:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# S3 bucket name# CLI flag: -experimental.tsdb.s3.bucket-name[bucket_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# S3 secret access key# CLI flag: -experimental.tsdb.s3.secret-access-key[secret_access_key:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# S3 access key ID# CLI flag: -experimental.tsdb.s3.access-key-id[access_key_id:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# If enabled, use http:// for the S3 endpoint instead of https://. This# could be useful in local dev/test environments while using an# S3-compatible backend storage, like Minio.# CLI flag: -experimental.tsdb.s3.insecure[insecure:\u0026lt;boolean\u0026gt;|default=false]gcs:# GCS bucket name# CLI flag: -experimental.tsdb.gcs.bucket-name[bucket_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# JSON representing either a Google Developers Console# client_credentials.json file or a Google Developers service account key# file. If empty, fallback to Google default logic.# CLI flag: -experimental.tsdb.gcs.service-account[service_account:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]azure:# Azure storage account name# CLI flag: -experimental.tsdb.azure.account-name[account_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Azure storage account key# CLI flag: -experimental.tsdb.azure.account-key[account_key:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Azure storage container name# CLI flag: -experimental.tsdb.azure.container-name[container_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Azure storage endpoint suffix without schema. The account name will be# prefixed to this value to create the FQDN# CLI flag: -experimental.tsdb.azure.endpoint-suffix[endpoint_suffix:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Number of retries for recoverable errors# CLI flag: -experimental.tsdb.azure.max-retries[max_retries:\u0026lt;int\u0026gt;|default=20]filesystem:# Local filesystem storage directory.# CLI flag: -experimental.tsdb.filesystem.dir[dir:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;] compactor_config The compactor_config configures the compactor for the experimental blocks storage.\ncompactor:# List of compaction time ranges.# CLI flag: -compactor.block-ranges[block_ranges:\u0026lt;listofduration\u0026gt;|default=2h0m0s,12h0m0s,24h0m0s]# Number of Go routines to use when syncing block index and chunks files from# the long term storage.# CLI flag: -compactor.block-sync-concurrency[block_sync_concurrency:\u0026lt;int\u0026gt;|default=20]# Number of Go routines to use when syncing block meta files from the long# term storage.# CLI flag: -compactor.meta-sync-concurrency[meta_sync_concurrency:\u0026lt;int\u0026gt;|default=20]# Minimum age of fresh (non-compacted) blocks before they are being processed.# Malformed blocks older than the maximum of consistency-delay and 48h0m0s# will be removed.# CLI flag: -compactor.consistency-delay[consistency_delay:\u0026lt;duration\u0026gt;|default=30m0s]# Data directory in which to cache blocks and process compactions# CLI flag: -compactor.data-dir[data_dir:\u0026lt;string\u0026gt;|default=\u0026#34;./data\u0026#34;]# The frequency at which the compaction runs# CLI flag: -compactor.compaction-interval[compaction_interval:\u0026lt;duration\u0026gt;|default=1h0m0s]# How many times to retry a failed compaction during a single compaction# interval# CLI flag: -compactor.compaction-retries[compaction_retries:\u0026lt;int\u0026gt;|default=3]# Shard tenants across multiple compactor instances. Sharding is required if# you run multiple compactor instances, in order to coordinate compactions and# avoid race conditions leading to the same tenant blocks simultaneously# compacted by different instances.# CLI flag: -compactor.sharding-enabled[sharding_enabled:\u0026lt;boolean\u0026gt;|default=false]sharding_ring:kvstore:# Backend storage to use for the ring. Supported values are: consul, etcd,# inmemory, multi, memberlist (experimental).# CLI flag: -compactor.ring.store[store:\u0026lt;string\u0026gt;|default=\u0026#34;consul\u0026#34;]# The prefix for the keys in the store. Should end with a /.# CLI flag: -compactor.ring.prefix[prefix:\u0026lt;string\u0026gt;|default=\u0026#34;collectors/\u0026#34;]# The consul_config configures the consul client.# The CLI flags prefix for this block config is: compactor.ring[consul:\u0026lt;consul_config\u0026gt;]# The etcd_config configures the etcd client.# The CLI flags prefix for this block config is: compactor.ring[etcd:\u0026lt;etcd_config\u0026gt;]multi:# Primary backend storage used by multi-client.# CLI flag: -compactor.ring.multi.primary[primary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Secondary backend storage used by multi-client.# CLI flag: -compactor.ring.multi.secondary[secondary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Mirror writes to secondary store.# CLI flag: -compactor.ring.multi.mirror-enabled[mirror_enabled:\u0026lt;boolean\u0026gt;|default=false]# Timeout for storing value to secondary store.# CLI flag: -compactor.ring.multi.mirror-timeout[mirror_timeout:\u0026lt;duration\u0026gt;|default=2s]# Period at which to heartbeat to the ring.# CLI flag: -compactor.ring.heartbeat-period[heartbeat_period:\u0026lt;duration\u0026gt;|default=5s]# The heartbeat timeout after which compactors are considered unhealthy# within the ring.# CLI flag: -compactor.ring.heartbeat-timeout[heartbeat_timeout:\u0026lt;duration\u0026gt;|default=1m0s] Known issues Migrating from the chunks to the blocks storage Currently, no smooth migration path is provided to migrate from chunks to blocks storage. For this reason, the blocks storage can only be enabled in new Cortex clusters.\n","excerpt":"The blocks storage is an experimental Cortex storage engine based on Prometheus TSDB: it stores each …","ref":"/docs/operations/blocks-storage/","title":"Blocks storage (experimental)"},{"body":" Cortex can be run as a single binary or as multiple independent microservices. The single-binary mode is easier to deploy and is aimed mainly at users wanting to try out Cortex or develop on it. The microservices mode is intended for production usage, as it allows you to independently scale different services and isolate failures. This document will focus on single-process Cortex. See the architecture doc For more information about the microservices.\nSeparately from single process vs microservices decision, Cortex can be configured to use local storage or cloud storage (DynamoDB, Bigtable, Cassandra, S3, GCS etc). This document will focus on using local storage. Local storage is explicitly not production ready at this time. Cortex can also make use of external memcacheds for caching and although these are not mandatory, they should be used in production.\nSingle instance, single process For simplicity and to get started, we\u0026rsquo;ll run it as a single process with no dependencies:\n$ go build ./cmd/cortex $ ./cortex -config.file=./docs/configuration/single-process-config.yaml This starts a single Cortex node storing chunks and index to your local filesystem in /tmp/cortex. It is not intended for production use.\nClone and build prometheus\n$ git clone https://github.com/prometheus/prometheus $ cd prometheus $ go build ./cmd/prometheus Add the following to your Prometheus config (documentation/examples/prometheus.yml in Prometheus repo):\nremote_write:-url:http://localhost:9009/api/prom/push And start Prometheus with that config file:\n$ ./prometheus --config.file=./documentation/examples/prometheus.yml Your Prometheus instance will now start pushing data to Cortex. To query that data, start a Grafana instance:\n$ docker run --rm -d --name=grafana -p 3000:3000 grafana/grafana In the Grafana UI (username/password admin/admin), add a Prometheus datasource for Cortex (http://host.docker.internal:9009/api/prom).\nTo clean up: press CTRL-C in both terminals (for Cortex and Promrtheus).\nHorizontally scale out Next we\u0026rsquo;re going to show how you can run a scale out Cortex cluster using Docker. We\u0026rsquo;ll need:\n A built Cortex image. A Docker network to put these containers on so they can resolve each other by name. A single node Consul instance to coordinate the Cortex cluster.\n$ make ./cmd/cortex/.uptodate $ docker network create cortex $ docker run -d --name=consul --network=cortex -e CONSUL_BIND_INTERFACE=eth0 consul Next we\u0026rsquo;ll run a couple of Cortex instances pointed at that Consul. You\u0026rsquo;ll note the Cortex configuration can be specified in either a config file or overridden on the command line. See the arguments documentation for more information about Cortex configuration options.\n$ docker run -d --name=cortex1 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config.yaml:/etc/single-process-config.yaml \\ -p 9001:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 $ docker run -d --name=cortex2 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config.yaml:/etc/single-process-config.yaml \\ -p 9002:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 If you go to http://localhost:9001/ring (or http://localhost:9002/ring) you should see both Cortex nodes join the ring.\nTo demonstrate the correct operation of Cortex clustering, we\u0026rsquo;ll send samples to one of the instances and queries to another. In production, you\u0026rsquo;d want to load balance both pushes and queries evenly among all the nodes.\nPoint Prometheus at the first:\nremote_write:-url:http://localhost:9001/api/prom/push$ ./prometheus --config.file=./documentation/examples/prometheus.yml And Grafana at the second:\n$ docker run -d --name=grafana --network=cortex -p 3000:3000 grafana/grafana In the Grafana UI (username/password admin/admin), add a Prometheus datasource for Cortex (http://cortex2:9009/api/prom).\nTo clean up: CTRL-C the Prometheus process and run:\n$ docker rm -f cortex1 cortex2 consul grafana $ docker network remove cortex High availability with replication In this last demo we\u0026rsquo;ll show how Cortex can replicate data among three nodes, and demonstrate Cortex can tolerate a node failure without affecting reads and writes.\nFirst, create a network and run a new Consul and Grafana:\n$ docker network create cortex $ docker run -d --name=consul --network=cortex -e CONSUL_BIND_INTERFACE=eth0 consul $ docker run -d --name=grafana --network=cortex -p 3000:3000 grafana/grafana Then, launch 3 Cortex nodes with replication factor 3:\n$ docker run -d --name=cortex1 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config.yaml:/etc/single-process-config.yaml \\ -p 9001:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 \\ -distributor.replication-factor=3 $ docker run -d --name=cortex2 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config.yaml:/etc/single-process-config.yaml \\ -p 9002:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 \\ -distributor.replication-factor=3 $ docker run -d --name=cortex3 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config.yaml:/etc/single-process-config.yaml \\ -p 9003:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 \\ -distributor.replication-factor=3 Configure Prometheus to send data to the first replica:\nremote_write:-url:http://localhost:9001/api/prom/push$ ./prometheus --config.file=./documentation/examples/prometheus.yml In Grafana, add a datasource for the 3rd Cortex replica (http://cortex3:9009/api/prom) and verify the same data appears in both Prometheus and Cortex.\nTo show that Cortex can tolerate a node failure, hard kill one of the Cortex replicas:\n$ docker rm -f cortex2 You should see writes and queries continue to work without error.\nTo clean up: CTRL-C the Prometheus process and run:\n$ docker rm -f cortex1 cortex2 cortex3 consul grafana $ docker network remove cortex ","excerpt":"Cortex can be run as a single binary or as multiple independent microservices. The single-binary …","ref":"/docs/getting-started/getting-started-chunks-storage/","title":"Getting Started with Chunks Storage"},{"body":" Cortex can be configured using a YAML file - specified using the -config.file flag - or CLI flags. In case you combine both, CLI flags take precedence over the YAML config file.\nThe current configuration of any Cortex component can be seen by visiting the /config HTTP path. Passwords are filtered out of this endpoint.\nReference To specify which configuration file to load, pass the -config.file flag at the command line. The file is written in YAML format, defined by the scheme below. Brackets indicate that a parameter is optional.\nGeneric placeholders \u0026lt;boolean\u0026gt;: a boolean that can take the values true or false \u0026lt;int\u0026gt;: any integer matching the regular expression [1-9]+[0-9]* \u0026lt;duration\u0026gt;: a duration matching the regular expression [0-9]+(ns|us|µs|ms|[smh]) \u0026lt;string\u0026gt;: a regular string \u0026lt;url\u0026gt;: an URL \u0026lt;prefix\u0026gt;: a CLI flag prefix based on the context (look at the parent configuration block to see which CLI flags prefix should be used) Use environment variables in the configuration You can use environment variable references in the config file to set values that need to be configurable during deployment by using the -config.expand-env flag. To do this, use:\n${VAR} Where VAR is the name of the environment variable.\nEach variable reference is replaced at startup by the value of the environment variable. The replacement is case-sensitive and occurs before the YAML file is parsed. References to undefined variables are replaced by empty strings unless you specify a default value or custom error text.\nTo specify a default value, use:\n${VAR:default_value} Where default_value is the value to use if the environment variable is undefined.\nSupported contents and default values of the config file # The Cortex service to run. Supported values are: all, distributor, ingester,# querier, query-frontend, table-manager, ruler, alertmanager, configs.# CLI flag: -target[target:\u0026lt;string\u0026gt;|default=\u0026#34;all\u0026#34;]# Set to false to disable auth.# CLI flag: -auth.enabled[auth_enabled:\u0026lt;boolean\u0026gt;|default=true]# HTTP path prefix for Cortex API.# CLI flag: -http.prefix[http_prefix:\u0026lt;string\u0026gt;|default=\u0026#34;/api/prom\u0026#34;]# The server_config configures the HTTP and gRPC server of the launched# service(s).[server:\u0026lt;server_config\u0026gt;]# The distributor_config configures the Cortex distributor.[distributor:\u0026lt;distributor_config\u0026gt;]# The querier_config configures the Cortex querier.[querier:\u0026lt;querier_config\u0026gt;]# The ingester_client_config configures how the Cortex distributors connect to# the ingesters.[ingester_client:\u0026lt;ingester_client_config\u0026gt;]# The ingester_config configures the Cortex ingester.[ingester:\u0026lt;ingester_config\u0026gt;]flusher:# Directory to read WAL from.# CLI flag: -flusher.wal-dir[wal_dir:\u0026lt;string\u0026gt;|default=\u0026#34;wal\u0026#34;]# Number of concurrent goroutines flushing to dynamodb.# CLI flag: -flusher.concurrent-flushes[concurrent_flushes:\u0026lt;int\u0026gt;|default=50]# Timeout for individual flush operations.# CLI flag: -flusher.flush-op-timeout[flush_op_timeout:\u0026lt;duration\u0026gt;|default=2m0s]# The storage_config configures where Cortex stores the data (chunks storage# engine).[storage:\u0026lt;storage_config\u0026gt;]# The chunk_store_config configures how Cortex stores the data (chunks storage# engine).[chunk_store:\u0026lt;chunk_store_config\u0026gt;]# The limits_config configures default and per-tenant limits imposed by Cortex# services (ie. distributor, ingester, ...).[limits:\u0026lt;limits_config\u0026gt;]# The frontend_worker_config configures the worker - running within the Cortex# querier - picking up and executing queries enqueued by the query-frontend.[frontend_worker:\u0026lt;frontend_worker_config\u0026gt;]# The query_frontend_config configures the Cortex query-frontend.[frontend:\u0026lt;query_frontend_config\u0026gt;]# The queryrange_config configures the query splitting and caching in the Cortex# query-frontend.[query_range:\u0026lt;queryrange_config\u0026gt;]# The table_manager_config configures the Cortex table-manager.[table_manager:\u0026lt;table_manager_config\u0026gt;]# The tsdb_config configures the experimental blocks storage.[tsdb:\u0026lt;tsdb_config\u0026gt;]# The compactor_config configures the compactor for the experimental blocks# storage.[compactor:\u0026lt;compactor_config\u0026gt;]# The purger_config configures the purger which takes care of delete requests[purger:\u0026lt;purger_config\u0026gt;]# The ruler_config configures the Cortex ruler.[ruler:\u0026lt;ruler_config\u0026gt;]# The configs_config configures the Cortex Configs DB and API.[configs:\u0026lt;configs_config\u0026gt;]# The alertmanager_config configures the Cortex alertmanager.[alertmanager:\u0026lt;alertmanager_config\u0026gt;]runtime_config:# How often to check runtime config file.# CLI flag: -runtime-config.reload-period[period:\u0026lt;duration\u0026gt;|default=10s]# File with the configuration that can be updated in runtime.# CLI flag: -runtime-config.file[file:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# The memberlist_config configures the Gossip memberlist.[memberlist:\u0026lt;memberlist_config\u0026gt;] server_config The server_config configures the HTTP and gRPC server of the launched service(s).\n# HTTP server listen address.# CLI flag: -server.http-listen-address[http_listen_address:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# HTTP server listen port.# CLI flag: -server.http-listen-port[http_listen_port:\u0026lt;int\u0026gt;|default=80]# Maximum number of simultaneous http connections, \u0026lt;=0 to disable# CLI flag: -server.http-conn-limit[http_listen_conn_limit:\u0026lt;int\u0026gt;|default=0]# gRPC server listen address.# CLI flag: -server.grpc-listen-address[grpc_listen_address:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# gRPC server listen port.# CLI flag: -server.grpc-listen-port[grpc_listen_port:\u0026lt;int\u0026gt;|default=9095]# Maximum number of simultaneous grpc connections, \u0026lt;=0 to disable# CLI flag: -server.grpc-conn-limit[grpc_listen_conn_limit:\u0026lt;int\u0026gt;|default=0]# Register the intrumentation handlers (/metrics etc).# CLI flag: -server.register-instrumentation[register_instrumentation:\u0026lt;boolean\u0026gt;|default=true]# Timeout for graceful shutdowns# CLI flag: -server.graceful-shutdown-timeout[graceful_shutdown_timeout:\u0026lt;duration\u0026gt;|default=30s]# Read timeout for HTTP server# CLI flag: -server.http-read-timeout[http_server_read_timeout:\u0026lt;duration\u0026gt;|default=30s]# Write timeout for HTTP server# CLI flag: -server.http-write-timeout[http_server_write_timeout:\u0026lt;duration\u0026gt;|default=30s]# Idle timeout for HTTP server# CLI flag: -server.http-idle-timeout[http_server_idle_timeout:\u0026lt;duration\u0026gt;|default=2m0s]# Limit on the size of a gRPC message this server can receive (bytes).# CLI flag: -server.grpc-max-recv-msg-size-bytes[grpc_server_max_recv_msg_size:\u0026lt;int\u0026gt;|default=4194304]# Limit on the size of a gRPC message this server can send (bytes).# CLI flag: -server.grpc-max-send-msg-size-bytes[grpc_server_max_send_msg_size:\u0026lt;int\u0026gt;|default=4194304]# Limit on the number of concurrent streams for gRPC calls (0 = unlimited)# CLI flag: -server.grpc-max-concurrent-streams[grpc_server_max_concurrent_streams:\u0026lt;int\u0026gt;|default=100]# The duration after which an idle connection should be closed. Default:# infinity# CLI flag: -server.grpc.keepalive.max-connection-idle[grpc_server_max_connection_idle:\u0026lt;duration\u0026gt;|default=2562047h47m16.854775807s]# The duration for the maximum amount of time a connection may exist before it# will be closed. Default: infinity# CLI flag: -server.grpc.keepalive.max-connection-age[grpc_server_max_connection_age:\u0026lt;duration\u0026gt;|default=2562047h47m16.854775807s]# An additive period after max-connection-age after which the connection will be# forcibly closed. Default: infinity# CLI flag: -server.grpc.keepalive.max-connection-age-grace[grpc_server_max_connection_age_grace:\u0026lt;duration\u0026gt;|default=2562047h47m16.854775807s]# Duration after which a keepalive probe is sent in case of no activity over the# connection., Default: 2h# CLI flag: -server.grpc.keepalive.time[grpc_server_keepalive_time:\u0026lt;duration\u0026gt;|default=2h0m0s]# After having pinged for keepalive check, the duration after which an idle# connection should be closed, Default: 20s# CLI flag: -server.grpc.keepalive.timeout[grpc_server_keepalive_timeout:\u0026lt;duration\u0026gt;|default=20s]# Only log messages with the given severity or above. Valid levels: [debug,# info, warn, error]# CLI flag: -log.level[log_level:\u0026lt;string\u0026gt;|default=\u0026#34;info\u0026#34;]# Base path to serve all API routes from (e.g. /v1/)# CLI flag: -server.path-prefix[http_path_prefix:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;] distributor_config The distributor_config configures the Cortex distributor.\npool:# How frequently to clean up clients for ingesters that have gone away.# CLI flag: -distributor.client-cleanup-period[client_cleanup_period:\u0026lt;duration\u0026gt;|default=15s]# Run a health check on each ingester client during periodic cleanup.# CLI flag: -distributor.health-check-ingesters[health_check_ingesters:\u0026lt;boolean\u0026gt;|default=false]ha_tracker:# Enable the distributors HA tracker so that it can accept samples from# Prometheus HA replicas gracefully (requires labels).# CLI flag: -distributor.ha-tracker.enable[enable_ha_tracker:\u0026lt;boolean\u0026gt;|default=false]# Update the timestamp in the KV store for a given cluster/replica only after# this amount of time has passed since the current stored timestamp.# CLI flag: -distributor.ha-tracker.update-timeout[ha_tracker_update_timeout:\u0026lt;duration\u0026gt;|default=15s]# Maximum jitter applied to the update timeout, in order to spread the HA# heartbeats over time.# CLI flag: -distributor.ha-tracker.update-timeout-jitter-max[ha_tracker_update_timeout_jitter_max:\u0026lt;duration\u0026gt;|default=5s]# If we don\u0026#39;t receive any samples from the accepted replica for a cluster in# this amount of time we will failover to the next replica we receive a sample# from. This value must be greater than the update timeout# CLI flag: -distributor.ha-tracker.failover-timeout[ha_tracker_failover_timeout:\u0026lt;duration\u0026gt;|default=30s]kvstore:# Backend storage to use for the ring. Supported values are: consul, etcd,# inmemory, multi, memberlist (experimental).# CLI flag: -distributor.ha-tracker.store[store:\u0026lt;string\u0026gt;|default=\u0026#34;consul\u0026#34;]# The prefix for the keys in the store. Should end with a /.# CLI flag: -distributor.ha-tracker.prefix[prefix:\u0026lt;string\u0026gt;|default=\u0026#34;ha-tracker/\u0026#34;]# The consul_config configures the consul client.# The CLI flags prefix for this block config is: distributor.ha-tracker[consul:\u0026lt;consul_config\u0026gt;]# The etcd_config configures the etcd client.# The CLI flags prefix for this block config is: distributor.ha-tracker[etcd:\u0026lt;etcd_config\u0026gt;]multi:# Primary backend storage used by multi-client.# CLI flag: -distributor.ha-tracker.multi.primary[primary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Secondary backend storage used by multi-client.# CLI flag: -distributor.ha-tracker.multi.secondary[secondary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Mirror writes to secondary store.# CLI flag: -distributor.ha-tracker.multi.mirror-enabled[mirror_enabled:\u0026lt;boolean\u0026gt;|default=false]# Timeout for storing value to secondary store.# CLI flag: -distributor.ha-tracker.multi.mirror-timeout[mirror_timeout:\u0026lt;duration\u0026gt;|default=2s]# remote_write API max receive message size (bytes).# CLI flag: -distributor.max-recv-msg-size[max_recv_msg_size:\u0026lt;int\u0026gt;|default=104857600]# Timeout for downstream ingesters.# CLI flag: -distributor.remote-timeout[remote_timeout:\u0026lt;duration\u0026gt;|default=2s]# Time to wait before sending more than the minimum successful query requests.# CLI flag: -distributor.extra-query-delay[extra_queue_delay:\u0026lt;duration\u0026gt;|default=0s]# Distribute samples based on all labels, as opposed to solely by user and# metric name.# CLI flag: -distributor.shard-by-all-labels[shard_by_all_labels:\u0026lt;boolean\u0026gt;|default=false]ring:kvstore:# Backend storage to use for the ring. Supported values are: consul, etcd,# inmemory, multi, memberlist (experimental).# CLI flag: -distributor.ring.store[store:\u0026lt;string\u0026gt;|default=\u0026#34;consul\u0026#34;]# The prefix for the keys in the store. Should end with a /.# CLI flag: -distributor.ring.prefix[prefix:\u0026lt;string\u0026gt;|default=\u0026#34;collectors/\u0026#34;]# The consul_config configures the consul client.# The CLI flags prefix for this block config is: distributor.ring[consul:\u0026lt;consul_config\u0026gt;]# The etcd_config configures the etcd client.# The CLI flags prefix for this block config is: distributor.ring[etcd:\u0026lt;etcd_config\u0026gt;]multi:# Primary backend storage used by multi-client.# CLI flag: -distributor.ring.multi.primary[primary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Secondary backend storage used by multi-client.# CLI flag: -distributor.ring.multi.secondary[secondary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Mirror writes to secondary store.# CLI flag: -distributor.ring.multi.mirror-enabled[mirror_enabled:\u0026lt;boolean\u0026gt;|default=false]# Timeout for storing value to secondary store.# CLI flag: -distributor.ring.multi.mirror-timeout[mirror_timeout:\u0026lt;duration\u0026gt;|default=2s]# Period at which to heartbeat to the ring.# CLI flag: -distributor.ring.heartbeat-period[heartbeat_period:\u0026lt;duration\u0026gt;|default=5s]# The heartbeat timeout after which distributors are considered unhealthy# within the ring.# CLI flag: -distributor.ring.heartbeat-timeout[heartbeat_timeout:\u0026lt;duration\u0026gt;|default=1m0s] ingester_config The ingester_config configures the Cortex ingester.\nwalconfig:# Enable writing of ingested data into WAL.# CLI flag: -ingester.wal-enabled[wal_enabled:\u0026lt;boolean\u0026gt;|default=false]# Enable checkpointing of in-memory chunks.# CLI flag: -ingester.checkpoint-enabled[checkpoint_enabled:\u0026lt;boolean\u0026gt;|default=false]# Recover data from existing WAL irrespective of WAL enabled/disabled.# CLI flag: -ingester.recover-from-wal[recover_from_wal:\u0026lt;boolean\u0026gt;|default=false]# Directory to store the WAL and/or recover from WAL.# CLI flag: -ingester.wal-dir[wal_dir:\u0026lt;string\u0026gt;|default=\u0026#34;wal\u0026#34;]# Interval at which checkpoints should be created.# CLI flag: -ingester.checkpoint-duration[checkpoint_duration:\u0026lt;duration\u0026gt;|default=30m0s]lifecycler:ring:kvstore:# Backend storage to use for the ring. Supported values are: consul, etcd,# inmemory, multi, memberlist (experimental).# CLI flag: -ring.store[store:\u0026lt;string\u0026gt;|default=\u0026#34;consul\u0026#34;]# The prefix for the keys in the store. Should end with a /.# CLI flag: -ring.prefix[prefix:\u0026lt;string\u0026gt;|default=\u0026#34;collectors/\u0026#34;]# The consul_config configures the consul client.[consul:\u0026lt;consul_config\u0026gt;]# The etcd_config configures the etcd client.[etcd:\u0026lt;etcd_config\u0026gt;]multi:# Primary backend storage used by multi-client.# CLI flag: -multi.primary[primary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Secondary backend storage used by multi-client.# CLI flag: -multi.secondary[secondary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Mirror writes to secondary store.# CLI flag: -multi.mirror-enabled[mirror_enabled:\u0026lt;boolean\u0026gt;|default=false]# Timeout for storing value to secondary store.# CLI flag: -multi.mirror-timeout[mirror_timeout:\u0026lt;duration\u0026gt;|default=2s]# The heartbeat timeout after which ingesters are skipped for reads/writes.# CLI flag: -ring.heartbeat-timeout[heartbeat_timeout:\u0026lt;duration\u0026gt;|default=1m0s]# The number of ingesters to write to and read from.# CLI flag: -distributor.replication-factor[replication_factor:\u0026lt;int\u0026gt;|default=3]# Number of tokens for each ingester.# CLI flag: -ingester.num-tokens[num_tokens:\u0026lt;int\u0026gt;|default=128]# Period at which to heartbeat to consul.# CLI flag: -ingester.heartbeat-period[heartbeat_period:\u0026lt;duration\u0026gt;|default=5s]# Observe tokens after generating to resolve collisions. Useful when using# gossiping ring.# CLI flag: -ingester.observe-period[observe_period:\u0026lt;duration\u0026gt;|default=0s]# Period to wait for a claim from another member; will join automatically# after this.# CLI flag: -ingester.join-after[join_after:\u0026lt;duration\u0026gt;|default=0s]# Minimum duration to wait before becoming ready. This is to work around race# conditions with ingesters exiting and updating the ring.# CLI flag: -ingester.min-ready-duration[min_ready_duration:\u0026lt;duration\u0026gt;|default=1m0s]# Name of network interface to read address from.# CLI flag: -ingester.lifecycler.interface[interface_names:\u0026lt;listofstring\u0026gt;|default=[eth0en0]]# Duration to sleep for before exiting, to ensure metrics are scraped.# CLI flag: -ingester.final-sleep[final_sleep:\u0026lt;duration\u0026gt;|default=30s]# File path where tokens are stored. If empty, tokens are not stored at# shutdown and restored at startup.# CLI flag: -ingester.tokens-file-path[tokens_file_path:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Number of times to try and transfer chunks before falling back to flushing.# Negative value or zero disables hand-over.# CLI flag: -ingester.max-transfer-retries[max_transfer_retries:\u0026lt;int\u0026gt;|default=10]# Period with which to attempt to flush chunks.# CLI flag: -ingester.flush-period[flushcheckperiod:\u0026lt;duration\u0026gt;|default=1m0s]# Period chunks will remain in memory after flushing.# CLI flag: -ingester.retain-period[retainperiod:\u0026lt;duration\u0026gt;|default=5m0s]# Maximum chunk idle time before flushing.# CLI flag: -ingester.max-chunk-idle[maxchunkidle:\u0026lt;duration\u0026gt;|default=5m0s]# Maximum chunk idle time for chunks terminating in stale markers before# flushing. 0 disables it and a stale series is not flushed until the# max-chunk-idle timeout is reached.# CLI flag: -ingester.max-stale-chunk-idle[maxstalechunkidle:\u0026lt;duration\u0026gt;|default=0s]# Timeout for individual flush operations.# CLI flag: -ingester.flush-op-timeout[flushoptimeout:\u0026lt;duration\u0026gt;|default=1m0s]# Maximum chunk age before flushing.# CLI flag: -ingester.max-chunk-age[maxchunkage:\u0026lt;duration\u0026gt;|default=12h0m0s]# Range of time to subtract from MaxChunkAge to spread out flushes# CLI flag: -ingester.chunk-age-jitter[chunkagejitter:\u0026lt;duration\u0026gt;|default=20m0s]# Number of concurrent goroutines flushing to dynamodb.# CLI flag: -ingester.concurrent-flushes[concurrentflushes:\u0026lt;int\u0026gt;|default=50]# If true, spread series flushes across the whole period of MaxChunkAge# CLI flag: -ingester.spread-flushes[spreadflushes:\u0026lt;boolean\u0026gt;|default=false]# Period with which to update the per-user ingestion rates.# CLI flag: -ingester.rate-update-period[rateupdateperiod:\u0026lt;duration\u0026gt;|default=15s] querier_config The querier_config configures the Cortex querier.\n# The maximum number of concurrent queries.# CLI flag: -querier.max-concurrent[maxconcurrent:\u0026lt;int\u0026gt;|default=20]# The timeout for a query.# CLI flag: -querier.timeout[timeout:\u0026lt;duration\u0026gt;|default=2m0s]# Use iterators to execute query, as opposed to fully materialising the series# in memory.# CLI flag: -querier.iterators[iterators:\u0026lt;boolean\u0026gt;|default=false]# Use batch iterators to execute query, as opposed to fully materialising the# series in memory. Takes precedent over the -querier.iterators flag.# CLI flag: -querier.batch-iterators[batchiterators:\u0026lt;boolean\u0026gt;|default=false]# Use streaming RPCs to query ingester.# CLI flag: -querier.ingester-streaming[ingesterstreaming:\u0026lt;boolean\u0026gt;|default=false]# Maximum number of samples a single query can load into memory.# CLI flag: -querier.max-samples[maxsamples:\u0026lt;int\u0026gt;|default=50000000]# Maximum lookback beyond which queries are not sent to ingester. 0 means all# queries are sent to ingester.# CLI flag: -querier.query-ingesters-within[query_ingesters_within:\u0026lt;duration\u0026gt;|default=0s]# The time after which a metric should only be queried from storage and not just# ingesters. 0 means all queries are sent to store.# CLI flag: -querier.query-store-after[query_store_after:\u0026lt;duration\u0026gt;|default=0s]# Maximum duration into the future you can query. 0 to disable.# CLI flag: -querier.max-query-into-future[max_query_into_future:\u0026lt;duration\u0026gt;|default=10m0s]# The default evaluation interval or step size for subqueries.# CLI flag: -querier.default-evaluation-interval[defaultevaluationinterval:\u0026lt;duration\u0026gt;|default=1m0s]# Active query tracker monitors active queries, and writes them to the file in# given directory. If Cortex discovers any queries in this log during startup,# it will log them to the log file. Setting to empty value disables active query# tracker, which also disables -querier.max-concurrent option.# CLI flag: -querier.active-query-tracker-dir[active_query_tracker_dir:\u0026lt;string\u0026gt;|default=\u0026#34;./active-query-tracker\u0026#34;] query_frontend_config The query_frontend_config configures the Cortex query-frontend.\n# Maximum number of outstanding requests per tenant per frontend; requests# beyond this error with HTTP 429.# CLI flag: -querier.max-outstanding-requests-per-tenant[max_outstanding_per_tenant:\u0026lt;int\u0026gt;|default=100]# Compress HTTP responses.# CLI flag: -querier.compress-http-responses[compress_responses:\u0026lt;boolean\u0026gt;|default=false]# URL of downstream Prometheus.# CLI flag: -frontend.downstream-url[downstream:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Log queries that are slower than the specified duration. 0 to disable.# CLI flag: -frontend.log-queries-longer-than[log_queries_longer_than:\u0026lt;duration\u0026gt;|default=0s] queryrange_config The queryrange_config configures the query splitting and caching in the Cortex query-frontend.\n# Split queries by an interval and execute in parallel, 0 disables it. You# should use an a multiple of 24 hours (same as the storage bucketing scheme),# to avoid queriers downloading and processing the same chunks. This also# determines how cache keys are chosen when result caching is enabled# CLI flag: -querier.split-queries-by-interval[split_queries_by_interval:\u0026lt;duration\u0026gt;|default=0s]# Deprecated: Split queries by day and execute in parallel.# CLI flag: -querier.split-queries-by-day[split_queries_by_day:\u0026lt;boolean\u0026gt;|default=false]# Mutate incoming queries to align their start and end with their step.# CLI flag: -querier.align-querier-with-step[align_queries_with_step:\u0026lt;boolean\u0026gt;|default=false]results_cache:cache:# Enable in-memory cache.# CLI flag: -frontend.cache.enable-fifocache[enable_fifocache:\u0026lt;boolean\u0026gt;|default=false]# The default validity of entries for caches unless overridden.# CLI flag: -frontend.default-validity[default_validity:\u0026lt;duration\u0026gt;|default=0s]background:# How many goroutines to use to write back to memcache.# CLI flag: -frontend.memcache.write-back-goroutines[writeback_goroutines:\u0026lt;int\u0026gt;|default=10]# How many key batches to buffer for background write-back.# CLI flag: -frontend.memcache.write-back-buffer[writeback_buffer:\u0026lt;int\u0026gt;|default=10000]# The memcached_config block configures how data is stored in Memcached (ie.# expiration).# The CLI flags prefix for this block config is: frontend[memcached:\u0026lt;memcached_config\u0026gt;]# The memcached_client_config configures the client used to connect to# Memcached.# The CLI flags prefix for this block config is: frontend[memcached_client:\u0026lt;memcached_client_config\u0026gt;]# The redis_config configures the Redis backend cache.# The CLI flags prefix for this block config is: frontend[redis:\u0026lt;redis_config\u0026gt;]# The fifo_cache_config configures the local in-memory cache.# The CLI flags prefix for this block config is: frontend[fifocache:\u0026lt;fifo_cache_config\u0026gt;]# Most recent allowed cacheable result, to prevent caching very recent results# that might still be in flux.# CLI flag: -frontend.max-cache-freshness[max_freshness:\u0026lt;duration\u0026gt;|default=1m0s]# Cache query results.# CLI flag: -querier.cache-results[cache_results:\u0026lt;boolean\u0026gt;|default=false]# Maximum number of retries for a single request; beyond this, the downstream# error is returned.# CLI flag: -querier.max-retries-per-request[max_retries:\u0026lt;int\u0026gt;|default=5]# Perform query parallelisations based on storage sharding configuration and# query ASTs. This feature is supported only by the chunks storage engine.# CLI flag: -querier.parallelise-shardable-queries[parallelise_shardable_queries:\u0026lt;boolean\u0026gt;|default=false] ruler_config The ruler_config configures the Cortex ruler.\n# URL of alerts return path.# CLI flag: -ruler.external.url[externalurl:\u0026lt;url\u0026gt;|default=]# How frequently to evaluate rules# CLI flag: -ruler.evaluation-interval[evaluationinterval:\u0026lt;duration\u0026gt;|default=1m0s]# How frequently to poll for rule changes# CLI flag: -ruler.poll-interval[pollinterval:\u0026lt;duration\u0026gt;|default=1m0s]storeconfig:# Method to use for backend rule storage (configdb)# CLI flag: -ruler.storage.type[type:\u0026lt;string\u0026gt;|default=\u0026#34;configdb\u0026#34;]# The configstore_config configures the config database storing rules and# alerts, and is used by the Cortex alertmanager.# The CLI flags prefix for this block config is: ruler[configdb:\u0026lt;configstore_config\u0026gt;]# file path to store temporary rule files for the prometheus rule managers# CLI flag: -ruler.rule-path[rulepath:\u0026lt;string\u0026gt;|default=\u0026#34;/rules\u0026#34;]# URL of the Alertmanager to send notifications to.# CLI flag: -ruler.alertmanager-url[alertmanagerurl:\u0026lt;url\u0026gt;|default=]# Use DNS SRV records to discover alertmanager hosts.# CLI flag: -ruler.alertmanager-discovery[alertmanagerdiscovery:\u0026lt;boolean\u0026gt;|default=false]# How long to wait between refreshing alertmanager hosts.# CLI flag: -ruler.alertmanager-refresh-interval[alertmanagerrefreshinterval:\u0026lt;duration\u0026gt;|default=1m0s]# If enabled requests to alertmanager will utilize the V2 API.# CLI flag: -ruler.alertmanager-use-v2[alertmanangerenablev2api:\u0026lt;boolean\u0026gt;|default=false]# Capacity of the queue for notifications to be sent to the Alertmanager.# CLI flag: -ruler.notification-queue-capacity[notificationqueuecapacity:\u0026lt;int\u0026gt;|default=10000]# HTTP timeout duration when sending notifications to the Alertmanager.# CLI flag: -ruler.notification-timeout[notificationtimeout:\u0026lt;duration\u0026gt;|default=10s]# Distribute rule evaluation using ring backend# CLI flag: -ruler.enable-sharding[enablesharding:\u0026lt;boolean\u0026gt;|default=false]# Time to spend searching for a pending ruler when shutting down.# CLI flag: -ruler.search-pending-for[searchpendingfor:\u0026lt;duration\u0026gt;|default=5m0s]ring:kvstore:# Backend storage to use for the ring. Supported values are: consul, etcd,# inmemory, multi, memberlist (experimental).# CLI flag: -ruler.ring.store[store:\u0026lt;string\u0026gt;|default=\u0026#34;consul\u0026#34;]# The prefix for the keys in the store. Should end with a /.# CLI flag: -ruler.ring.prefix[prefix:\u0026lt;string\u0026gt;|default=\u0026#34;rulers/\u0026#34;]# The consul_config configures the consul client.# The CLI flags prefix for this block config is: ruler.ring[consul:\u0026lt;consul_config\u0026gt;]# The etcd_config configures the etcd client.# The CLI flags prefix for this block config is: ruler.ring[etcd:\u0026lt;etcd_config\u0026gt;]multi:# Primary backend storage used by multi-client.# CLI flag: -ruler.ring.multi.primary[primary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Secondary backend storage used by multi-client.# CLI flag: -ruler.ring.multi.secondary[secondary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Mirror writes to secondary store.# CLI flag: -ruler.ring.multi.mirror-enabled[mirror_enabled:\u0026lt;boolean\u0026gt;|default=false]# Timeout for storing value to secondary store.# CLI flag: -ruler.ring.multi.mirror-timeout[mirror_timeout:\u0026lt;duration\u0026gt;|default=2s]# Period at which to heartbeat to the ring.# CLI flag: -ruler.ring.heartbeat-period[heartbeat_period:\u0026lt;duration\u0026gt;|default=5s]# The heartbeat timeout after which rulers are considered unhealthy within the# ring.# CLI flag: -ruler.ring.heartbeat-timeout[heartbeat_timeout:\u0026lt;duration\u0026gt;|default=1m0s]# Number of tokens for each ingester.# CLI flag: -ruler.ring.num-tokens[num_tokens:\u0026lt;int\u0026gt;|default=128]# Period with which to attempt to flush rule groups.# CLI flag: -ruler.flush-period[flushcheckperiod:\u0026lt;duration\u0026gt;|default=1m0s]# Enable the ruler api# CLI flag: -experimental.ruler.enable-api[enable_api:\u0026lt;boolean\u0026gt;|default=false] alertmanager_config The alertmanager_config configures the Cortex alertmanager.\n# Base path for data storage.# CLI flag: -alertmanager.storage.path[datadir:\u0026lt;string\u0026gt;|default=\u0026#34;data/\u0026#34;]# How long to keep data for.# CLI flag: -alertmanager.storage.retention[retention:\u0026lt;duration\u0026gt;|default=120h0m0s]# The URL under which Alertmanager is externally reachable (for example, if# Alertmanager is served via a reverse proxy). Used for generating relative and# absolute links back to Alertmanager itself. If the URL has a path portion, it# will be used to prefix all HTTP endpoints served by Alertmanager. If omitted,# relevant URL components will be derived automatically.# CLI flag: -alertmanager.web.external-url[externalurl:\u0026lt;url\u0026gt;|default=]# How frequently to poll Cortex configs# CLI flag: -alertmanager.configs.poll-interval[pollinterval:\u0026lt;duration\u0026gt;|default=15s]# Listen address for cluster.# CLI flag: -cluster.listen-address[clusterbindaddr:\u0026lt;string\u0026gt;|default=\u0026#34;0.0.0.0:9094\u0026#34;]# Explicit address to advertise in cluster.# CLI flag: -cluster.advertise-address[clusteradvertiseaddr:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Initial peers (may be repeated).# CLI flag: -cluster.peer[peers:\u0026lt;listofstring\u0026gt;|default=]# Time to wait between peers to send notifications.# CLI flag: -cluster.peer-timeout[peertimeout:\u0026lt;duration\u0026gt;|default=15s]# Filename of fallback config to use if none specified for instance.# CLI flag: -alertmanager.configs.fallback[fallbackconfigfile:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Root of URL to generate if config is http://internal.monitor# CLI flag: -alertmanager.configs.auto-webhook-root[autowebhookroot:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]store:# Type of backend to use to store alertmanager configs. Supported values are:# \u0026#34;configdb\u0026#34;, \u0026#34;local\u0026#34;.# CLI flag: -alertmanager.storage.type[type:\u0026lt;string\u0026gt;|default=\u0026#34;configdb\u0026#34;]# The configstore_config configures the config database storing rules and# alerts, and is used by the Cortex alertmanager.# The CLI flags prefix for this block config is: alertmanager[configdb:\u0026lt;configstore_config\u0026gt;]local:# Path at which alertmanager configurations are stored.# CLI flag: -alertmanager.storage.local.path[path:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;] table_manager_config The table_manager_config configures the Cortex table-manager.\n# If true, disable all changes to DB capacity# CLI flag: -table-manager.throughput-updates-disabled[throughput_updates_disabled:\u0026lt;boolean\u0026gt;|default=false]# If true, enables retention deletes of DB tables# CLI flag: -table-manager.retention-deletes-enabled[retention_deletes_enabled:\u0026lt;boolean\u0026gt;|default=false]# Tables older than this retention period are deleted. Note: This setting is# destructive to data!(default: 0, which disables deletion)# CLI flag: -table-manager.retention-period[retention_period:\u0026lt;duration\u0026gt;|default=0s]# How frequently to poll DynamoDB to learn our capacity.# CLI flag: -dynamodb.poll-interval[dynamodb_poll_interval:\u0026lt;duration\u0026gt;|default=2m0s]# DynamoDB periodic tables grace period (duration which table will be# created/deleted before/after it\u0026#39;s needed).# CLI flag: -dynamodb.periodic-table.grace-period[creation_grace_period:\u0026lt;duration\u0026gt;|default=10m0s]index_tables_provisioning:# Enables on demand throughput provisioning for the storage provider (if# supported). Applies only to tables which are not autoscaled# CLI flag: -dynamodb.periodic-table.enable-ondemand-throughput-mode[provisioned_throughput_on_demand_mode:\u0026lt;boolean\u0026gt;|default=false]# DynamoDB table default write throughput.# CLI flag: -dynamodb.periodic-table.write-throughput[provisioned_write_throughput:\u0026lt;int\u0026gt;|default=1000]# DynamoDB table default read throughput.# CLI flag: -dynamodb.periodic-table.read-throughput[provisioned_read_throughput:\u0026lt;int\u0026gt;|default=300]# Enables on demand throughput provisioning for the storage provider (if# supported). Applies only to tables which are not autoscaled# CLI flag: -dynamodb.periodic-table.inactive-enable-ondemand-throughput-mode[inactive_throughput_on_demand_mode:\u0026lt;boolean\u0026gt;|default=false]# DynamoDB table write throughput for inactive tables.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput[inactive_write_throughput:\u0026lt;int\u0026gt;|default=1]# DynamoDB table read throughput for inactive tables.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput[inactive_read_throughput:\u0026lt;int\u0026gt;|default=300]write_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.periodic-table.write-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]inactive_write_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]# Number of last inactive tables to enable write autoscale.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale-last-n[inactive_write_scale_lastn:\u0026lt;int\u0026gt;|default=4]read_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.periodic-table.read-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]inactive_read_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]# Number of last inactive tables to enable read autoscale.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale-last-n[inactive_read_scale_lastn:\u0026lt;int\u0026gt;|default=4]chunk_tables_provisioning:# Enables on demand throughput provisioning for the storage provider (if# supported). Applies only to tables which are not autoscaled# CLI flag: -dynamodb.chunk-table.enable-ondemand-throughput-mode[provisioned_throughput_on_demand_mode:\u0026lt;boolean\u0026gt;|default=false]# DynamoDB table default write throughput.# CLI flag: -dynamodb.chunk-table.write-throughput[provisioned_write_throughput:\u0026lt;int\u0026gt;|default=1000]# DynamoDB table default read throughput.# CLI flag: -dynamodb.chunk-table.read-throughput[provisioned_read_throughput:\u0026lt;int\u0026gt;|default=300]# Enables on demand throughput provisioning for the storage provider (if# supported). Applies only to tables which are not autoscaled# CLI flag: -dynamodb.chunk-table.inactive-enable-ondemand-throughput-mode[inactive_throughput_on_demand_mode:\u0026lt;boolean\u0026gt;|default=false]# DynamoDB table write throughput for inactive tables.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput[inactive_write_throughput:\u0026lt;int\u0026gt;|default=1]# DynamoDB table read throughput for inactive tables.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput[inactive_read_throughput:\u0026lt;int\u0026gt;|default=300]write_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.chunk-table.write-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]inactive_write_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]# Number of last inactive tables to enable write autoscale.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale-last-n[inactive_write_scale_lastn:\u0026lt;int\u0026gt;|default=4]read_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.chunk-table.read-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]inactive_read_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]# Number of last inactive tables to enable read autoscale.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale-last-n[inactive_read_scale_lastn:\u0026lt;int\u0026gt;|default=4] storage_config The storage_config configures where Cortex stores the data (chunks storage engine).\n# The storage engine to use: chunks or tsdb. Be aware tsdb is experimental and# shouldn\u0026#39;t be used in production.# CLI flag: -store.engine[engine:\u0026lt;string\u0026gt;|default=\u0026#34;chunks\u0026#34;]aws:dynamodbconfig:# DynamoDB endpoint URL with escaped Key and Secret encoded. If only region# is specified as a host, proper endpoint will be deduced. Use# inmemory:///\u0026lt;table-name\u0026gt; to use a mock in-memory implementation.# CLI flag: -dynamodb.url[dynamodb:\u0026lt;url\u0026gt;|default=]# DynamoDB table management requests per second limit.# CLI flag: -dynamodb.api-limit[apilimit:\u0026lt;float\u0026gt;|default=2]# DynamoDB rate cap to back off when throttled.# CLI flag: -dynamodb.throttle-limit[throttlelimit:\u0026lt;float\u0026gt;|default=10]# ApplicationAutoscaling endpoint URL with escaped Key and Secret encoded.# CLI flag: -applicationautoscaling.url[applicationautoscaling:\u0026lt;url\u0026gt;|default=]metrics:# Use metrics-based autoscaling, via this query URL# CLI flag: -metrics.url[url:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Queue length above which we will scale up capacity# CLI flag: -metrics.target-queue-length[targetqueuelen:\u0026lt;int\u0026gt;|default=100000]# Scale up capacity by this multiple# CLI flag: -metrics.scale-up-factor[scaleupfactor:\u0026lt;float\u0026gt;|default=1.3]# Ignore throttling below this level (rate per second)# CLI flag: -metrics.ignore-throttle-below[minthrottling:\u0026lt;float\u0026gt;|default=1]# query to fetch ingester queue length# CLI flag: -metrics.queue-length-query[queuelengthquery:\u0026lt;string\u0026gt;|default=\u0026#34;sum(avg_over_time(cortex_ingester_flush_queue_length{job=\\\u0026#34;cortex/ingester\\\u0026#34;}[2m]))\u0026#34;]# query to fetch throttle rates per table# CLI flag: -metrics.write-throttle-query[throttlequery:\u0026lt;string\u0026gt;|default=\u0026#34;sum(rate(cortex_dynamo_throttled_total{operation=\\\u0026#34;DynamoDB.BatchWriteItem\\\u0026#34;}[1m])) by (table) \u0026gt; 0\u0026#34;]# query to fetch write capacity usage per table# CLI flag: -metrics.usage-query[usagequery:\u0026lt;string\u0026gt;|default=\u0026#34;sum(rate(cortex_dynamo_consumed_capacity_total{operation=\\\u0026#34;DynamoDB.BatchWriteItem\\\u0026#34;}[15m])) by (table) \u0026gt; 0\u0026#34;]# query to fetch read capacity usage per table# CLI flag: -metrics.read-usage-query[readusagequery:\u0026lt;string\u0026gt;|default=\u0026#34;sum(rate(cortex_dynamo_consumed_capacity_total{operation=\\\u0026#34;DynamoDB.QueryPages\\\u0026#34;}[1h])) by (table) \u0026gt; 0\u0026#34;]# query to fetch read errors per table# CLI flag: -metrics.read-error-query[readerrorquery:\u0026lt;string\u0026gt;|default=\u0026#34;sum(increase(cortex_dynamo_failures_total{operation=\\\u0026#34;DynamoDB.QueryPages\\\u0026#34;,error=\\\u0026#34;ProvisionedThroughputExceededException\\\u0026#34;}[1m])) by (table) \u0026gt; 0\u0026#34;]# Number of chunks to group together to parallelise fetches (zero to# disable)# CLI flag: -dynamodb.chunk.gang.size[chunkgangsize:\u0026lt;int\u0026gt;|default=10]# Max number of chunk-get operations to start in parallel# CLI flag: -dynamodb.chunk.get.max.parallelism[chunkgetmaxparallelism:\u0026lt;int\u0026gt;|default=32]# S3 endpoint URL with escaped Key and Secret encoded. If only region is# specified as a host, proper endpoint will be deduced. Use# inmemory:///\u0026lt;bucket-name\u0026gt; to use a mock in-memory implementation.# CLI flag: -s3.url[s3:\u0026lt;url\u0026gt;|default=]# Comma separated list of bucket names to evenly distribute chunks over.# Overrides any buckets specified in s3.url flag# CLI flag: -s3.buckets[bucketnames:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Set this to `true` to force the request to use path-style addressing.# CLI flag: -s3.force-path-style[s3forcepathstyle:\u0026lt;boolean\u0026gt;|default=false]azure:# Name of the blob container used to store chunks. Defaults to `cortex`. This# container must be created before running cortex.# CLI flag: -azure.container-name[container_name:\u0026lt;string\u0026gt;|default=\u0026#34;cortex\u0026#34;]# The Microsoft Azure account name to be used# CLI flag: -azure.account-name[account_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# The Microsoft Azure account key to use.# CLI flag: -azure.account-key[account_key:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Preallocated buffer size for downloads (default is 512KB)# CLI flag: -azure.download-buffer-size[download_buffer_size:\u0026lt;int\u0026gt;|default=512000]# Preallocated buffer size for up;oads (default is 256KB)# CLI flag: -azure.upload-buffer-size[upload_buffer_size:\u0026lt;int\u0026gt;|default=256000]# Number of buffers used to used to upload a chunk. (defaults to 1)# CLI flag: -azure.download-buffer-count[upload_buffer_count:\u0026lt;int\u0026gt;|default=1]# Timeout for requests made against azure blob storage. Defaults to 30# seconds.# CLI flag: -azure.request-timeout[request_timeout:\u0026lt;duration\u0026gt;|default=30s]# Number of retries for a request which times out.# CLI flag: -azure.max-retries[max_retries:\u0026lt;int\u0026gt;|default=5]# Minimum time to wait before retrying a request.# CLI flag: -azure.min-retry-delay[min_retry_delay:\u0026lt;duration\u0026gt;|default=10ms]# Maximum time to wait before retrying a request.# CLI flag: -azure.max-retry-delay[max_retry_delay:\u0026lt;duration\u0026gt;|default=500ms]bigtable:# Bigtable project ID.# CLI flag: -bigtable.project[project:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Bigtable instance ID.# CLI flag: -bigtable.instance[instance:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]grpc_client_config:# gRPC client max receive message size (bytes).# CLI flag: -bigtable.grpc-max-recv-msg-size[max_recv_msg_size:\u0026lt;int\u0026gt;|default=104857600]# gRPC client max send message size (bytes).# CLI flag: -bigtable.grpc-max-send-msg-size[max_send_msg_size:\u0026lt;int\u0026gt;|default=16777216]# Use compression when sending messages.# CLI flag: -bigtable.grpc-use-gzip-compression[use_gzip_compression:\u0026lt;boolean\u0026gt;|default=false]# Rate limit for gRPC client; 0 means disabled.# CLI flag: -bigtable.grpc-client-rate-limit[rate_limit:\u0026lt;float\u0026gt;|default=0]# Rate limit burst for gRPC client.# CLI flag: -bigtable.grpc-client-rate-limit-burst[rate_limit_burst:\u0026lt;int\u0026gt;|default=0]# Enable backoff and retry when we hit ratelimits.# CLI flag: -bigtable.backoff-on-ratelimits[backoff_on_ratelimits:\u0026lt;boolean\u0026gt;|default=false]backoff_config:# Minimum delay when backing off.# CLI flag: -bigtable.backoff-min-period[minbackoff:\u0026lt;duration\u0026gt;|default=100ms]# Maximum delay when backing off.# CLI flag: -bigtable.backoff-max-period[maxbackoff:\u0026lt;duration\u0026gt;|default=10s]# Number of times to backoff and retry before failing.# CLI flag: -bigtable.backoff-retries[maxretries:\u0026lt;int\u0026gt;|default=10]# If enabled, once a tables info is fetched, it is cached.# CLI flag: -bigtable.table-cache.enabled[tablecacheenabled:\u0026lt;boolean\u0026gt;|default=true]# Duration to cache tables before checking again.# CLI flag: -bigtable.table-cache.expiration[tablecacheexpiration:\u0026lt;duration\u0026gt;|default=30m0s]gcs:# Name of GCS bucket to put chunks in.# CLI flag: -gcs.bucketname[bucket_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# The size of the buffer that GCS client for each PUT request. 0 to disable# buffering.# CLI flag: -gcs.chunk-buffer-size[chunk_buffer_size:\u0026lt;int\u0026gt;|default=0]# The duration after which the requests to GCS should be timed out.# CLI flag: -gcs.request-timeout[request_timeout:\u0026lt;duration\u0026gt;|default=0s]cassandra:# Comma-separated hostnames or IPs of Cassandra instances.# CLI flag: -cassandra.addresses[addresses:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Port that Cassandra is running on# CLI flag: -cassandra.port[port:\u0026lt;int\u0026gt;|default=9042]# Keyspace to use in Cassandra.# CLI flag: -cassandra.keyspace[keyspace:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Consistency level for Cassandra.# CLI flag: -cassandra.consistency[consistency:\u0026lt;string\u0026gt;|default=\u0026#34;QUORUM\u0026#34;]# Replication factor to use in Cassandra.# CLI flag: -cassandra.replication-factor[replication_factor:\u0026lt;int\u0026gt;|default=1]# Instruct the cassandra driver to not attempt to get host info from the# system.peers table.# CLI flag: -cassandra.disable-initial-host-lookup[disable_initial_host_lookup:\u0026lt;boolean\u0026gt;|default=false]# Use SSL when connecting to cassandra instances.# CLI flag: -cassandra.ssl[SSL:\u0026lt;boolean\u0026gt;|default=false]# Require SSL certificate validation.# CLI flag: -cassandra.host-verification[host_verification:\u0026lt;boolean\u0026gt;|default=true]# Path to certificate file to verify the peer.# CLI flag: -cassandra.ca-path[CA_path:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Enable password authentication when connecting to cassandra.# CLI flag: -cassandra.auth[auth:\u0026lt;boolean\u0026gt;|default=false]# Username to use when connecting to cassandra.# CLI flag: -cassandra.username[username:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Password to use when connecting to cassandra.# CLI flag: -cassandra.password[password:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# File containing password to use when connecting to cassandra.# CLI flag: -cassandra.password-file[password_file:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# If set, when authenticating with cassandra a custom authenticator will be# expected during the handshake. This flag can be set multiple times.# CLI flag: -cassandra.custom-authenticator[custom_authenticators:\u0026lt;listofstring\u0026gt;|default=]# Timeout when connecting to cassandra.# CLI flag: -cassandra.timeout[timeout:\u0026lt;duration\u0026gt;|default=2s]# Initial connection timeout, used during initial dial to server.# CLI flag: -cassandra.connect-timeout[connect_timeout:\u0026lt;duration\u0026gt;|default=5s]# Number of retries to perform on a request. (Default is 0: no retries)# CLI flag: -cassandra.max-retries[max_retries:\u0026lt;int\u0026gt;|default=0]# Maximum time to wait before retrying a failed request. (Default = 10s)# CLI flag: -cassandra.retry-max-backoff[retry_max_backoff:\u0026lt;duration\u0026gt;|default=10s]# Minimum time to wait before retrying a failed request. (Default = 100ms)# CLI flag: -cassandra.retry-min-backoff[retry_min_backoff:\u0026lt;duration\u0026gt;|default=100ms]boltdb:# Location of BoltDB index files.# CLI flag: -boltdb.dir[directory:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]filesystem:# Directory to store chunks in.# CLI flag: -local.chunk-directory[directory:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Cache validity for active index entries. Should be no higher than# -ingester.max-chunk-idle.# CLI flag: -store.index-cache-validity[indexcachevalidity:\u0026lt;duration\u0026gt;|default=5m0s]index_queries_cache_config:# Cache config for index entry reading. Enable in-memory cache.# CLI flag: -store.index-cache-read.cache.enable-fifocache[enable_fifocache:\u0026lt;boolean\u0026gt;|default=false]# Cache config for index entry reading. The default validity of entries for# caches unless overridden.# CLI flag: -store.index-cache-read.default-validity[default_validity:\u0026lt;duration\u0026gt;|default=0s]background:# Cache config for index entry reading. How many goroutines to use to write# back to memcache.# CLI flag: -store.index-cache-read.memcache.write-back-goroutines[writeback_goroutines:\u0026lt;int\u0026gt;|default=10]# Cache config for index entry reading. How many key batches to buffer for# background write-back.# CLI flag: -store.index-cache-read.memcache.write-back-buffer[writeback_buffer:\u0026lt;int\u0026gt;|default=10000]# The memcached_config block configures how data is stored in Memcached (ie.# expiration).# The CLI flags prefix for this block config is: store.index-cache-read[memcached:\u0026lt;memcached_config\u0026gt;]# The memcached_client_config configures the client used to connect to# Memcached.# The CLI flags prefix for this block config is: store.index-cache-read[memcached_client:\u0026lt;memcached_client_config\u0026gt;]# The redis_config configures the Redis backend cache.# The CLI flags prefix for this block config is: store.index-cache-read[redis:\u0026lt;redis_config\u0026gt;]# The fifo_cache_config configures the local in-memory cache.# The CLI flags prefix for this block config is: store.index-cache-read[fifocache:\u0026lt;fifo_cache_config\u0026gt;]delete_store:# Store for keeping delete request# CLI flag: -deletes.store[store:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Name of the table which stores delete requests# CLI flag: -deletes.requests-table-name[requests_table_name:\u0026lt;string\u0026gt;|default=\u0026#34;delete_requests\u0026#34;] chunk_store_config The chunk_store_config configures how Cortex stores the data (chunks storage engine).\nchunk_cache_config:# Cache config for chunks. Enable in-memory cache.# CLI flag: -cache.enable-fifocache[enable_fifocache:\u0026lt;boolean\u0026gt;|default=false]# Cache config for chunks. The default validity of entries for caches unless# overridden.# CLI flag: -default-validity[default_validity:\u0026lt;duration\u0026gt;|default=0s]background:# Cache config for chunks. How many goroutines to use to write back to# memcache.# CLI flag: -memcache.write-back-goroutines[writeback_goroutines:\u0026lt;int\u0026gt;|default=10]# Cache config for chunks. How many key batches to buffer for background# write-back.# CLI flag: -memcache.write-back-buffer[writeback_buffer:\u0026lt;int\u0026gt;|default=10000]# The memcached_config block configures how data is stored in Memcached (ie.# expiration).[memcached:\u0026lt;memcached_config\u0026gt;]# The memcached_client_config configures the client used to connect to# Memcached.[memcached_client:\u0026lt;memcached_client_config\u0026gt;]# The redis_config configures the Redis backend cache.[redis:\u0026lt;redis_config\u0026gt;]# The fifo_cache_config configures the local in-memory cache.[fifocache:\u0026lt;fifo_cache_config\u0026gt;]write_dedupe_cache_config:# Cache config for index entry writing. Enable in-memory cache.# CLI flag: -store.index-cache-write.cache.enable-fifocache[enable_fifocache:\u0026lt;boolean\u0026gt;|default=false]# Cache config for index entry writing. The default validity of entries for# caches unless overridden.# CLI flag: -store.index-cache-write.default-validity[default_validity:\u0026lt;duration\u0026gt;|default=0s]background:# Cache config for index entry writing. How many goroutines to use to write# back to memcache.# CLI flag: -store.index-cache-write.memcache.write-back-goroutines[writeback_goroutines:\u0026lt;int\u0026gt;|default=10]# Cache config for index entry writing. How many key batches to buffer for# background write-back.# CLI flag: -store.index-cache-write.memcache.write-back-buffer[writeback_buffer:\u0026lt;int\u0026gt;|default=10000]# The memcached_config block configures how data is stored in Memcached (ie.# expiration).# The CLI flags prefix for this block config is: store.index-cache-write[memcached:\u0026lt;memcached_config\u0026gt;]# The memcached_client_config configures the client used to connect to# Memcached.# The CLI flags prefix for this block config is: store.index-cache-write[memcached_client:\u0026lt;memcached_client_config\u0026gt;]# The redis_config configures the Redis backend cache.# The CLI flags prefix for this block config is: store.index-cache-write[redis:\u0026lt;redis_config\u0026gt;]# The fifo_cache_config configures the local in-memory cache.# The CLI flags prefix for this block config is: store.index-cache-write[fifocache:\u0026lt;fifo_cache_config\u0026gt;]# Cache index entries older than this period. 0 to disable.# CLI flag: -store.cache-lookups-older-than[cache_lookups_older_than:\u0026lt;duration\u0026gt;|default=0s]# Limit how long back data can be queried# CLI flag: -store.max-look-back-period[max_look_back_period:\u0026lt;duration\u0026gt;|default=0s] ingester_client_config The ingester_client_config configures how the Cortex distributors connect to the ingesters.\ngrpc_client_config:# gRPC client max receive message size (bytes).# CLI flag: -ingester.client.grpc-max-recv-msg-size[max_recv_msg_size:\u0026lt;int\u0026gt;|default=104857600]# gRPC client max send message size (bytes).# CLI flag: -ingester.client.grpc-max-send-msg-size[max_send_msg_size:\u0026lt;int\u0026gt;|default=16777216]# Use compression when sending messages.# CLI flag: -ingester.client.grpc-use-gzip-compression[use_gzip_compression:\u0026lt;boolean\u0026gt;|default=false]# Rate limit for gRPC client; 0 means disabled.# CLI flag: -ingester.client.grpc-client-rate-limit[rate_limit:\u0026lt;float\u0026gt;|default=0]# Rate limit burst for gRPC client.# CLI flag: -ingester.client.grpc-client-rate-limit-burst[rate_limit_burst:\u0026lt;int\u0026gt;|default=0]# Enable backoff and retry when we hit ratelimits.# CLI flag: -ingester.client.backoff-on-ratelimits[backoff_on_ratelimits:\u0026lt;boolean\u0026gt;|default=false]backoff_config:# Minimum delay when backing off.# CLI flag: -ingester.client.backoff-min-period[minbackoff:\u0026lt;duration\u0026gt;|default=100ms]# Maximum delay when backing off.# CLI flag: -ingester.client.backoff-max-period[maxbackoff:\u0026lt;duration\u0026gt;|default=10s]# Number of times to backoff and retry before failing.# CLI flag: -ingester.client.backoff-retries[maxretries:\u0026lt;int\u0026gt;|default=10] frontend_worker_config The frontend_worker_config configures the worker - running within the Cortex querier - picking up and executing queries enqueued by the query-frontend.\n# Address of query frontend service.# CLI flag: -querier.frontend-address[address:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Number of simultaneous queries to process.# CLI flag: -querier.worker-parallelism[parallelism:\u0026lt;int\u0026gt;|default=10]# How often to query DNS.# CLI flag: -querier.dns-lookup-period[dnslookupduration:\u0026lt;duration\u0026gt;|default=10s]grpc_client_config:# gRPC client max receive message size (bytes).# CLI flag: -querier.frontend-client.grpc-max-recv-msg-size[max_recv_msg_size:\u0026lt;int\u0026gt;|default=104857600]# gRPC client max send message size (bytes).# CLI flag: -querier.frontend-client.grpc-max-send-msg-size[max_send_msg_size:\u0026lt;int\u0026gt;|default=16777216]# Use compression when sending messages.# CLI flag: -querier.frontend-client.grpc-use-gzip-compression[use_gzip_compression:\u0026lt;boolean\u0026gt;|default=false]# Rate limit for gRPC client; 0 means disabled.# CLI flag: -querier.frontend-client.grpc-client-rate-limit[rate_limit:\u0026lt;float\u0026gt;|default=0]# Rate limit burst for gRPC client.# CLI flag: -querier.frontend-client.grpc-client-rate-limit-burst[rate_limit_burst:\u0026lt;int\u0026gt;|default=0]# Enable backoff and retry when we hit ratelimits.# CLI flag: -querier.frontend-client.backoff-on-ratelimits[backoff_on_ratelimits:\u0026lt;boolean\u0026gt;|default=false]backoff_config:# Minimum delay when backing off.# CLI flag: -querier.frontend-client.backoff-min-period[minbackoff:\u0026lt;duration\u0026gt;|default=100ms]# Maximum delay when backing off.# CLI flag: -querier.frontend-client.backoff-max-period[maxbackoff:\u0026lt;duration\u0026gt;|default=10s]# Number of times to backoff and retry before failing.# CLI flag: -querier.frontend-client.backoff-retries[maxretries:\u0026lt;int\u0026gt;|default=10] etcd_config The etcd_config configures the etcd client.\n# The etcd endpoints to connect to.# CLI flag: -\u0026lt;prefix\u0026gt;.etcd.endpoints[endpoints:\u0026lt;listofstring\u0026gt;|default=[]]# The dial timeout for the etcd connection.# CLI flag: -\u0026lt;prefix\u0026gt;.etcd.dial-timeout[dial_timeout:\u0026lt;duration\u0026gt;|default=10s]# The maximum number of retries to do for failed ops.# CLI flag: -\u0026lt;prefix\u0026gt;.etcd.max-retries[max_retries:\u0026lt;int\u0026gt;|default=10] consul_config The consul_config configures the consul client.\n# Hostname and port of Consul.# CLI flag: -\u0026lt;prefix\u0026gt;.consul.hostname[host:\u0026lt;string\u0026gt;|default=\u0026#34;localhost:8500\u0026#34;]# ACL Token used to interact with Consul.# CLI flag: -\u0026lt;prefix\u0026gt;.consul.acltoken[acltoken:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# HTTP timeout when talking to Consul# CLI flag: -\u0026lt;prefix\u0026gt;.consul.client-timeout[httpclienttimeout:\u0026lt;duration\u0026gt;|default=20s]# Enable consistent reads to Consul.# CLI flag: -\u0026lt;prefix\u0026gt;.consul.consistent-reads[consistentreads:\u0026lt;boolean\u0026gt;|default=true]# Rate limit when watching key or prefix in Consul, in requests per second. 0# disables the rate limit.# CLI flag: -\u0026lt;prefix\u0026gt;.consul.watch-rate-limit[watchkeyratelimit:\u0026lt;float\u0026gt;|default=0]# Burst size used in rate limit. Values less than 1 are treated as 1.# CLI flag: -\u0026lt;prefix\u0026gt;.consul.watch-burst-size[watchkeyburstsize:\u0026lt;int\u0026gt;|default=1] memberlist_config The memberlist_config configures the Gossip memberlist.\n# Name of the node in memberlist cluster. Defaults to hostname.# CLI flag: -memberlist.nodename[node_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# The timeout for establishing a connection with a remote node, and for# read/write operations. Uses memberlist LAN defaults if 0.# CLI flag: -memberlist.stream-timeout[stream_timeout:\u0026lt;duration\u0026gt;|default=0s]# Multiplication factor used when sending out messages (factor * log(N+1)).# CLI flag: -memberlist.retransmit-factor[retransmit_factor:\u0026lt;int\u0026gt;|default=0]# How often to use pull/push sync. Uses memberlist LAN defaults if 0.# CLI flag: -memberlist.pullpush-interval[pull_push_interval:\u0026lt;duration\u0026gt;|default=0s]# How often to gossip. Uses memberlist LAN defaults if 0.# CLI flag: -memberlist.gossip-interval[gossip_interval:\u0026lt;duration\u0026gt;|default=0s]# How many nodes to gossip to. Uses memberlist LAN defaults if 0.# CLI flag: -memberlist.gossip-nodes[gossip_nodes:\u0026lt;int\u0026gt;|default=0]# How long to keep gossiping to dead nodes, to give them chance to refute their# death. Uses memberlist LAN defaults if 0.# CLI flag: -memberlist.gossip-to-dead-nodes-time[gossip_to_dead_nodes_time:\u0026lt;duration\u0026gt;|default=0s]# How soon can dead node\u0026#39;s name be reclaimed with new address. Defaults to 0,# which is disabled.# CLI flag: -memberlist.dead-node-reclaim-time[dead_node_reclaim_time:\u0026lt;duration\u0026gt;|default=0s]# Other cluster members to join. Can be specified multiple times. Memberlist# store is EXPERIMENTAL.# CLI flag: -memberlist.join[join_members:\u0026lt;listofstring\u0026gt;|default=]# If this node fails to join memberlist cluster, abort.# CLI flag: -memberlist.abort-if-join-fails[abort_if_cluster_join_fails:\u0026lt;boolean\u0026gt;|default=true]# How long to keep LEFT ingesters in the ring.# CLI flag: -memberlist.left-ingesters-timeout[left_ingesters_timeout:\u0026lt;duration\u0026gt;|default=5m0s]# Timeout for leaving memberlist cluster.# CLI flag: -memberlist.leave-timeout[leave_timeout:\u0026lt;duration\u0026gt;|default=5s]# IP address to listen on for gossip messages. Multiple addresses may be# specified. Defaults to 0.0.0.0# CLI flag: -memberlist.bind-addr[bind_addr:\u0026lt;listofstring\u0026gt;|default=]# Port to listen on for gossip messages.# CLI flag: -memberlist.bind-port[bind_port:\u0026lt;int\u0026gt;|default=7946]# Timeout used when connecting to other nodes to send packet.# CLI flag: -memberlist.packet-dial-timeout[packet_dial_timeout:\u0026lt;duration\u0026gt;|default=5s]# Timeout for writing \u0026#39;packet\u0026#39; data.# CLI flag: -memberlist.packet-write-timeout[packet_write_timeout:\u0026lt;duration\u0026gt;|default=5s] limits_config The limits_config configures default and per-tenant limits imposed by Cortex services (ie. distributor, ingester, \u0026hellip;).\n# Per-user ingestion rate limit in samples per second.# CLI flag: -distributor.ingestion-rate-limit[ingestion_rate:\u0026lt;float\u0026gt;|default=25000]# Whether the ingestion rate limit should be applied individually to each# distributor instance (local), or evenly shared across the cluster (global).# CLI flag: -distributor.ingestion-rate-limit-strategy[ingestion_rate_strategy:\u0026lt;string\u0026gt;|default=\u0026#34;local\u0026#34;]# Per-user allowed ingestion burst size (in number of samples).# CLI flag: -distributor.ingestion-burst-size[ingestion_burst_size:\u0026lt;int\u0026gt;|default=50000]# Flag to enable, for all users, handling of samples with external labels# identifying replicas in an HA Prometheus setup.# CLI flag: -distributor.ha-tracker.enable-for-all-users[accept_ha_samples:\u0026lt;boolean\u0026gt;|default=false]# Prometheus label to look for in samples to identify a Prometheus HA cluster.# CLI flag: -distributor.ha-tracker.cluster[ha_cluster_label:\u0026lt;string\u0026gt;|default=\u0026#34;cluster\u0026#34;]# Prometheus label to look for in samples to identify a Prometheus HA replica.# CLI flag: -distributor.ha-tracker.replica[ha_replica_label:\u0026lt;string\u0026gt;|default=\u0026#34;__replica__\u0026#34;]# This flag can be used to specify label names that to drop during sample# ingestion within the distributor and can be repeated in order to drop multiple# labels.# CLI flag: -distributor.drop-label[drop_labels:\u0026lt;listofstring\u0026gt;|default=]# Maximum length accepted for label names# CLI flag: -validation.max-length-label-name[max_label_name_length:\u0026lt;int\u0026gt;|default=1024]# Maximum length accepted for label value. This setting also applies to the# metric name# CLI flag: -validation.max-length-label-value[max_label_value_length:\u0026lt;int\u0026gt;|default=2048]# Maximum number of label names per series.# CLI flag: -validation.max-label-names-per-series[max_label_names_per_series:\u0026lt;int\u0026gt;|default=30]# Reject old samples.# CLI flag: -validation.reject-old-samples[reject_old_samples:\u0026lt;boolean\u0026gt;|default=false]# Maximum accepted sample age before rejecting.# CLI flag: -validation.reject-old-samples.max-age[reject_old_samples_max_age:\u0026lt;duration\u0026gt;|default=336h0m0s]# Duration which table will be created/deleted before/after it\u0026#39;s needed; we# won\u0026#39;t accept sample from before this time.# CLI flag: -validation.create-grace-period[creation_grace_period:\u0026lt;duration\u0026gt;|default=10m0s]# Enforce every sample has a metric name.# CLI flag: -validation.enforce-metric-name[enforce_metric_name:\u0026lt;boolean\u0026gt;|default=true]# Per-user subring to shard metrics to ingesters. 0 is disabled.# CLI flag: -experimental.distributor.user-subring-size[user_subring_size:\u0026lt;int\u0026gt;|default=0]# The maximum number of series that a query can return.# CLI flag: -ingester.max-series-per-query[max_series_per_query:\u0026lt;int\u0026gt;|default=100000]# The maximum number of samples that a query can return.# CLI flag: -ingester.max-samples-per-query[max_samples_per_query:\u0026lt;int\u0026gt;|default=1000000]# The maximum number of active series per user, per ingester. 0 to disable.# CLI flag: -ingester.max-series-per-user[max_series_per_user:\u0026lt;int\u0026gt;|default=5000000]# The maximum number of active series per metric name, per ingester. 0 to# disable.# CLI flag: -ingester.max-series-per-metric[max_series_per_metric:\u0026lt;int\u0026gt;|default=50000]# The maximum number of active series per user, across the cluster. 0 to# disable. Supported only if -distributor.shard-by-all-labels is true.# CLI flag: -ingester.max-global-series-per-user[max_global_series_per_user:\u0026lt;int\u0026gt;|default=0]# The maximum number of active series per metric name, across the cluster. 0 to# disable.# CLI flag: -ingester.max-global-series-per-metric[max_global_series_per_metric:\u0026lt;int\u0026gt;|default=0]# Minimum number of samples in an idle chunk to flush it to the store. Use with# care, if chunks are less than this size they will be discarded.# CLI flag: -ingester.min-chunk-length[min_chunk_length:\u0026lt;int\u0026gt;|default=0]# Maximum number of chunks that can be fetched in a single query.# CLI flag: -store.query-chunk-limit[max_chunks_per_query:\u0026lt;int\u0026gt;|default=2000000]# Limit to length of chunk store queries, 0 to disable.# CLI flag: -store.max-query-length[max_query_length:\u0026lt;duration\u0026gt;|default=0s]# Maximum number of queries will be scheduled in parallel by the frontend.# CLI flag: -querier.max-query-parallelism[max_query_parallelism:\u0026lt;int\u0026gt;|default=14]# Cardinality limit for index queries.# CLI flag: -store.cardinality-limit[cardinality_limit:\u0026lt;int\u0026gt;|default=100000]# File name of per-user overrides. [deprecated, use -runtime-config.file# instead]# CLI flag: -limits.per-user-override-config[per_tenant_override_config:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Period with which to reload the overrides. [deprecated, use# -runtime-config.reload-period instead]# CLI flag: -limits.per-user-override-period[per_tenant_override_period:\u0026lt;duration\u0026gt;|default=10s] redis_config The redis_config configures the Redis backend cache.\n# Redis service endpoint to use when caching chunks. If empty, no redis will be# used.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.endpoint[endpoint:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Maximum time to wait before giving up on redis requests.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.timeout[timeout:\u0026lt;duration\u0026gt;|default=100ms]# How long keys stay in the redis.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.expiration[expiration:\u0026lt;duration\u0026gt;|default=0s]# Maximum number of idle connections in pool.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.max-idle-conns[max_idle_conns:\u0026lt;int\u0026gt;|default=80]# Maximum number of active connections in pool.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.max-active-conns[max_active_conns:\u0026lt;int\u0026gt;|default=0]# Password to use when connecting to redis.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.password[password:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Enables connecting to redis with TLS.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.enable-tls[enable_tls:\u0026lt;boolean\u0026gt;|default=false] memcached_config The memcached_config block configures how data is stored in Memcached (ie. expiration).\n# How long keys stay in the memcache.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.expiration[expiration:\u0026lt;duration\u0026gt;|default=0s]# How many keys to fetch in each batch.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.batchsize[batch_size:\u0026lt;int\u0026gt;|default=0]# Maximum active requests to memcache.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.parallelism[parallelism:\u0026lt;int\u0026gt;|default=100] memcached_client_config The memcached_client_config configures the client used to connect to Memcached.\n# Hostname for memcached service to use when caching chunks. If empty, no# memcached will be used.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.hostname[host:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# SRV service used to discover memcache servers.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.service[service:\u0026lt;string\u0026gt;|default=\u0026#34;memcached\u0026#34;]# Maximum time to wait before giving up on memcached requests.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.timeout[timeout:\u0026lt;duration\u0026gt;|default=100ms]# Maximum number of idle connections in pool.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.max-idle-conns[max_idle_conns:\u0026lt;int\u0026gt;|default=16]# Period with which to poll DNS for memcache servers.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.update-interval[update_interval:\u0026lt;duration\u0026gt;|default=1m0s]# Use consistent hashing to distribute to memcache servers.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.consistent-hash[consistent_hash:\u0026lt;boolean\u0026gt;|default=false] fifo_cache_config The fifo_cache_config configures the local in-memory cache.\n# The number of entries to cache.# CLI flag: -\u0026lt;prefix\u0026gt;.fifocache.size[size:\u0026lt;int\u0026gt;|default=0]# The expiry duration for the cache.# CLI flag: -\u0026lt;prefix\u0026gt;.fifocache.duration[validity:\u0026lt;duration\u0026gt;|default=0s] configs_config The configs_config configures the Cortex Configs DB and API.\ndatabase:# URI where the database can be found (for dev you can use memory://)# CLI flag: -configs.database.uri[uri:\u0026lt;string\u0026gt;|default=\u0026#34;postgres://postgres@configs-db.weave.local/configs?sslmode=disable\u0026#34;]# Path where the database migration files can be found# CLI flag: -configs.database.migrations-dir[migrations_dir:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# File containing password (username goes in URI)# CLI flag: -configs.database.password-file[password_file:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]api:notifications:# Disable Email notifications for Alertmanager.# CLI flag: -configs.notifications.disable-email[disable_email:\u0026lt;boolean\u0026gt;|default=false]# Disable WebHook notifications for Alertmanager.# CLI flag: -configs.notifications.disable-webhook[disable_webhook:\u0026lt;boolean\u0026gt;|default=false] configstore_config The configstore_config configures the config database storing rules and alerts, and is used by the Cortex alertmanager.\n# URL of configs API server.# CLI flag: -\u0026lt;prefix\u0026gt;.configs.url[configsapiurl:\u0026lt;url\u0026gt;|default=]# Timeout for requests to Weave Cloud configs service.# CLI flag: -\u0026lt;prefix\u0026gt;.configs.client-timeout[clienttimeout:\u0026lt;duration\u0026gt;|default=5s] tsdb_config The tsdb_config configures the experimental blocks storage.\n# Local directory to store TSDBs in the ingesters.# CLI flag: -experimental.tsdb.dir[dir:\u0026lt;string\u0026gt;|default=\u0026#34;tsdb\u0026#34;]# TSDB blocks range period.# CLI flag: -experimental.tsdb.block-ranges-period[block_ranges_period:\u0026lt;listofduration\u0026gt;|default=2h0m0s]# TSDB blocks retention in the ingester before a block is removed. This should# be larger than the block_ranges_period and large enough to give queriers# enough time to discover newly uploaded blocks.# CLI flag: -experimental.tsdb.retention-period[retention_period:\u0026lt;duration\u0026gt;|default=6h0m0s]# How frequently the TSDB blocks are scanned and new ones are shipped to the# storage. 0 means shipping is disabled.# CLI flag: -experimental.tsdb.ship-interval[ship_interval:\u0026lt;duration\u0026gt;|default=1m0s]# Maximum number of tenants concurrently shipping blocks to the storage.# CLI flag: -experimental.tsdb.ship-concurrency[ship_concurrency:\u0026lt;int\u0026gt;|default=10]# Backend storage to use. Supported backends are: s3, gcs, azure, filesystem.# CLI flag: -experimental.tsdb.backend[backend:\u0026lt;string\u0026gt;|default=\u0026#34;s3\u0026#34;]bucket_store:# Directory to store synchronized TSDB index headers.# CLI flag: -experimental.tsdb.bucket-store.sync-dir[sync_dir:\u0026lt;string\u0026gt;|default=\u0026#34;tsdb-sync\u0026#34;]# How frequently scan the bucket to look for changes (new blocks shipped by# ingesters and blocks removed by retention or compaction). 0 disables it.# CLI flag: -experimental.tsdb.bucket-store.sync-interval[sync_interval:\u0026lt;duration\u0026gt;|default=5m0s]# Size in bytes of in-memory index cache used to speed up blocks index lookups# (shared between all tenants).# CLI flag: -experimental.tsdb.bucket-store.index-cache-size-bytes[index_cache_size_bytes:\u0026lt;int\u0026gt;|default=1073741824]# Max size - in bytes - of a per-tenant chunk pool, used to reduce memory# allocations.# CLI flag: -experimental.tsdb.bucket-store.max-chunk-pool-bytes[max_chunk_pool_bytes:\u0026lt;int\u0026gt;|default=2147483648]# Max number of samples per query when loading series from the long-term# storage. 0 disables the limit.# CLI flag: -experimental.tsdb.bucket-store.max-sample-count[max_sample_count:\u0026lt;int\u0026gt;|default=0]# Max number of concurrent queries to execute against the long-term storage on# a per-tenant basis.# CLI flag: -experimental.tsdb.bucket-store.max-concurrent[max_concurrent:\u0026lt;int\u0026gt;|default=20]# Maximum number of concurrent tenants synching blocks.# CLI flag: -experimental.tsdb.bucket-store.tenant-sync-concurrency[tenant_sync_concurrency:\u0026lt;int\u0026gt;|default=10]# Maximum number of concurrent blocks synching per tenant.# CLI flag: -experimental.tsdb.bucket-store.block-sync-concurrency[block_sync_concurrency:\u0026lt;int\u0026gt;|default=20]# Number of Go routines to use when syncing block meta files from object# storage per tenant.# CLI flag: -experimental.tsdb.bucket-store.meta-sync-concurrency[meta_sync_concurrency:\u0026lt;int\u0026gt;|default=20]# Whether the bucket store should use the binary index header. If false, it# uses the JSON index header.# CLI flag: -experimental.tsdb.bucket-store.binary-index-header-enabled[binary_index_header_enabled:\u0026lt;boolean\u0026gt;|default=true]# Minimum age of a block before it\u0026#39;s being read. Set it to safe value (e.g# 30m) if your object storage is eventually consistent. GCS and S3 are# (roughly) strongly consistent.# CLI flag: -experimental.tsdb.bucket-store.consistency-delay[consistency_delay:\u0026lt;duration\u0026gt;|default=0s]# How frequently does Cortex try to compact TSDB head. Block is only created if# data covers smallest block range. Must be greater than 0 and max 5 minutes.# CLI flag: -experimental.tsdb.head-compaction-interval[head_compaction_interval:\u0026lt;duration\u0026gt;|default=1m0s]# Maximum number of tenants concurrently compacting TSDB head into a new block# CLI flag: -experimental.tsdb.head-compaction-concurrency[head_compaction_concurrency:\u0026lt;int\u0026gt;|default=5]# The number of shards of series to use in TSDB (must be a power of 2). Reducing# this will decrease memory footprint, but can negatively impact performance.# CLI flag: -experimental.tsdb.stripe-size[stripe_size:\u0026lt;int\u0026gt;|default=16384]# limit the number of concurrently opening TSDB\u0026#39;s on startup# CLI flag: -experimental.tsdb.max-tsdb-opening-concurrency-on-startup[max_tsdb_opening_concurrency_on_startup:\u0026lt;int\u0026gt;|default=10]s3:# S3 endpoint without schema# CLI flag: -experimental.tsdb.s3.endpoint[endpoint:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# S3 bucket name# CLI flag: -experimental.tsdb.s3.bucket-name[bucket_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# S3 secret access key# CLI flag: -experimental.tsdb.s3.secret-access-key[secret_access_key:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# S3 access key ID# CLI flag: -experimental.tsdb.s3.access-key-id[access_key_id:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# If enabled, use http:// for the S3 endpoint instead of https://. This could# be useful in local dev/test environments while using an S3-compatible# backend storage, like Minio.# CLI flag: -experimental.tsdb.s3.insecure[insecure:\u0026lt;boolean\u0026gt;|default=false]gcs:# GCS bucket name# CLI flag: -experimental.tsdb.gcs.bucket-name[bucket_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# JSON representing either a Google Developers Console client_credentials.json# file or a Google Developers service account key file. If empty, fallback to# Google default logic.# CLI flag: -experimental.tsdb.gcs.service-account[service_account:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]azure:# Azure storage account name# CLI flag: -experimental.tsdb.azure.account-name[account_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Azure storage account key# CLI flag: -experimental.tsdb.azure.account-key[account_key:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Azure storage container name# CLI flag: -experimental.tsdb.azure.container-name[container_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Azure storage endpoint suffix without schema. The account name will be# prefixed to this value to create the FQDN# CLI flag: -experimental.tsdb.azure.endpoint-suffix[endpoint_suffix:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Number of retries for recoverable errors# CLI flag: -experimental.tsdb.azure.max-retries[max_retries:\u0026lt;int\u0026gt;|default=20]filesystem:# Local filesystem storage directory.# CLI flag: -experimental.tsdb.filesystem.dir[dir:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;] compactor_config The compactor_config configures the compactor for the experimental blocks storage.\n# List of compaction time ranges.# CLI flag: -compactor.block-ranges[block_ranges:\u0026lt;listofduration\u0026gt;|default=2h0m0s,12h0m0s,24h0m0s]# Number of Go routines to use when syncing block index and chunks files from# the long term storage.# CLI flag: -compactor.block-sync-concurrency[block_sync_concurrency:\u0026lt;int\u0026gt;|default=20]# Number of Go routines to use when syncing block meta files from the long term# storage.# CLI flag: -compactor.meta-sync-concurrency[meta_sync_concurrency:\u0026lt;int\u0026gt;|default=20]# Minimum age of fresh (non-compacted) blocks before they are being processed.# Malformed blocks older than the maximum of consistency-delay and 48h0m0s will# be removed.# CLI flag: -compactor.consistency-delay[consistency_delay:\u0026lt;duration\u0026gt;|default=30m0s]# Data directory in which to cache blocks and process compactions# CLI flag: -compactor.data-dir[data_dir:\u0026lt;string\u0026gt;|default=\u0026#34;./data\u0026#34;]# The frequency at which the compaction runs# CLI flag: -compactor.compaction-interval[compaction_interval:\u0026lt;duration\u0026gt;|default=1h0m0s]# How many times to retry a failed compaction during a single compaction# interval# CLI flag: -compactor.compaction-retries[compaction_retries:\u0026lt;int\u0026gt;|default=3]# Shard tenants across multiple compactor instances. Sharding is required if you# run multiple compactor instances, in order to coordinate compactions and avoid# race conditions leading to the same tenant blocks simultaneously compacted by# different instances.# CLI flag: -compactor.sharding-enabled[sharding_enabled:\u0026lt;boolean\u0026gt;|default=false]sharding_ring:kvstore:# Backend storage to use for the ring. Supported values are: consul, etcd,# inmemory, multi, memberlist (experimental).# CLI flag: -compactor.ring.store[store:\u0026lt;string\u0026gt;|default=\u0026#34;consul\u0026#34;]# The prefix for the keys in the store. Should end with a /.# CLI flag: -compactor.ring.prefix[prefix:\u0026lt;string\u0026gt;|default=\u0026#34;collectors/\u0026#34;]# The consul_config configures the consul client.# The CLI flags prefix for this block config is: compactor.ring[consul:\u0026lt;consul_config\u0026gt;]# The etcd_config configures the etcd client.# The CLI flags prefix for this block config is: compactor.ring[etcd:\u0026lt;etcd_config\u0026gt;]multi:# Primary backend storage used by multi-client.# CLI flag: -compactor.ring.multi.primary[primary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Secondary backend storage used by multi-client.# CLI flag: -compactor.ring.multi.secondary[secondary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Mirror writes to secondary store.# CLI flag: -compactor.ring.multi.mirror-enabled[mirror_enabled:\u0026lt;boolean\u0026gt;|default=false]# Timeout for storing value to secondary store.# CLI flag: -compactor.ring.multi.mirror-timeout[mirror_timeout:\u0026lt;duration\u0026gt;|default=2s]# Period at which to heartbeat to the ring.# CLI flag: -compactor.ring.heartbeat-period[heartbeat_period:\u0026lt;duration\u0026gt;|default=5s]# The heartbeat timeout after which compactors are considered unhealthy within# the ring.# CLI flag: -compactor.ring.heartbeat-timeout[heartbeat_timeout:\u0026lt;duration\u0026gt;|default=1m0s] purger_config The purger_config configures the purger which takes care of delete requests\n# Enable purger to allow deletion of series. Be aware that Delete series feature# is still experimental# CLI flag: -purger.enable[enable:\u0026lt;boolean\u0026gt;|default=false]# Number of workers executing delete plans in parallel# CLI flag: -purger.num-workers[num_workers:\u0026lt;int\u0026gt;|default=2]# Name of the object store to use for storing delete plans# CLI flag: -purger.object-store-type[object_store_type:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]","excerpt":"Cortex can be configured using a YAML file - specified using the -config.file flag - or CLI flags. …","ref":"/docs/configuration/configuration-file/","title":"Configuration file"},{"body":" \nCortex provides horizontally scalable, highly available, multi-tenant, long term storage for Prometheus.\n Horizontally scalable: Cortex can run across multiple machines in a cluster, exceeding the throughput and storage of a single machine. This enables you to send the metrics from multiple Prometheus servers to a single Cortex cluster and run \u0026ldquo;globally aggregated\u0026rdquo; queries across all data in a single place. Highly available: When run in a cluster, Cortex can replicate data between machines. This allows you to survive machine failure without gaps in your graphs. Multi-tenant: Cortex can isolate data and queries from multiple different independent Prometheus sources in a single cluster, allowing untrusted parties to share the same cluster. Long term storage: Cortex supports Amazon DynamoDB, Google Bigtable, Cassandra, S3 and GCS for long term storage of metric data. This allows you to durably store data for longer than the lifetime of any single machine, and use this data for long term capacity planning. Cortex is a CNCF sandbox project used in several production systems including Weave Cloud and Grafana Cloud. Cortex is primarily used as a remote write destination for Prometheus, exposing a Prometheus-compatible query API.\nDocumentation Read the getting started guide if you\u0026rsquo;re new to the project. Before deploying Cortex with a permanent storage backend you should read:\n An overview of Cortex\u0026rsquo;s architecture A general guide to running Cortex Information regarding configuring Cortex For a guide to contributing to Cortex, see the contributor guidelines.\nFurther reading To learn more about Cortex, consult the following documents \u0026amp; talks:\n May 2019 KubeCon talks; \u0026ldquo;Cortex: Intro\u0026rdquo; (video, slides, blog post) and \u0026ldquo;Cortex: Deep Dive\u0026rdquo; (video, slides) Feb 2019 blog post \u0026amp; podcast; \u0026ldquo;Prometheus Scalability with Bryan Boreham\u0026rdquo; (podcast) Feb 2019 blog post; \u0026ldquo;How Aspen Mesh Runs Cortex in Production\u0026ldquo; Dec 2018 KubeCon talk; \u0026ldquo;Cortex: Infinitely Scalable Prometheus\u0026rdquo; (video, slides) Dec 2018 CNCF blog post; \u0026ldquo;Cortex: a multi-tenant, horizontally scalable Prometheus-as-a-Service\u0026ldquo; Nov 2018 CloudNative London meetup talk; \u0026ldquo;Cortex: Horizontally Scalable, Highly Available Prometheus\u0026rdquo; (slides) Nov 2018 CNCF TOC Presentation; \u0026ldquo;Horizontally Scalable, Multi-tenant Prometheus\u0026rdquo; (slides) Sept 2018 blog post; \u0026ldquo;What is Cortex?\u0026ldquo; Aug 2018 PromCon panel; \u0026ldquo;Prometheus Long-Term Storage Approaches\u0026rdquo; (video) Jul 2018 design doc; \u0026ldquo;Cortex Query Optimisations\u0026ldquo; Aug 2017 PromCon talk; \u0026ldquo;Cortex: Prometheus as a Service, One Year On\u0026rdquo; (videos, slides, write up part 1, part 2, part 3) Jun 2017 Prometheus London meetup talk; \u0026ldquo;Cortex: open-source, horizontally-scalable, distributed Prometheus\u0026rdquo; (video) Dec 2016 KubeCon talk; \u0026ldquo;Weave Cortex: Multi-tenant, horizontally scalable Prometheus as a Service\u0026rdquo; (video, slides) Aug 2016 PromCon talk; \u0026ldquo;Project Frankenstein: Multitenant, Scale-Out Prometheus\u0026rdquo;: (video, slides) Jun 2016 design document; \u0026ldquo;Project Frankenstein: A Multi Tenant, Scale Out Prometheus\u0026ldquo; Getting Help If you have any questions about Cortex:\n Ask a question on the Cortex Slack channel. To invite yourself to the CNCF Slack, visit http://slack.cncf.io/. File an issue. Send an email to cortex-users@lists.cncf.io Your feedback is always welcome.\nHosted Cortex (Prometheus as a service) There are several commercial services where you can use Cortex on-demand:\nWeave Cloud Weave Cloud from Weaveworks lets you deploy, manage, and monitor container-based applications. Sign up at https://cloud.weave.works and follow the instructions there. Additional help can also be found in the Weave Cloud documentation.\nInstrumenting Your App: Best Practices\nGrafana Cloud To use Cortex as part of Grafana Cloud, sign up for Grafana Cloud by clicking \u0026ldquo;Log In\u0026rdquo; in the top right and then \u0026ldquo;Sign Up Now\u0026rdquo;. Cortex is included as part of the Starter and Basic Hosted Grafana plans.\n","excerpt":"Cortex provides horizontally scalable, highly available, multi-tenant, long term storage for …","ref":"/docs/","title":"Documentation"},{"body":" This document assumes you have read the architecture document.\nIn addition to the general advice in this document, please see these platform-specific notes:\n AWS Planning Tenants If you will run Cortex as a multi-tenant system, you need to give each tenant a unique ID - this can be any string. Managing tenants and allocating IDs must be done outside of Cortex. You must also configure Authentication and Authorisation.\nStorage Cortex requires a scalable storage back-end. Commercial cloud options are DynamoDB and Bigtable: the advantage is you don\u0026rsquo;t have to know how to manage them, but the downside is they have specific costs. Alternatively you can choose Cassandra, which you will have to install and manage.\nComponents Every Cortex installation will need Distributor, Ingester and Querier. Alertmanager, Ruler and Query-frontend are optional.\nOther dependencies Cortex needs a KV store to track sharding of data between processes. This can be either Etcd or Consul.\nIf you want to configure recording and alerting rules (i.e. if you will run the Ruler and Alertmanager components) then a Postgres database is required to store configs.\nMemcached is not essential but highly recommended.\nIngester replication factor The standard replication factor is three, so that we can drop one replica and be unconcerned, as we still have two copies of the data left for redundancy. This is configurable: you can run with more redundancy or less, depending on your risk appetite.\nSchema See schema config file docs.\nChunk encoding Standard choice would be Bigchunk, which is the most flexible chunk encoding. You may get better compression from Varbit, if many of your timeseries do not change value from one day to the next.\nSizing You will want to estimate how many nodes are required, how many of each component to run, and how much storage space will be required. In practice, these will vary greatly depending on the metrics being sent to Cortex.\nSome key parameters are:\n The number of active series. If you have Prometheus already you can query prometheus_tsdb_head_series to see this number. Sampling rate, e.g. a new sample for each series every 15 seconds. Multiply this by the number of active series to get the total rate at which samples will arrive at Cortex. The rate at which series are added and removed. This can be very high if you monitor objects that come and go - for example if you run thousands of batch jobs lasting a minute or so and capture metrics with a unique ID for each one. Read how to analyse this on Prometheus How compressible the time-series data are. If a metric stays at the same value constantly, then Cortex can compress it very well, so 12 hours of data sampled every 15 seconds would be around 2KB. On the other hand if the value jumps around a lot it might take 10KB. There are not currently any tools available to analyse this. How long you want to retain data for, e.g. 1 month or 2 years. Other parameters which can become important if you have particularly high values:\n Number of different series under one metric name. Number of labels per series. Rate and complexity of queries. Now, some rules of thumb:\n Each million series in an ingester takes 15GB of RAM. Total number of series in ingesters is number of active series times the replication factor. This is with the default of 12-hour chunks - RAM required will reduce if you set -ingester.max-chunk-age lower (trading off more back-end database IO) Each million series (including churn) consumes 15GB of chunk storage and 4GB of index, per day (so multiply by the retention period). Each 100,000 samples/sec arriving takes 1 CPU in distributors. Distributors don\u0026rsquo;t need much RAM. If you turn on compression between distributors and ingesters (for example to save on inter-zone bandwidth charges at AWS) they will use significantly more CPU (approx 100% more for distributor and 50% more for ingester).\nCaching Correctly configured caching is important for a production-ready Cortex cluster.\nSee Caching In Cortex for more information.\nOrchestration Because Cortex is designed to run multiple instances of each component (ingester, querier, etc.), you probably want to automate the placement and shepherding of these instances. Most users choose Kubernetes to do this, but this is not mandatory.\nConfiguration Resource requests If using Kubernetes, each container should specify resource requests so that the scheduler can place them on a node with sufficient capacity.\nFor example an ingester might request:\n resources: requests: cpu: 4 memory: 10Gi The specific values here should be adjusted based on your own experiences running Cortex - they are very dependent on rate of data arriving and other factors such as series churn.\nTake extra care with ingesters Ingesters hold hours of timeseries data in memory; you can configure Cortex to replicate the data but you should take steps to avoid losing all replicas at once: - Don\u0026rsquo;t run multiple ingesters on the same node. - Don\u0026rsquo;t run ingesters on preemptible/spot nodes. - Spread out ingesters across racks / availability zones / whatever applies in your datacenters.\nYou can ask Kubernetes to avoid running on the same node like this:\n affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: name operator: In values: - ingester topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; Give plenty of time for an ingester to hand over or flush data to store when shutting down; for Kubernetes this looks like:\n terminationGracePeriodSeconds: 2400 Ask Kubernetes to limit rolling updates to one ingester at a time, and signal the old one to stop before the new one is ready:\n strategy: rollingUpdate: maxSurge: 0 maxUnavailable: 1 Ingesters provide an http hook to signal readiness when all is well; this is valuable because it stops a rolling update at the first problem:\n readinessProbe: httpGet: path: /ready port: 80 We do not recommend configuring a liveness probe on ingesters - killing them is a last resort and should not be left to a machine.\nRemote writing Prometheus To configure your Prometheus instances for remote writes take a look at the Prometheus Remote Write Config. We recommend to tune the following parameters of the queue_config:\nremote_write:-queue_config:capacity:5000max_shards:20min_shards:5max_samples_per_send:1000 Please take note that these values are tweaked for our use cases and may be necessary to adapt depending on your workload. Take a look at the remote write tuning docs.\nIf you experience a rather high delay for your metrics to appear in Cortex (15s+) you can try increasing the min_shards in your remote write config. Sometimes Prometheus does not increase the number of shards even though it hasn\u0026rsquo;t caught up the lag. You can monitor the delay with this Prometheus query:\ntime() - sum by (statefulset_kubernetes_io_pod_name) (prometheus_remote_storage_queue_highest_sent_timestamp_seconds) Optimising Optimising Storage These ingester options reduce the chance of storing multiple copies of the same data:\n -ingester.spread-flushes=true -ingester.chunk-age-jitter=0 Add a chunk cache via -memcached.hostname to allow writes to be de-duplicated.\nAs recommended under Chunk encoding, use Bigchunk:\n -ingester.chunk-encoding=3 # bigchunk ","excerpt":"This document assumes you have read the architecture document.\nIn addition to the general advice in …","ref":"/docs/guides/running-in-production/","title":"Running Cortex in Production"},{"body":" Cortex uses a NoSQL Store to store its index and optionally an Object store to store its chunks. Cortex has overtime evolved its schema to be more optimal and better fit the use cases and query patterns that arose.\nCurrently there are 9 schemas that are used in production but we recommend running with v9 schema when possible. You can move from one schema to another if a new schema fits your purpose better, but you still need to configure Cortex to make sure it can read the old data in the old schemas.\nYou can configure the schemas using a YAML config file, that you can point to using the -schema-config-file flag. It has the following YAML spec:\nconfigs:[]\u0026lt;period_config\u0026gt; Where period_config is\n# In YYYY-MM-DD format, for example: 2020-03-01. from: \u0026lt;string\u0026gt; # The index client to use, valid options: aws-dynamo, bigtable, bigtable-hashed, cassandra, boltdb. store: \u0026lt;string\u0026gt; # The object client to use. If none is specified, `store` is used for storing chunks as well. Valid options: s3, aws-dynamo, bigtable, bigtable-hashed, gcs, cassandra, filesystem. object_store: \u0026lt;string\u0026gt; # The schema version to use. Valid ones are v1, v2, v3,... v6, v9, v10, v11. Recommended for production: v9. schema: \u0026lt;string\u0026gt; index: \u0026lt;periodic_table_config\u0026gt; chunks: \u0026lt;periodic_table_config\u0026gt; Where periodic_table_config is\n# The prefix to use for the tables. prefix: \u0026lt;string\u0026gt; # We typically run Cortex with new tables every week to keep the index size low and to make retention easier. This sets the period at which new tables are created and used. Typically 168h (1week). period: \u0026lt;duration\u0026gt; # The tags that can be set on the dynamo table. tags: \u0026lt;map[string]string\u0026gt; Now an example of this file (also something recommended when starting out) is:\nconfigs: - from: \u0026quot;2020-03-01\u0026quot; # Or typically a week before the Cortex cluster was created. schema: v9 index: period: 168h prefix: cortex_index_ # Chunks section is optional and required only if you're not using a # separate object store. chunks: period: 168h prefix: cortex_chunks store: aws-dynamo/bigtable-hashed/cassandra/boltdb object_store: \u0026lt;above options\u0026gt;/s3/gcs/azure/filesystem An example of an advanced schema file with a lot of changes:\nconfigs: # Starting from 2018-08-23 Cortex should store chunks and indexes # on Google BigTable using weekly periodic tables. The chunks table # names will be prefixed with \u0026quot;dev_chunks_\u0026quot;, while index tables will be # prefixed with \u0026quot;dev_index_\u0026quot;. - from: \u0026quot;2018-08-23\u0026quot; schema: v9 chunks: period: 168h0m0s prefix: dev_chunks_ index: period: 168h0m0s prefix: dev_index_ store: gcp-columnkey # Starting 2018-02-13 we moved from BigTable to GCS for storing the chunks. - from: \u0026quot;2019-02-13\u0026quot; schema: v9 chunks: period: 168h prefix: dev_chunks_ index: period: 168h prefix: dev_index_ object_store: gcs store: gcp-columnkey # Starting 2019-02-24 we moved our index from bigtable-columnkey to bigtable-hashed # which improves the distribution of keys. - from: \u0026quot;2019-02-24\u0026quot; schema: v9 chunks: period: 168h prefix: dev_chunks_ index: period: 168h prefix: dev_index_ object_store: gcs store: bigtable-hashed # Starting 2019-03-05 we moved from v9 schema to v10 schema. - from: \u0026quot;2019-03-05\u0026quot; schema: v10 chunks: period: 168h prefix: dev_chunks_ index: period: 168h prefix: dev_index_ object_store: gcs store: bigtable-hashed Note how we started out with v9 and just Bigtable, but later migrated to GCS as the object store, finally moving to v10. This is a complex schema file showing several changes changes over the time, while a typical schema config file usually has just one or two schema versions.\nMigrating from flags to schema file Legacy versions of Cortex did support the ability to configure schema via flags. If you are still using flags, you need to migrate your configuration from flags to the config file.\nIf you\u0026rsquo;re using:\n chunk.storage-client: then set the corresponding object_store field correctly in the schema file. dynamodb.daily-buckets-from: then set the corresponding from date with v2 schema. dynamodb.base64-buckets-from: then set the corresponding from date with v3 schema. dynamodb.v{4,5,6,9}-schema-from: then set the corresponding from date with schema v{4,5,6,9} bigtable.column-key-from: then set the corresponding from date and use the store as bigtable-columnkey. dynamodb.use-periodic-tables: then set the right index and chunk fields with corresponding values from dynamodb.periodic-table.{prefix, period, tag} and dynamodb.chunk-table.{prefix, period, tag} flags. Note that the default period is 7 days, so please set the period as 168h in the config file if none is set in the flags. ","excerpt":"Cortex uses a NoSQL Store to store its index and optionally an Object store to store its chunks. …","ref":"/docs/configuration/schema-configuration/","title":"Schema Configuration"},{"body":"All Cortex components take the tenant ID from a header X-Scope-OrgID on each request. They trust this value completely: if you need to protect your Cortex installation from accidental or malicious calls then you must add an additional layer of protection.\nTypically this means you run Cortex behind a reverse proxy, and ensure that all callers, both machines sending data over the remote_write interface and humans sending queries from GUIs, supply credentials which identify them and confirm they are authorised.\nWhen configuring the remote_write API in Prometheus there is no way to add extra headers. The user and password fields of http Basic auth, or Bearer token, can be used to convey tenant ID and/or credentials.\n","excerpt":"All Cortex components take the tenant ID from a header X-Scope-OrgID on each request. They trust …","ref":"/docs/guides/auth/","title":"Authentication and Authorisation"},{"body":" Cortex can be run as a single binary or as multiple independent microservices. The single-binary mode is easier to deploy and is aimed mainly at users wanting to try out Cortex or develop on it. The microservices mode is intended for production usage, as it allows you to independently scale different services and isolate failures.\nThis document will focus on single-process Cortex with the experimental blocks storage. See the architecture doc for more information about the microservices and blocks operation for more information about the blocks storage.\nSeparately from single process vs microservices decision, Cortex can be configured to use local storage or cloud storage (S3, GCS and Azure). This document will focus on using S3. Cortex can also make use of external memcacheds and redis for caching but this feature is not available (yet) using block storage.\nSingle instance, single process For simplicity and to get started, we\u0026rsquo;ll run it as a single process with no dependencies:\n$ go build ./cmd/cortex $ ./cortex -config.file=./docs/configuration/single-process-config-blocks.yaml This starts a single Cortex node storing blocks to S3 in bucket cortex. It is not intended for production use.\nClone and build prometheus\n$ git clone https://github.com/prometheus/prometheus $ cd prometheus $ go build ./cmd/prometheus Add the following to your Prometheus config (documentation/examples/prometheus.yml in Prometheus repo):\nremote_write:-url:http://localhost:9009/api/prom/push And start Prometheus with that config file:\n$ ./prometheus --config.file=./documentation/examples/prometheus.yml Your Prometheus instance will now start pushing data to Cortex. To query that data, start a Grafana instance:\n$ docker run --rm -d --name=grafana -p 3000:3000 grafana/grafana In the Grafana UI (username/password admin/admin), add a Prometheus datasource for Cortex (http://host.docker.internal:9009/api/prom).\nTo clean up: press CTRL-C in both terminals (for Cortex and Promrtheus).\nHorizontally scale out Next we\u0026rsquo;re going to show how you can run a scale out Cortex cluster using Docker. We\u0026rsquo;ll need:\n A built Cortex image. A Docker network to put these containers on so they can resolve each other by name. A single node Consul instance to coordinate the Cortex cluster.\n$ make ./cmd/cortex/.uptodate $ docker network create cortex $ docker run -d --name=consul --network=cortex -e CONSUL_BIND_INTERFACE=eth0 consul Next we\u0026rsquo;ll run a couple of Cortex instances pointed at that Consul. You\u0026rsquo;ll note the Cortex configuration can be specified in either a config file or overridden on the command line. See the arguments documentation for more information about Cortex configuration options.\n$ docker run -d --name=cortex1 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config-blocks.yaml:/etc/single-process-config-blocks.yaml \\ -p 9001:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config-blocks.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 $ docker run -d --name=cortex2 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config-blocks.yaml:/etc/single-process-config-blocks.yaml \\ -p 9002:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config-blocks.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 If you go to http://localhost:9001/ring (or http://localhost:9002/ring) you should see both Cortex nodes join the ring.\nTo demonstrate the correct operation of Cortex clustering, we\u0026rsquo;ll send samples to one of the instances and queries to another. In production, you\u0026rsquo;d want to load balance both pushes and queries evenly among all the nodes.\nPoint Prometheus at the first:\nremote_write:-url:http://localhost:9001/api/prom/push$ ./prometheus --config.file=./documentation/examples/prometheus.yml And Grafana at the second:\n$ docker run -d --name=grafana --network=cortex -p 3000:3000 grafana/grafana In the Grafana UI (username/password admin/admin), add a Prometheus datasource for Cortex (http://cortex2:9009/api/prom).\nTo clean up: CTRL-C the Prometheus process and run:\n$ docker rm -f cortex1 cortex2 consul grafana $ docker network remove cortex High availability with replication In this last demo we\u0026rsquo;ll show how Cortex can replicate data among three nodes, and demonstrate Cortex can tolerate a node failure without affecting reads and writes.\nFirst, create a network and run a new Consul and Grafana:\n$ docker network create cortex $ docker run -d --name=consul --network=cortex -e CONSUL_BIND_INTERFACE=eth0 consul $ docker run -d --name=grafana --network=cortex -p 3000:3000 grafana/grafana Then, launch 3 Cortex nodes with replication factor 3:\n$ docker run -d --name=cortex1 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config-blocks.yaml:/etc/single-process-config-blocks.yaml \\ -p 9001:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config-blocks.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 \\ -distributor.replication-factor=3 $ docker run -d --name=cortex2 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config-blocks.yaml:/etc/single-process-config-blocks.yaml \\ -p 9002:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config-blocks.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 \\ -distributor.replication-factor=3 $ docker run -d --name=cortex3 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config-blocks.yaml:/etc/single-process-config-blocks.yaml \\ -p 9003:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config-blocks.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 \\ -distributor.replication-factor=3 Configure Prometheus to send data to the first replica:\nremote_write:-url:http://localhost:9001/api/prom/push$ ./prometheus --config.file=./documentation/examples/prometheus.yml In Grafana, add a datasource for the 3rd Cortex replica (http://cortex3:9009/api/prom) and verify the same data appears in both Prometheus and Cortex.\nTo show that Cortex can tolerate a node failure, hard kill one of the Cortex replicas:\n$ docker rm -f cortex2 You should see writes and queries continue to work without error.\nTo clean up: CTRL-C the Prometheus process and run:\n$ docker rm -f cortex1 cortex2 cortex3 consul grafana $ docker network remove cortex ","excerpt":"Cortex can be run as a single binary or as multiple independent microservices. The single-binary …","ref":"/docs/getting-started/getting-started-blocks-storage/","title":"Getting Started with Blocks Storage (experimental)"},{"body":" General Notes Cortex has evolved over several years, and the command-line options sometimes reflect this heritage. In some cases the default value for options is not the recommended value, and in some cases names do not reflect the true meaning. We do intend to clean this up, but it requires a lot of care to avoid breaking existing installations. In the meantime we regret the inconvenience.\nDuration arguments should be specified with a unit like 5s or 3h. Valid time units are \u0026ldquo;ms\u0026rdquo;, \u0026ldquo;s\u0026rdquo;, \u0026ldquo;m\u0026rdquo;, \u0026ldquo;h\u0026rdquo;.\nQuerier -querier.max-concurrent The maximum number of top-level PromQL queries that will execute at the same time, per querier process. If using the query frontend, this should be set to at least (querier.worker-parallelism * number of query frontend replicas). Otherwise queries may queue in the queriers and not the frontend, which will affect QoS.\n -querier.query-parallelism This refers to database queries against the store (e.g. Bigtable or DynamoDB). This is the max subqueries run in parallel per higher-level query.\n -querier.timeout The timeout for a top-level PromQL query.\n -querier.max-samples Maximum number of samples a single query can load into memory, to avoid blowing up on enormous queries.\nThe next three options only apply when the querier is used together with the Query Frontend:\n -querier.frontend-address Address of query frontend service, used by workers to find the frontend which will give them queries to execute.\n -querier.dns-lookup-period How often the workers will query DNS to re-check where the frontend is.\n -querier.worker-parallelism Number of simultaneous queries to process, per worker process. See note on -querier.max-concurrent\nQuerier and Ruler The ingester query API was improved over time, but defaults to the old behaviour for backwards-compatibility. For best results both of these next two flags should be set to true:\n -querier.batch-iterators This uses iterators to execute query, as opposed to fully materialising the series in memory, and fetches multiple results per loop.\n -querier.ingester-streaming Use streaming RPCs to query ingester, to reduce memory pressure in the ingester.\n -querier.iterators This is similar to -querier.batch-iterators but less efficient. If both iterators and batch-iterators are true, batch-iterators will take precedence.\n -promql.lookback-delta Time since the last sample after which a time series is considered stale and ignored by expression evaluations.\nQuery Frontend -querier.parallelise-shardable-queries If set to true, will cause the query frontend to mutate incoming queries when possible by turning sum operations into sharded sum operations. This requires a shard-compatible schema (v10+). An abridged example: sum by (foo) (rate(bar{baz=”blip”}[1m])) -\u0026gt;\n sum by (foo) ( sum by (foo) (rate(bar{baz=”blip”,__cortex_shard__=”0of16”}[1m])) or sum by (foo) (rate(bar{baz=”blip”,__cortex_shard__=”1of16”}[1m])) or ... sum by (foo) (rate(bar{baz=”blip”,__cortex_shard__=”15of16”}[1m])) ) When enabled, the query-frontend requires a schema config to determine how/when to shard queries, either from a file or from flags (i.e. by the config-yaml CLI flag). This is the same schema config the queriers consume. It\u0026rsquo;s also advised to increase downstream concurrency controls as well to account for more queries of smaller sizes:\n querier.max-outstanding-requests-per-tenant querier.max-query-parallelism querier.max-concurrent server.grpc-max-concurrent-streams (for both query-frontends and queriers) Furthermore, both querier and query-frontend components require the querier.query-ingesters-within parameter to know when to start sharding requests (ingester queries are not sharded). It\u0026rsquo;s recommended to align this with ingester.max-chunk-age.\nInstrumentation (traces) also scale with the number of sharded queries and it\u0026rsquo;s suggested to account for increased throughput there as well (for instance via JAEGER_REPORTER_MAX_QUEUE_SIZE).\n -querier.align-querier-with-step If set to true, will cause the query frontend to mutate incoming queries and align their start and end parameters to the step parameter of the query. This improves the cacheability of the query results.\n -querier.split-queries-by-day If set to true, will cause the query frontend to split multi-day queries into multiple single-day queries and execute them in parallel.\n -querier.cache-results If set to true, will cause the querier to cache query results. The cache will be used to answer future, overlapping queries. The query frontend calculates extra queries required to fill gaps in the cache.\n -frontend.max-cache-freshness When caching query results, it is desirable to prevent the caching of very recent results that might still be in flux. Use this parameter to configure the age of results that should be excluded.\n -memcached.{hostname, service, timeout} Use these flags to specify the location and timeout of the memcached cluster used to cache query results.\n -redis.{endpoint, timeout} Use these flags to specify the location and timeout of the Redis service used to cache query results.\nDistributor -distributor.shard-by-all-labels In the original Cortex design, samples were sharded amongst distributors by the combination of (userid, metric name). Sharding by metric name was designed to reduce the number of ingesters you need to hit on the read path; the downside was that you could hotspot the write path.\nIn hindsight, this seems like the wrong choice: we do many orders of magnitude more writes than reads, and ingester reads are in-memory and cheap. It seems the right thing to do is to use all the labels to shard, improving load balancing and support for very high cardinality metrics.\nSet this flag to true for the new behaviour.\nUpgrade notes: As this flag also makes all queries always read from all ingesters, the upgrade path is pretty trivial; just enable the flag. When you do enable it, you\u0026rsquo;ll see a spike in the number of active series as the writes are \u0026ldquo;reshuffled\u0026rdquo; amongst the ingesters, but over the next stale period all the old series will be flushed, and you should end up with much better load balancing. With this flag enabled in the queriers, reads will always catch all the data from all ingesters.\n -distributor.extra-query-delay This is used by a component with an embedded distributor (Querier and Ruler) to control how long to wait until sending more than the minimum amount of queries needed for a successful response.\n distributor.ha-tracker.enable-for-all-users Flag to enable, for all users, handling of samples with external labels identifying replicas in an HA Prometheus setup. This defaults to false, and is technically defined in the Distributor limits.\n distributor.ha-tracker.enable Enable the distributors HA tracker so that it can accept samples from Prometheus HA replicas gracefully (requires labels). Global (for distributors), this ensures that the necessary internal data structures for the HA handling are created. The option enable-for-all-users is still needed to enable ingestion of HA samples for all users.\n distributor.drop-label This flag can be used to specify label names that to drop during sample ingestion within the distributor and can be repeated in order to drop multiple labels.\n Ring/HA Tracker Store The KVStore client is used by both the Ring and HA Tracker. - {ring,distributor.ha-tracker}.prefix The prefix for the keys in the store. Should end with a /. For example with a prefix of foo/, the key bar would be stored under foo/bar. - {ring,distributor.ha-tracker}.store Backend storage to use for the ring (consul, etcd, inmemory, memberlist, multi).\nConsul By default these flags are used to configure Consul used for the ring. To configure Consul for the HA tracker, prefix these flags with distributor.ha-tracker.\n consul.hostname Hostname and port of Consul. consul.acltoken ACL token used to interact with Consul. consul.client-timeout HTTP timeout when talking to Consul. consul.consistent-reads Enable consistent reads to Consul. etcd By default these flags are used to configure etcd used for the ring. To configure etcd for the HA tracker, prefix these flags with distributor.ha-tracker.\n etcd.endpoints The etcd endpoints to connect to. etcd.dial-timeout The timeout for the etcd connection. etcd.max-retries The maximum number of retries to do for failed ops. memberlist (EXPERIMENTAL) Flags for configuring KV store based on memberlist library. This feature is experimental, please don\u0026rsquo;t use it yet.\n memberlist.nodename Name of the node in memberlist cluster. Defaults to hostname. memberlist.retransmit-factor Multiplication factor used when sending out messages (factor * log(N+1)). If not set, default value is used. memberlist.join Other cluster members to join. Can be specified multiple times. memberlist.abort-if-join-fails If this node fails to join memberlist cluster, abort. memberlist.left-ingesters-timeout How long to keep LEFT ingesters in the ring. Note: this is only used for gossiping, LEFT ingesters are otherwise invisible. memberlist.leave-timeout Timeout for leaving memberlist cluster. memberlist.gossip-interval How often to gossip with other cluster members. Uses memberlist LAN defaults if 0. memberlist.gossip-nodes How many nodes to gossip with in each gossip interval. Uses memberlist LAN defaults if 0. memberlist.pullpush-interval How often to use pull/push sync. Uses memberlist LAN defaults if 0. memberlist.bind-addr IP address to listen on for gossip messages. Multiple addresses may be specified. Defaults to 0.0.0.0. memberlist.bind-port Port to listen on for gossip messages. Defaults to 7946. memberlist.packet-dial-timeout Timeout used when connecting to other nodes to send packet. memberlist.packet-write-timeout Timeout for writing \u0026lsquo;packet\u0026rsquo; data. memberlist.transport-debug Log debug transport messages. Note: global log.level must be at debug level as well. memberlist.gossip-to-dead-nodes-time How long to keep gossiping to the nodes that seem to be dead. After this time, dead node is removed from list of nodes. If \u0026ldquo;dead\u0026rdquo; node appears again, it will simply join the cluster again, if its name is not reused by other node in the meantime. If the name has been reused, such a reanimated node will be ignored by other members. memberlist.dead-node-reclaim-time How soon can dead\u0026rsquo;s node name be reused by a new node (using different IP). Disabled by default, name reclaim is not allowed until gossip-to-dead-nodes-time expires. This can be useful to set to low numbers when reusing node names, eg. in stateful sets. If memberlist library detects that new node is trying to reuse the name of previous node, it will log message like this: Conflicting address for ingester-6. Mine: 10.44.12.251:7946 Theirs: 10.44.12.54:7946 Old state: 2. Node states are: \u0026ldquo;alive\u0026rdquo; = 0, \u0026ldquo;suspect\u0026rdquo; = 1 (doesn\u0026rsquo;t respond, will be marked as dead if it doesn\u0026rsquo;t respond), \u0026ldquo;dead\u0026rdquo; = 2. Multi KV This is a special key-value implementation that uses two different KV stores (eg. consul, etcd or memberlist). One of them is always marked as primary, and all reads and writes go to primary store. Other one, secondary, is only used for writes. The idea is that operator can use multi KV store to migrate from primary to secondary store in runtime.\nFor example, migration from Consul to Etcd would look like this:\n Set ring.store to use multi store. Set -multi.primary=consul and -multi.secondary=etcd. All consul and etcd settings must still be specified. Start all Cortex microservices. They will still use Consul as primary KV, but they will also write share ring via etcd. Operator can now use \u0026ldquo;runtime config\u0026rdquo; mechanism to switch primary store to etcd. After all Cortex microservices have picked up new primary store, and everything looks correct, operator can now shut down Consul, and modify Cortex configuration to use -ring.store=etcd only. At this point, Consul can be shut down. Multi KV has following parameters:\n multi.primary - name of primary KV store. Same values as in ring.store are supported, except multi. multi.secondary - name of secondary KV store. multi.mirror-enabled - enable mirroring of values to secondary store, defaults to true multi.mirror-timeout - wait max this time to write to secondary store to finish. Default to 2 seconds. Errors writing to secondary store are not reported to caller, but are logged and also reported via cortex_multikv_mirror_write_errors_total metric. Multi KV also reacts on changes done via runtime configuration. It uses this section:\nmulti_kv_config:mirror-enabled:falseprimary:memberlist Note that runtime configuration values take precedence over command line options.\nHA Tracker HA tracking has two of its own flags: - distributor.ha-tracker.cluster Prometheus label to look for in samples to identify a Prometheus HA cluster. (default \u0026ldquo;cluster\u0026rdquo;) - distributor.ha-tracker.replica Prometheus label to look for in samples to identify a Prometheus HA replica. (default \u0026ldquo;__replica__\u0026rdquo;)\nIt\u0026rsquo;s reasonable to assume people probably already have a cluster label, or something similar. If not, they should add one along with __replica__ via external labels in their Prometheus config. If you stick to these default values your Prometheus config could look like this (POD_NAME is an environment variable which must be set by you):\nglobal:external_labels:cluster:clustername__replica__:$POD_NAME HA Tracking looks for the two labels (which can be overwritten per user)\nIt also talks to a KVStore and has it\u0026rsquo;s own copies of the same flags used by the Distributor to connect to for the ring. - distributor.ha-tracker.failover-timeout If we don\u0026rsquo;t receive any samples from the accepted replica for a cluster in this amount of time we will failover to the next replica we receive a sample from. This value must be greater than the update timeout (default 30s) - distributor.ha-tracker.store Backend storage to use for the ring (consul, etcd, inmemory). (default \u0026ldquo;consul\u0026rdquo;) - distributor.ha-tracker.update-timeout Update the timestamp in the KV store for a given cluster/replica only after this amount of time has passed since the current stored timestamp. (default 15s)\nIngester -ingester.max-chunk-age The maximum duration of a timeseries chunk in memory. If a timeseries runs for longer than this the current chunk will be flushed to the store and a new chunk created. (default 12h)\n -ingester.max-chunk-idle If a series doesn\u0026rsquo;t receive a sample for this duration, it is flushed and removed from memory.\n -ingester.max-stale-chunk-idle If a series receives a staleness marker, then we wait for this duration to get another sample before we close and flush this series, removing it from memory. You want it to be at least 2x the scrape interval as you don\u0026rsquo;t want a single failed scrape to cause a chunk flush.\n -ingester.chunk-age-jitter To reduce load on the database exactly 12 hours after starting, the age limit is reduced by a varying amount up to this. (default 20m)\n -ingester.spread-flushes Makes the ingester flush each timeseries at a specific point in the max-chunk-age cycle. This means multiple replicas of a chunk are very likely to contain the same contents which cuts chunk storage space by up to 66%. Set -ingester.chunk-age-jitter to 0 when using this option. If a chunk cache is configured (via -memcached.hostname) then duplicate chunk writes are skipped which cuts write IOPs.\n -ingester.join-after How long to wait in PENDING state during the hand-over process. (default 0s)\n -ingester.max-transfer-retries How many times a LEAVING ingester tries to find a PENDING ingester during the hand-over process. Each attempt takes a second or so. Negative value or zero disables hand-over process completely. (default 10)\n -ingester.normalise-tokens Deprecated. New ingesters always write \u0026ldquo;normalised\u0026rdquo; tokens to the ring. Normalised tokens consume less memory to encode and decode; as the ring is unmarshalled regularly, this significantly reduces memory usage of anything that watches the ring.\nCortex 0.4.0 is the last version that can write denormalised tokens. Cortex 0.5.0 and above always write normalised tokens.\nCortex 0.6.0 is the last version that can read denormalised tokens. Starting with Cortex 0.7.0 only normalised tokens are supported, and ingesters writing denormalised tokens to the ring (running Cortex 0.4.0 or earlier with -ingester.normalise-tokens=false) are ignored by distributors. Such ingesters should either switch to using normalised tokens, or be upgraded to Cortex 0.5.0 or later.\n -ingester.chunk-encoding Pick one of the encoding formats for timeseries data, which have different performance characteristics. Bigchunk uses the Prometheus V2 code, and expands in memory to arbitrary length. Varbit, Delta and DoubleDelta use Prometheus V1 code, and are fixed at 1K per chunk. Defaults to DoubleDelta, but we recommend Bigchunk.\n -store.bigchunk-size-cap-bytes When using bigchunks, start a new bigchunk and flush the old one if the old one reaches this size. Use this setting to limit memory growth of ingesters with a lot of timeseries that last for days.\n -ingester-client.expected-timeseries When push requests arrive, pre-allocate this many slots to decode them. Tune this setting to reduce memory allocations and garbage. This should match the max_samples_per_send in your queue_config for Prometheus.\n -ingester-client.expected-samples-per-series When push requests arrive, pre-allocate this many slots to decode them. Tune this setting to reduce memory allocations and garbage. Under normal conditions, Prometheus scrapes should arrive with one sample per series.\n -ingester-client.expected-labels When push requests arrive, pre-allocate this many slots to decode them. Tune this setting to reduce memory allocations and garbage. The optimum value will depend on how many labels are sent with your timeseries samples.\n -store.chunk-cache-stubs Where you don\u0026rsquo;t want to cache every chunk written by ingesters, but you do want to take advantage of chunk write deduplication, this option will make ingesters write a placeholder to the cache for each chunk. Make sure you configure ingesters with a different cache to queriers, which need the whole value.\nWAL -ingester.wal-dir Directory where the WAL data should be stored and/or recovered from.\n -ingester.wal-enabled\n Setting this to true enables writing to WAL during ingestion.\n -ingester.checkpoint-enabled Set this to true to enable checkpointing of in-memory chunks to disk. This is optional which helps in speeding up the replay process.\n -ingester.checkpoint-duration This is the interval at which checkpoints should be created.\n -ingester.recover-from-wal Set this to true to recover data from an existing WAL. The data is recovered even if WAL is disabled and this is set to true. The WAL dir needs to be set for this.\n Flusher -flusher.wal-dir Directory where the WAL data should be recovered from.\n -flusher.concurrent-flushes Number of concurrent flushes.\n -flusher.flush-op-timeout Duration after which a flush should timeout.\n Runtime Configuration file Cortex has a concept of \u0026ldquo;runtime config\u0026rdquo; file, which is simply a file that is reloaded while Cortex is running. It is used by some Cortex components to allow operator to change some aspects of Cortex configuration without restarting it. File is specified by using -runtime-config.file=\u0026lt;filename\u0026gt; flag and reload period (which defaults to 10 seconds) can be changed by -runtime-config.reload-period=\u0026lt;duration\u0026gt; flag. Previously this mechanism was only used by limits overrides, and flags were called -limits.per-user-override-config=\u0026lt;filename\u0026gt; and -limits.per-user-override-period=10s respectively. These are still used, if -runtime-config.file=\u0026lt;filename\u0026gt; is not specified.\nAt the moment, two components use runtime configuration: limits and multi KV store.\nExample runtime configuration file:\noverrides:tenant1:ingestion_rate:10000max_series_per_metric:100000max_series_per_query:100000tenant2:max_samples_per_query:1000000max_series_per_metric:100000max_series_per_query:100000multi_kv_config:mirror-enabled:falseprimary:memberlist When running Cortex on Kubernetes, store this file in a config map and mount it in each services\u0026rsquo; containers. When changing the values there is no need to restart the services, unless otherwise specified.\nIngester, Distributor \u0026amp; Querier limits. Cortex implements various limits on the requests it can process, in order to prevent a single tenant overwhelming the cluster. There are various default global limits which apply to all tenants which can be set on the command line. These limits can also be overridden on a per-tenant basis by using overrides field of runtime configuration file.\nThe overrides field is a map of tenant ID (same values as passed in the X-Scope-OrgID header) to the various limits. An example could look like:\noverrides:tenant1:ingestion_rate:10000max_series_per_metric:100000max_series_per_query:100000tenant2:max_samples_per_query:1000000max_series_per_metric:100000max_series_per_query:100000 Valid per-tenant limits are (with their corresponding flags for default values):\n ingestion_rate_strategy / -distributor.ingestion-rate-limit-strategy ingestion_rate / -distributor.ingestion-rate-limit ingestion_burst_size / -distributor.ingestion-burst-size The per-tenant rate limit (and burst size), in samples per second. It supports two strategies: local (default) and global.\nThe local strategy enforces the limit on a per distributor basis, actual effective rate limit will be N times higher, where N is the number of distributor replicas.\nThe global strategy enforces the limit globally, configuring a per-distributor local rate limiter as ingestion_rate / N, where N is the number of distributor replicas (it\u0026rsquo;s automatically adjusted if the number of replicas change). The ingestion_burst_size refers to the per-distributor local rate limiter (even in the case of the global strategy) and should be set at least to the maximum number of samples expected in a single push request. For this reason, the global strategy requires that push requests are evenly distributed across the pool of distributors; if you use a load balancer in front of the distributors you should be already covered, while if you have a custom setup (ie. an authentication gateway in front) make sure traffic is evenly balanced across distributors.\nThe global strategy requires the distributors to form their own ring, which is used to keep track of the current number of healthy distributor replicas. The ring is configured by distributor: { ring: {}} / -distributor.ring.*.\n max_label_name_length / -validation.max-length-label-name max_label_value_length / -validation.max-length-label-value max_label_names_per_series / -validation.max-label-names-per-series Also enforced by the distributor, limits on the on length of labels and their values, and the total number of labels allowed per series.\n reject_old_samples / -validation.reject-old-samples reject_old_samples_max_age / -validation.reject-old-samples.max-age creation_grace_period / -validation.create-grace-period Also enforce by the distributor, limits on how far in the past (and future) timestamps that we accept can be.\n max_series_per_user / -ingester.max-series-per-user max_series_per_metric / -ingester.max-series-per-metric Enforced by the ingesters; limits the number of active series a user (or a given metric) can have. When running with -distributor.shard-by-all-labels=false (the default), this limit will enforce the maximum number of series a metric can have \u0026lsquo;globally\u0026rsquo;, as all series for a single metric will be sent to the same replication set of ingesters. This is not the case when running with -distributor.shard-by-all-labels=true, so the actual limit will be N/RF times higher, where N is number of ingester replicas and RF is configured replication factor.\nAn active series is a series to which a sample has been written in the last -ingester.max-chunk-idle duration, which defaults to 5 minutes.\n max_global_series_per_user / -ingester.max-global-series-per-user max_global_series_per_metric / -ingester.max-global-series-per-metric Like max_series_per_user and max_series_per_metric, but the limit is enforced across the cluster. Each ingester is configured with a local limit based on the replication factor, the -distributor.shard-by-all-labels setting and the current number of healthy ingesters, and is kept updated whenever the number of ingesters change.\nRequires -distributor.replication-factor and -distributor.shard-by-all-labels set for the ingesters too.\n max_series_per_query / -ingester.max-series-per-query max_samples_per_query / -ingester.max-samples-per-query Limits on the number of timeseries and samples returns by a single ingester during a query.\nStorage s3.force-path-style Set this to true to force the request to use path-style addressing (http://s3.amazonaws.com/BUCKET/KEY). By default, the S3 client will use virtual hosted bucket addressing when possible (http://BUCKET.s3.amazonaws.com/KEY).\n","excerpt":"General Notes Cortex has evolved over several years, and the command-line options sometimes reflect …","ref":"/docs/configuration/arguments/","title":"Cortex Arguments"},{"body":"New maintainers are proposed by an existing maintainer and are elected by majority vote. Once the vote passed, the following steps should be done to add a new member to the maintainers team:\n Submit a PR to add the new member to MAINTAINERS Invite to GitHub organization Invite to cortex-team group Invite to Quay.io repository Invite to Docker Hub organization Invite to CNCF cncf-cortex-maintainers mailing list (via CNCF Service Desk) ","excerpt":"New maintainers are proposed by an existing maintainer and are elected by majority vote. Once the …","ref":"/docs/governance/how-to-add-a-maintainer/","title":"How to add a maintainer"},{"body":" The Cortex documentation is compiled into a website published at cortexmetrics.io. These instructions explain how to run the website locally, in order to have a quick feedback loop while contributing to the documentation or website styling.\nInitial setup The following initial setup is required only once:\n Install Hugo v0.59.1 Install Node.js v12 or above (alternatively via nvm) Install required Node modules with:\ncd website \u0026amp;\u0026amp; npm install postcss-cli autoprefixer \u0026amp;\u0026amp; cd - Run make BUILD_IN_CONTAINER=false web-build\n Run it Once the initial setup is completed, you can run the website with the following command. The local website will run at http://localhost:1313/\n# Keep this running make web-serve Whenever you change the content of docs/ or markdown files in the repository root / you should run:\nmake BUILD_IN_CONTAINER=false web-pre Whenever you change the config file or CLI flags in the Cortex code, you should rebuild the config file reference documentation:\nmake BUILD_IN_CONTAINER=false doc web-pre","excerpt":"The Cortex documentation is compiled into a website published at cortexmetrics.io. These …","ref":"/docs/contributing/how-to-run-the-website-locally/","title":"How to run the website locally"},{"body":" The query auditor is a tool bundled in the Cortex repository, but not included in Docker images \u0026ndash; this must be built from source. It\u0026rsquo;s primarily useful for those developing Cortex, but can be helpful to operators as well during certain scenarios (backend migrations come to mind).\nHow it works The query-audit tool performs a set of queries against two backends that expose the Prometheus read API. This is generally the query-frontend component of two Cortex deployments. It will then compare the differences in the responses to determine the average difference for each query. It does this by:\n Ensuring the resulting label sets match. For each label set, ensuring they contain the same number of samples as their pair from the other backend. For each sample, calculates their difference against it\u0026rsquo;s pair from the other backend/label set. Calculates the average diff per query from the above diffs. Limitations It currently only supports queries with Matrix response types.\nUse cases Correctness testing when working on the read path. Comparing results from different backends. Example Configuration control:host:http://localhost:8080/api/promheaders:\u0026#34;X-Scope-OrgID\u0026#34;:1234test:host:http://localhost:8081/api/promheaders:\u0026#34;X-Scope-OrgID\u0026#34;:1234queries:-query:\u0026#39;sum(rate(container_cpu_usage_seconds_total[5m]))\u0026#39;start:2019-11-25T00:00:00Zend:2019-11-28T00:00:00Zstep_size:15m-query:\u0026#39;sum(rate(container_cpu_usage_seconds_total[5m])) by (container_name)\u0026#39;start:2019-11-25T00:00:00Zend:2019-11-28T00:00:00Zstep_size:15m-query:\u0026#39;sum(rate(container_cpu_usage_seconds_total[5m])) without (container_name)\u0026#39;start:2019-11-25T00:00:00Zend:2019-11-26T00:00:00Zstep_size:15m-query:\u0026#39;histogram_quantile(0.9, sum(rate(cortex_cache_value_size_bytes_bucket[5m])) by (le, job))\u0026#39;start:2019-11-25T00:00:00Zend:2019-11-25T06:00:00Zstep_size:15m# two shardable legs-query:\u0026#39;sum without (instance, job) (rate(cortex_query_frontend_queue_length[5m])) or sum by (job) (rate(cortex_query_frontend_queue_length[5m]))\u0026#39;start:2019-11-25T00:00:00Zend:2019-11-25T06:00:00Zstep_size:15m# one shardable leg-query:\u0026#39;sum without (instance, job) (rate(cortex_cache_request_duration_seconds_count[5m])) or rate(cortex_cache_request_duration_seconds_count[5m])\u0026#39;start:2019-11-25T00:00:00Zend:2019-11-25T06:00:00Zstep_size:15m Example Output Under ideal circumstances, you\u0026rsquo;ll see output like the following:\n$ go run ./tools/query-audit/ -f config.yaml 0.000000% avg diff for: query: sum(rate(container_cpu_usage_seconds_total[5m])) series: 1 samples: 289 start: 2019-11-25 00:00:00 +0000 UTC end: 2019-11-28 00:00:00 +0000 UTC step: 15m0s 0.000000% avg diff for: query: sum(rate(container_cpu_usage_seconds_total[5m])) by (container_name) series: 95 samples: 25877 start: 2019-11-25 00:00:00 +0000 UTC end: 2019-11-28 00:00:00 +0000 UTC step: 15m0s 0.000000% avg diff for: query: sum(rate(container_cpu_usage_seconds_total[5m])) without (container_name) series: 4308 samples: 374989 start: 2019-11-25 00:00:00 +0000 UTC end: 2019-11-26 00:00:00 +0000 UTC step: 15m0s 0.000000% avg diff for: query: histogram_quantile(0.9, sum(rate(cortex_cache_value_size_bytes_bucket[5m])) by (le, job)) series: 13 samples: 325 start: 2019-11-25 00:00:00 +0000 UTC end: 2019-11-25 06:00:00 +0000 UTC step: 15m0s 0.000000% avg diff for: query: sum without (instance, job) (rate(cortex_query_frontend_queue_length[5m])) or sum by (job) (rate(cortex_query_frontend_queue_length[5m])) series: 21 samples: 525 start: 2019-11-25 00:00:00 +0000 UTC end: 2019-11-25 06:00:00 +0000 UTC step: 15m0s 0.000000% avg diff for: query: sum without (instance, job) (rate(cortex_cache_request_duration_seconds_count[5m])) or rate(cortex_cache_request_duration_seconds_count[5m]) series: 942 samples: 23550 start: 2019-11-25 00:00:00 +0000 UTC end: 2019-11-25 06:00:00 +0000 UTC step: 15m0s 0.000000% avg diff for: query: sum by (namespace) (predict_linear(container_cpu_usage_seconds_total[5m], 10)) series: 16 samples: 400 start: 2019-11-25 00:00:00 +0000 UTC end: 2019-11-25 06:00:00 +0000 UTC step: 15m0s 0.000000% avg diff for: query: sum by (namespace) (avg_over_time((rate(container_cpu_usage_seconds_total[5m]))[10m:]) \u0026gt; 1) series: 4 samples: 52 start: 2019-11-25 00:00:00 +0000 UTC end: 2019-11-25 01:00:00 +0000 UTC step: 5m0s ","excerpt":"The query auditor is a tool bundled in the Cortex repository, but not included in Docker images …","ref":"/docs/operations/query-auditor/","title":"Query Auditor (tool)"},{"body":"","excerpt":"","ref":"/docs/getting-started/","title":"Getting Started"},{"body":"To upgrade the Golang version:\n Upgrade build image version Upgrade Golang version in build-image/Dockerfile Build new image make build-image/.uptodate Publish the new image to quay.io (requires a maintainer) Update the Docker image tag in .circleci/config.yml Upgrade integration tests version Update the Golang version installed in the integration job in .circleci/config.yml If the minimum support Golang version should be upgraded as well:\n Upgrade go version in go.mod ","excerpt":"To upgrade the Golang version:\n Upgrade build image version Upgrade Golang version in …","ref":"/docs/contributing/how-to-upgrade-golang-version/","title":"How to upgrade Golang version"},{"body":"You can use the Cortex query frontend with any Prometheus-API compatible service, including Prometheus and Thanos. Use this config file to get the benefits of query parallelisation and caching.\n# Disable the requirement that every request to Cortex has a# X-Scope-OrgID header. `fake` will be substituted in instead.auth_enabled:false# We only want to run the query-frontend module.target:query-frontend# We don\u0026#39;t want the usual /api/prom prefix.http_prefix:server:http_listen_port:9091query_range:split_queries_by_day:truealign_queries_with_step:truecache_results:trueresults_cache:max_freshness:1mcache:# We\u0026#39;re going to use the in-process \u0026#34;FIFO\u0026#34; cache, but you can enable# memcached below.enable_fifocache:truefifocache:size:1024validity:24h# If you want to use a memcached cluster, configure a headless service# in Kubernetes and Cortex will discover the individual instances using# a SRV DNS query. Cortex will then do client-side hashing to spread# the load evenly.# memcached:# memcached_client:# host: memcached.default.svc.cluster.local# service: memcached# consistent_hash: truefrontend:log_queries_longer_than:1scompress_responses:true","excerpt":"You can use the Cortex query frontend with any Prometheus-API compatible service, including …","ref":"/docs/configuration/prometheus-frontend/","title":"Prometheus Frontend"},{"body":" The query-tee is a standalone service which can be used for testing purposes to compare the query performances of 2+ backend systems (ie. Cortex clusters) ingesting the same exact series.\nThis service exposes Prometheus-compatible read API endpoints and, for each received request, performs the request against all backends tracking the response time of each backend and then sends back to the client one of the received responses.\nHow to run it You can run query-tee in two ways:\n Build it from sources\ngo run ./cmd/query-tee -help Run it via the provided Docker image\ndocker run quay.io/cortexproject/query-tee -help The service requires at least 1 backend endpoint (but 2 are required in order to compare performances) configured as comma-separated HTTP(S) URLs via the CLI flag -backend.endpoints. For each incoming request, query-tee will clone the request and send it to each backend, tracking performance metrics for each backend before sending back the response to the client.\nHow it works API endpoints The following Prometheus API endpoints are supported by query-tee:\n /api/v1/query (GET) /api/v1/query_range (GET) /api/v1/labels (GET) /api/v1/label/{name}/values (GET) /api/v1/series (GET) Authentication query-tee supports HTTP basic authentication. It allows either to configure username and password in the backend URL, to forward the request auth to the backend or merge the two.\nThe request sent from the query-tee to the backend includes HTTP basic authentication when one of the following conditions are met:\n If the endpoint URL has username and password, query-tee uses it. If the endpoint URL has username only, query-tee keeps the username and inject the password received in the incoming request (if any). If the endpoint URL has no username and no password, query-tee forwards the incoming request basic authentication (if any). Backend response selection query-tee allows to configure a preferred backend from which picking the response to send back to the client. The preferred backend can be configured via the CLI flag -backend.preferred=\u0026lt;hostname\u0026gt;, setting it to the hostname of the preferred backend.\nWhen a preferred backend is set, query-tee sends back to the client:\n The preferred backend response if the status code is 2xx or 4xx Otherwise, the first received 2xx or 4xx response if at least a backend succeeded Otherwise, the first received response When a preferred backend is not set, query-tee sends back to the client:\n The first received 2xx or 4xx response if at least a backend succeeded Otherwise, the first received response Note: from the query-tee perspective, a backend request is considered successful even if the status code is 4xx because it generally means the error is due to an invalid request and not to a backend issue.\nExported metrics query-tee exposes the following Prometheus metrics on the port configured via the CLI flag -server.metrics-port:\n# HELP cortex_querytee_request_duration_seconds Time (in seconds) spent serving HTTP requests. # TYPE cortex_querytee_request_duration_seconds histogram cortex_querytee_request_duration_seconds_bucket{backend=\u0026#34;\u0026lt;hostname\u0026gt;\u0026#34;,method=\u0026#34;\u0026lt;method\u0026gt;\u0026#34;,route=\u0026#34;\u0026lt;route\u0026gt;\u0026#34;,status_code=\u0026#34;\u0026lt;status\u0026gt;\u0026#34;,le=\u0026#34;\u0026lt;bucket\u0026gt;\u0026#34;} cortex_querytee_request_duration_seconds_sum{backend=\u0026#34;\u0026lt;hostname\u0026gt;\u0026#34;,method=\u0026#34;\u0026lt;method\u0026gt;\u0026#34;,route=\u0026#34;\u0026lt;route\u0026gt;\u0026#34;,status_code=\u0026#34;\u0026lt;status\u0026gt;\u0026#34;} cortex_querytee_request_duration_seconds_count{backend=\u0026#34;\u0026lt;hostname\u0026gt;\u0026#34;,method=\u0026#34;\u0026lt;method\u0026gt;\u0026#34;,route=\u0026#34;\u0026lt;route\u0026gt;\u0026#34;,status_code=\u0026#34;\u0026lt;status\u0026gt;\u0026#34;}","excerpt":"The query-tee is a standalone service which can be used for testing purposes to compare the query …","ref":"/docs/operations/query-tee/","title":"Query Tee (service)"},{"body":" [this is a work in progress]\nSee also the Running in Production document.\nCredentials You can supply credentials to Cortex by setting environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY (and AWS_SESSION_TOKEN if you use MFA), or use a short-term token solution such as kiam.\nShould I use S3 or DynamoDB ? Note that the choices are: \u0026ldquo;chunks\u0026rdquo; of timeseries data in S3 and index in DynamoDB, or everything in DynamoDB. Using just S3 is not an option.\nBroadly S3 is much more expensive to read and write, while DynamoDB is much more expensive to store over months. S3 charges differently, so the cross-over will depend on the size of your chunks, and how long you keep them. Very roughly: for 3KB chunks if you keep them longer than 8 months then S3 is cheaper.\nDynamoDB capacity provisioning By default, the Cortex Tablemanager will provision tables with 1,000 units of write capacity and 300 read - these numbers are chosen to be high enough that most trial installations won\u0026rsquo;t see a bottleneck on storage, but do note that that AWS will charge you approximately $60 per day for this capacity.\nTo match your costs to requirements, observe the actual capacity utilisation via CloudWatch or Prometheus metrics, then adjust the Tablemanager provision via command-line options -dynamodb.chunk-table.write-throughput, read-throughput and similar with .periodic-table which controls the index table.\nTablemanager can even adjust the capacity dynamically, by watching metrics for DynamoDB throttling and ingester queue length. Here is an example set of command-line parameters from a fairly modest install:\n -target=table-manager -metrics.url=http://prometheus.monitoring.svc.cluster.local./api/prom/ -metrics.target-queue-length=100000 -dynamodb.url=dynamodb://us-east-1/ -schema-config-file=/etc/schema.yaml -dynamodb.periodic-table.write-throughput=1000 -dynamodb.periodic-table.write-throughput.scale.enabled=true -dynamodb.periodic-table.write-throughput.scale.min-capacity=200 -dynamodb.periodic-table.write-throughput.scale.max-capacity=2000 -dynamodb.periodic-table.write-throughput.scale.out-cooldown=300 # 5 minutes between scale ups -dynamodb.periodic-table.inactive-enable-ondemand-throughput-mode=true -dynamodb.periodic-table.read-throughput=300 -dynamodb.periodic-table.tag=product_area=cortex -dynamodb.chunk-table.write-throughput=800 -dynamodb.chunk-table.write-throughput.scale.enabled=true -dynamodb.chunk-table.write-throughput.scale.min-capacity=200 -dynamodb.chunk-table.write-throughput.scale.max-capacity=1000 -dynamodb.chunk-table.write-throughput.scale.out-cooldown=300 # 5 minutes between scale ups -dynamodb.chunk-table.inactive-enable-ondemand-throughput-mode=true -dynamodb.chunk-table.read-throughput=300 -dynamodb.chunk-table.tag=product_area=cortex Several things to note here:\n -metrics.url points at a Prometheus server running within the cluster, scraping Cortex. Currently it is not possible to use Cortex itself as the target here. -metrics.target-queue-length: when the ingester queue is below this level, Tablemanager will not scale up. When the queue is growing above this level, Tablemanager will scale up whatever table is being throttled. The plain throughput values are used when the tables are first created. Scale-up to any level up to this value will be very quick, but if you go higher than this initial value, AWS may take tens of minutes to finish scaling. In the config above they are set. ondemand-throughput-mode tells AWS to charge for what you use, as opposed to continuous provisioning. This mode is cost-effective for older data, which is never written and only read sporadically. ","excerpt":"[this is a work in progress]\nSee also the Running in Production document.\nCredentials You can supply …","ref":"/docs/guides/aws/","title":"Running Cortex at AWS"},{"body":" Cortex consists of multiple horizontally scalable microservices. Each microservice uses the most appropriate technique for horizontal scaling; most are stateless and can handle requests for any users while some (namely the ingesters) are semi-stateful and depend on consistent hashing. This document provides a basic overview of Cortex\u0026rsquo;s architecture.\nThe role of Prometheus Prometheus instances scrape samples from various targets and then push them to Cortex (using Prometheus\u0026rsquo; remote write API). That remote write API emits batched Snappy-compressed Protocol Buffer messages inside the body of an HTTP PUT request.\nCortex requires that each HTTP request bear a header specifying a tenant ID for the request. Request authentication and authorization are handled by an external reverse proxy.\nIncoming samples (writes from Prometheus) are handled by the distributor while incoming reads (PromQL queries) are handled by the querier or optionally by the query frontend.\nStorage Cortex currently supports two storage engines to store and query the time series:\n Chunks (default, stable) Blocks (experimental) The two engines mostly share the same Cortex architecture with few differences outlined in the rest of the document.\nChunks storage (default) The chunks storage stores each single time series into a separate object called Chunk. Each Chunk contains the samples for a given period (defaults to 12 hours). Chunks are then indexed by time range and labels, in order to provide a fast lookup across many (over millions) Chunks.\nFor this reason, the chunks storage consists of:\n An index for the Chunks. This index can be backed by: Amazon DynamoDB Google Bigtable Apache Cassandra An object store for the Chunk data itself, which can be: Amazon DynamoDB Google Bigtable Apache Cassandra Amazon S3 Google Cloud Storage Microsoft Azure Storage Internally, the access to the chunks storage relies on a unified interface called \u0026ldquo;chunks store\u0026rdquo;. Unlike other Cortex components, the chunk store is not a separate service, but rather a library embedded in the services that need to access the long-term storage: ingester, querier and ruler.\nThe chunk and index format are versioned, this allows Cortex operators to upgrade the cluster to take advantage of new features and improvements. This strategy enables changes in the storage format without requiring any downtime or complex procedures to rewrite the stored data. A set of schemas are used to map the version while reading and writing time series belonging to a specific period of time.\nThe current schema recommendation is the v10 schema (v11 is still experimental). For more information about the schema, please check out the Schema documentation.\nBlocks storage (experimental) The blocks storage is based on Prometheus TSDB: it stores each tenant\u0026rsquo;s time series into their own TSDB which write out their series to a on-disk Block (defaults to 2h block range periods). Each Block is composed by few files storing the chunks and the block index.\nThe TSDB chunk files contain the samples for multiple series. The series inside the Chunks are then indexed by a per-block index, which indexes metric names and labels to time series in the chunk files.\nThe blocks storage doesn\u0026rsquo;t require a dedicated storage backend for the index. The only requirement is an object store for the Block files, which can be:\n Amazon S3 Google Cloud Storage Microsoft Azure Storage Local Filesystem (single node only) For more information, please check out the Blocks storage documentation.\nServices Cortex has a service-based architecture, in which the overall system is split up into a variety of components that perform a specific task. These components run separately and in parallel. Cortex can alternatively run in a single process mode, where all components are executed within a single process. The single process mode is particularly handy for local testing and development.\nCortex is, for the most part, a shared-nothing system. Each layer of the system can run multiple instances of each component and they don\u0026rsquo;t coordinate or communicate with each other within that layer.\nThe Cortex services are:\n Distributor Ingester Querier Query frontend (optional) Ruler (optional) Alertmanager (optional) Configs API (optional) Distributor The distributor service is responsible for handling incoming samples from Prometheus. It\u0026rsquo;s the first stop in the write path for series samples. Once the distributor receives samples from Prometheus, each sample is validated for correctness and to ensure that it is within the configured tenant limits, falling back to default ones in case limits have not been overridden for the specific tenant. Valid samples are then split into batches and sent to multiple ingesters in parallel.\nThe validation done by the distributor includes:\n The metric labels name are formally correct The configured max number of labels per metric is respected The configured max length of a label name and value is respected The timestamp is not older/newer than the configured min/max time range Distributors are stateless and can be scaled up and down as needed.\nHigh Availability Tracker The distributor features a High Availability (HA) Tracker. When enabled, the distributor deduplicates incoming samples from redundant Prometheus servers. This allows you to have multiple HA replicas of the same Prometheus servers, writing the same series to Cortex and then deduplicate these series in the Cortex distributor.\nThe HA Tracker deduplicates incoming samples based on a cluster and replica label. The cluster label uniquely identifies the cluster of redundant Prometheus servers for a given tenant, while the replica label uniquely identifies the replica within the Prometheus cluster. Incoming samples are considered duplicated (and thus dropped) if received by any replica which is not the current primary within a cluster.\nThe HA Tracker requires a key-value (KV) store to coordinate which replica is currently elected. The distributor will only accept samples from the current leader. Samples with one or no labels (of the replica and cluster) are accepted by default and never deduplicated.\nThe supported KV stores for the HA tracker are:\n Consul Etcd For more information, please refer to config for sending HA pairs data to Cortex in the documentation.\nHashing Distributors use consistent hashing, in conjunction with a configurable replication factor, to determine which ingester instance(s) should receive a given series.\nCortex supports two hashing strategies:\n Hash the metric name and tenant ID (default) Hash the metric name, labels and tenant ID (enabled with -distributor.shard-by-all-labels=true) The trade-off associated with the latter is that writes are more balanced across ingesters but each query needs to talk to any ingester since a metric could be spread across multiple ingesters given different label sets.\nThe hash ring A hash ring (stored in a key-value store) is used to achieve consistent hashing for the series sharding and replication across the ingesters. All ingesters register themselves into the hash ring with a set of tokens they own; each token is a random unsigned 32-bit number. Each incoming series is hashed in the distributor and then pushed to the ingester owning the tokens range for the series hash number plus N-1 subsequent ingesters in the ring, where N is the replication factor.\nTo do the hash lookup, distributors find the smallest appropriate token whose value is larger than the hash of the series. When the replication factor is larger than 1, the next subsequent tokens (clockwise in the ring) that belong to different ingesters will also be included in the result.\nThe effect of this hash set up is that each token that an ingester owns is responsible for a range of hashes. If there are three tokens with values 0, 25, and 50, then a hash of 3 would be given to the ingester that owns the token 25; the ingester owning token 25 is responsible for the hash range of 1-25.\nThe supported KV stores for the hash ring are:\n Consul Etcd Gossip memberlist (experimental) Quorum consistency Since all distributors share access to the same hash ring, write requests can be sent to any distributor and you can setup a stateless load balancer in front of it.\nTo ensure consistent query results, Cortex uses Dynamo-style quorum consistency on reads and writes. This means that the distributor will wait for a positive response of at least one half plus one of the ingesters to send the sample to before successfully responding to the Prometheus write request.\nLoad balancing across distributors We recommend randomly load balancing write requests across distributor instances. For example, if you\u0026rsquo;re running Cortex in a Kubernetes cluster, you could run the distributors as a Kubernetes Service.\nIngester The ingester service is responsible for writing incoming series to a long-term storage backend on the write path and returning in-memory series samples for queries on the read path.\nIncoming series are not immediately written to the storage but kept in memory and periodically flushed to the storage (by default, 12 hours for the chunks storage and 2 hours for the experimental blocks storage). For this reason, the queriers may need to fetch samples both from ingesters and long-term storage while executing a query on the read path.\nIngesters contain a lifecycler which manages the lifecycle of an ingester and stores the ingester state in the hash ring. Each ingester could be in one of the following states:\n PENDING is an ingester\u0026rsquo;s state when it just started and is waiting for a hand-over from another ingester that is LEAVING. If no hand-over occurs within the configured timeout period (\u0026ldquo;auto-join timeout\u0026rdquo;, configurable via -ingester.join-after option), the ingester will join the ring with a new set of random tokens (ie. during a scale up). When hand-over process starts, state changes to JOINING.\n JOINING is an ingester\u0026rsquo;s state in two situations. First, ingester will switch to a JOINING state from PENDING state after auto-join timeout. In this case, ingester will generate tokens, store them into the ring, optionally observe the ring for token conflicts and then move to ACTIVE state. Second, ingester will also switch into a JOINING state as a result of another LEAVING ingester initiating a hand-over process with PENDING (which then switches to JOINING state). JOINING ingester then receives series and tokens from LEAVING ingester, and if everything goes well, JOINING ingester switches to ACTIVE state. If hand-over process fails, JOINING ingester will move back to PENDING state and either wait for another hand-over or auto-join timeout.\n ACTIVE is an ingester\u0026rsquo;s state when it is fully initialized. It may receive both write and read requests for tokens it owns.\n LEAVING is an ingester\u0026rsquo;s state when it is shutting down. It cannot receive write requests anymore, while it could still receive read requests for series it has in memory. While in this state, the ingester may look for a PENDING ingester to start a hand-over process with, used to transfer the state from LEAVING ingester to the PENDING one, during a rolling update (PENDING ingester moves to JOINING state during hand-over process). If there is no new ingester to accept hand-over, ingester in LEAVING state will flush data to storage instead.\n UNHEALTHY is an ingester\u0026rsquo;s state when it has failed to heartbeat to the ring\u0026rsquo;s KV Store. While in this state, distributors skip the ingester while building the replication set for incoming series and the ingester does not receive write or read requests.\n For more information about the hand-over process, please check out the Ingester hand-over documentation.\nIngesters are semi-stateful.\nIngesters failure and data loss If an ingester process crashes or exits abruptly, all the in-memory series that have not yet been flushed to the long-term storage will be lost. There are two main ways to mitigate this failure mode:\n Replication Write-ahead log (WAL) The replication is used to hold multiple (typically 3) replicas of each time series in the ingesters. If the Cortex cluster looses an ingester, the in-memory series hold by the lost ingester are also replicated at least to another ingester. In the event of a single ingester failure, no time series samples will be lost while, in the event of multiple ingesters failure, time series may be potentially lost if failure affects all the ingesters holding the replicas of a specific time series.\nThe write-ahead log (WAL) is used to write to a persistent local disk all incoming series samples until they\u0026rsquo;re flushed to the long-term storage. In the event of an ingester failure, a subsequent process restart will replay the WAL and recover the in-memory series samples.\nContrary to the sole replication and given the persistent local disk data is not lost, in the event of multiple ingesters failure each ingester will recover the in-memory series samples from WAL upon subsequent restart. The replication is still recommended in order to ensure no temporary failures on the read path in the event of a single ingester failure.\nThe WAL for the chunks storage is an experimental feature (disabled by default), while it\u0026rsquo;s always enabled for the blocks storage.\nIngesters write de-amplification Ingesters store recently received samples in-memory in order to perform write de-amplification. If the ingesters would immediately write received samples to the long-term storage, the system would be very difficult to scale due to the very high pressure on the storage. For this reason, the ingesters batch and compress samples in-memory and periodically flush them out to the storage.\nWrite de-amplification is the main source of Cortex\u0026rsquo;s low total cost of ownership (TCO).\nQuerier The querier service handles queries using the PromQL query language.\nQueriers fetch series samples both from the ingesters and long-term storage: the ingesters hold the in-memory series which have not yet been flushed to the long-term storage. Because of the replication factor, it is possible that the querier may receive duplicated samples; to resolve this, for a given time series the querier internally deduplicates samples with the same exact timestamp.\nQueriers are stateless and can be scaled up and down as needed.\nQuery frontend The query frontend is an optional service providing the querier\u0026rsquo;s API endpoints and can be used to accelerate the read path. When the query frontend is in place, incoming query requests should be directed to the query frontend instead of the queriers. The querier service will be still required within the cluster, in order to execute the actual queries.\nThe query frontend internally performs some query adjustments and holds queries in an internal queue. In this setup, queriers act as workers which pull jobs from the queue, execute them, and return them to the query-frontend for aggregation. Queriers need to be configured with the query frontend address (via the -querier.frontend-address CLI flag) in order to allow them to connect to the query frontends.\nQuery frontends are stateless. However, due to how the internal queue works, it\u0026rsquo;s recommended to run a few query frontend replicas to reap the benefit of fair scheduling. Two replicas should suffice in most cases.\nQueueing The query frontend queuing mechanism is used to:\n Ensure that large queries, that could cause an out-of-memory (OOM) error in the querier, will be retried on failure. This allows administrators to under-provision memory for queries, or optimistically run more small queries in parallel, which helps to reduce the TCO. Prevent multiple large requests from being convoyed on a single querier by distributing them across all queriers using a first-in/first-out queue (FIFO). Prevent a single tenant from denial-of-service-ing (DOSing) other tenants by fairly scheduling queries between tenants. Splitting The query frontend splits multi-day queries into multiple single-day queries, executing these queries in parallel on downstream queriers and stitching the results back together again. This prevents large (multi-day) queries from causing out of memory issues in a single querier and helps to execute them faster.\nCaching The query frontend supports caching query results and reuses them on subsequent queries. If the cached results are incomplete, the query frontend calculates the required subqueries and executes them in parallel on downstream queriers. The query frontend can optionally align queries with their step parameter to improve the cacheability of the query results. The result cache is compatible with any cortex caching backend (currently memcached, redis, and an in-memory cache).\nRuler The ruler is an optional service executing PromQL queries for recording rules and alerts. The ruler requires a database storing the recording rules and alerts for each tenant.\nRuler is semi-stateful and can be scaled horizontally. Running rules internally have state, as well as the ring the rulers initiate. However, if the rulers all fail and restart, Prometheus alert rules have a feature where an alert is restored and returned to a firing state if it would have been active in its for period. However, there would be gaps in the series generated by the recording rules.\nAlertmanager The alertmanager is an optional service responsible for accepting alert notifications from the ruler, deduplicating and grouping them, and routing them to the correct notification channel, such as email, PagerDuty or OpsGenie.\nThe Cortex alertmanager is built on top of the Prometheus Alertmanager, adding multi-tenancy support. Like the ruler, the alertmanager requires a database storing the per-tenant configuration.\nAlertmanager is semi-stateful. The Alertmanager persists information about silences and active alerts to its disk. If all of the alertmanager nodes failed simultaneously there would be a loss of data.\nConfigs API The configs API is an optional service managing the configuration of Rulers and Alertmanagers. It provides APIs to get/set/update the rules and alertmanager configurations and store them into backend. Current supported backend are PostgreSQL and in-memory.\nConfigs API is stateless.\n","excerpt":"Cortex consists of multiple horizontally scalable microservices. Each microservice uses the most …","ref":"/docs/architecture/","title":"Cortex Architecture"},{"body":" Context One option to scale the ruler is by scaling it horizontally. However, with multiple ruler instances running they will need to coordinate to determine which instance will evaluate which rule. Similar to the ingesters, the rulers establish a hash ring to divide up the responsibilities of evaluating rules.\nConfig In order to enable sharding in the ruler the following flag needs to be set:\n -ruler.enable-sharding=true In addition the ruler requires it\u0026rsquo;s own ring to be configured, for instance:\n -ruler.ring.consul.hostname=consul.dev.svc.cluster.local:8500 The only configuration that is required is to enable sharding and configure a key value store. From there the rulers will shard and handle the division of rules automatically.\nUnlike ingesters, rulers do not hand over responsibility: all rules are re-sharded randomly every time a ruler is added to or removed from the ring.\n","excerpt":"Context One option to scale the ruler is by scaling it horizontally. However, with multiple ruler …","ref":"/docs/guides/ruler-sharding/","title":"Config for horizontally scaling the Ruler"},{"body":" Context You can have more than a single Prometheus monitoring and ingesting the same metrics for redundancy. Cortex already does replication for redundancy and it doesn\u0026rsquo;t make sense to ingest the same data twice. So in Cortex, we made sure we can dedupe the data we receive from HA Pairs of Prometheus. We do this via the following:\nAssume that there are two teams, each running their own Prometheus, monitoring different services. Let\u0026rsquo;s call the Prometheis T1 and T2. Now, if the teams are running HA pairs, let\u0026rsquo;s call the individual Prometheis, T1.a, T1.b and T2.a and T2.b.\nIn Cortex we make sure we only ingest from one of T1.a and T1.b, and only from one of T2.a and T2.b. We do this by electing a leader replica for each cluster of Prometheus. For example, in the case of T1, let it be T1.a. As long as T1.a is the leader, we drop the samples sent by T1.b. And if Cortex sees no new samples from T1.a for a short period (30s by default), it\u0026rsquo;ll switch the leader to be T1.b.\nThis means if T1.a goes down for a few minutes Cortex\u0026rsquo;s HA sample handling will have switched and elected T1.b as the leader. This failover timeout is what enables us to only accept samples from a single replica at a time, but ensure we don\u0026rsquo;t drop too much data in case of issues. Note that with the default scrape period of 15s, and the default timeouts in Cortex, in most cases you\u0026rsquo;ll only lose a single scrape of data in the case of a leader election failover. For any rate queries the rate window should be at least 4x the scrape period to account for any of these failover scenarios, for example with the default scrape period of 15s then you should calculate rates over at least 1m periods.\nNow we do the same leader election process T2.\nConfig Client Side So for Cortex to achieve this, we need 2 identifiers for each process, one identifier for the cluster (T1 or T2, etc) and one identifier to identify the replica in the cluster (a or b). The easiest way to do with is by setting external labels, ideally cluster and replica (note the default is __replica__). For example:\ncluster: prom-team1 replica: replica1 (or pod-name) and\ncluster: prom-team1 replica: replica2 Note: These are external labels and have nothing to do with remote_write config.\nThese two label names are configurable per-tenant within Cortex, and should be set to something sensible. For example, cluster label is already used by some workloads, and you should set the label to be something else but uniquely identifies the cluster. Good examples for this label-name would be team, cluster, prometheus, etc.\nThe replica label should be set so that the value for each prometheus is unique in that cluster. Note: Cortex drops this label when ingesting data, but preserves the cluster label. This way, your timeseries won\u0026rsquo;t change when replicas change.\nServer Side To enable handling of samples, see the distributor flags having ha-tracker in them.\n","excerpt":"Context You can have more than a single Prometheus monitoring and ingesting the same metrics for …","ref":"/docs/guides/ha-pair-handling/","title":"Config for sending HA Pairs data to Cortex"},{"body":"Configuration for running Cortex in single-process mode. This should not be used in production. It is only for getting started and development.\n# Disable the requirement that every request to Cortex has a# X-Scope-OrgID header. `fake` will be substituted in instead.auth_enabled:falseserver:http_listen_port:9009# Configure the server to allow messages up to 100MB.grpc_server_max_recv_msg_size:104857600grpc_server_max_send_msg_size:104857600grpc_server_max_concurrent_streams:1000distributor:shard_by_all_labels:truepool:health_check_ingesters:trueingester_client:grpc_client_config:# Configure the client to allow messages up to 100MB.max_recv_msg_size:104857600max_send_msg_size:104857600use_gzip_compression:trueingester:#chunk_idle_period: 15mlifecycler:# The address to advertise for this ingester. Will be autodiscovered by# looking up address on eth0 or en0; can be specified if this fails.# address: 127.0.0.1# We want to start immediately and flush on shutdown.join_after:0claim_on_rollout:falsefinal_sleep:0snum_tokens:512# Use an in memory ring store, so we don\u0026#39;t need to launch a Consul.ring:kvstore:store:inmemoryreplication_factor:1# Use local storage - BoltDB for the index, and the filesystem# for the chunks.schema:configs:-from:2019-07-29store:boltdbobject_store:filesystemschema:v10index:prefix:index_period:168hstorage:boltdb:directory:/tmp/cortex/indexfilesystem:directory:/tmp/cortex/chunks","excerpt":"Configuration for running Cortex in single-process mode. This should not be used in production. It …","ref":"/docs/configuration/single-process-config/","title":"Single-process"},{"body":" Correctly configured caching is important for a production-ready Cortex cluster. Cortex has many opportunities for using caching to accelerate queries and reduce cost. Cortex can use a cache for:\n The results of a whole query And for the chunk storage:\n Individual chunks Index lookups for one label on one day Reducing duplication of writes. This doc aims to describe what each cache does, how to configure them and how to tune them.\nCortex Caching Options Cortex can use various different technologies for caching - Memcached, Redis or an in-process FIFO cache. The recommended caching technology for production workloads is Memcached. Using Memcached in your Cortex install means results from one process can be re-used by another. In-process caching can cut fetch times slightly and reduce the load on Memcached, but can only be used by a single process.\nIf multiple caches are enabled for each caching opportunities, they will be tiered – writes will go to all caches, but reads will first go to the in-memory FIFO cache, then memcached, then redis.\nMemcached For small deployments you can use a single memcached cluster for all the caching opportunities – the keys do not collide.\nFor large deployments we recommend separate memcached deployments for each of the caching opportunities, as this allows more sophisticated sizing, monitoring and configuration of each cache. For help provisioning and monitoring memcached clusters using tanka, see the memcached jsonnet module and the memcached-mixin.\nCortex uses DNS SRV records to find the various memcached servers in a cluster. You should ensure your memcached servers are not behind any kind of load balancer. If deploying Cortex on Kubernetes, Cortex should be pointed at a memcached headless service.\nThe flags used to configure memcached are common for each caching caching opportunity, differentiated by a prefix:\n-\u0026lt;prefix\u0026gt;.memcache.write-back-buffer int How many chunks to buffer for background write back. (default 10000) -\u0026lt;prefix\u0026gt;.memcache.write-back-goroutines int How many goroutines to use to write back to memcache. (default 10) -\u0026lt;prefix\u0026gt;.memcached.batchsize int How many keys to fetch in each batch. -\u0026lt;prefix\u0026gt;.memcached.consistent-hash Use consistent hashing to distribute to memcache servers. -\u0026lt;prefix\u0026gt;.memcached.expiration duration How long keys stay in the memcache. -\u0026lt;prefix\u0026gt;.memcached.hostname string Hostname for memcached service to use when caching chunks. If empty, no memcached will be used. -\u0026lt;prefix\u0026gt;.memcached.max-idle-conns int Maximum number of idle connections in pool. (default 16) -\u0026lt;prefix\u0026gt;.memcached.parallelism int Maximum active requests to memcache. (default 100) -\u0026lt;prefix\u0026gt;.memcached.service string SRV service used to discover memcache servers. (default \u0026quot;memcached\u0026quot;) -\u0026lt;prefix\u0026gt;.memcached.timeout duration Maximum time to wait before giving up on memcached requests. (default 100ms) -\u0026lt;prefix\u0026gt;.memcached.update-interval duration Period with which to poll DNS for memcache servers. (default 1m0s) See the memcached_config and memcached_client_config documentation if you use a config file with Cortex.\nFIFO Cache (Experimental) The FIFO cache is an in-memory, in-process (non-shared) cache that uses a First-In-First-Out (FIFO) eviction strategy. The FIFO cache is useful for simple scenarios where deploying an additional memcached server is too much work, such as when experimenting with the Query Frontend. The FIFO cache can also be used in front of Memcached to reduce latency for commonly accessed keys. The FIFO cache stores a fixed number of entries, and therefore it’s memory usage depends on the caches value’s size.\nTo enable the FIFO cache, use the following flags:\n-\u0026lt;prefix\u0026gt;.cache.enable-fifocache Enable in-memory cache. -\u0026lt;prefix\u0026gt;.fifocache.duration duration The expiry duration for the cache. -\u0026lt;prefix\u0026gt;.fifocache.size int The number of entries to cache. See fifo_cache_config documentation if you use a config file with Cortex.\nRedis (Experimental) You can also use Redis for out-of-process caching; this is a relatively new addition to Cortex and is under active development.\n-\u0026lt;prefix\u0026gt;.redis.enable-tls Enables connecting to redis with TLS. -\u0026lt;prefix\u0026gt;.redis.endpoint string Redis service endpoint to use when caching chunks. If empty, no redis will be used. -\u0026lt;prefix\u0026gt;.redis.expiration duration How long keys stay in the redis. -\u0026lt;prefix\u0026gt;.redis.max-active-conns int Maximum number of active connections in pool. -\u0026lt;prefix\u0026gt;.redis.max-idle-conns int Maximum number of idle connections in pool. (default 80) -\u0026lt;prefix\u0026gt;.redis.password value Password to use when connecting to redis. -\u0026lt;prefix\u0026gt;.redis.timeout duration Maximum time to wait before giving up on redis requests. (default 100ms) See redis_config documentation if you use a config file with Cortex.\nCortex Caching Opportunities Chunks Cache The chunk cache stores immutable compressed chunks. The cache is used by queries to reduce load on the chunk store. These are typically a few KB in size, and depend mostly on the duration and encoding of your chunks. The chunk cache is a write-through cache - chunks are written to the cache as they are flushed to the chunk store. This ensures the cache always contains the most recent chunks. Items stay in the cache indefinitely.\nThe chunk cache should be configured on the ingester, querier and ruler using the flags without a prefix.\nIt is best practice to ensure the chunk cache is big enough to accommodate at least 24 hours of chunk data. You can use the following query (from the cortex-mixin) to estimate the required number of memcached replicas:\n// 4 x in-memory series size = 24hrs of data. ( 4 * sum by(cluster, namespace) ( cortex_ingester_memory_series{job=~\u0026#34;.+/ingester\u0026#34;} * cortex_ingester_chunk_size_bytes_sum{job=~\u0026#34;.+/ingester\u0026#34;} / cortex_ingester_chunk_size_bytes_count{job=~\u0026#34;.+/ingester\u0026#34;} ) / 1e9 ) \u0026gt; ( sum by (cluster, namespace) (memcached_limit_bytes{job=~\u0026#34;.+/memcached\u0026#34;}) / 1e9 ) Index Read Cache The index read cache stores entire rows from the inverted label index. The cache is used by queries to reduce load on the index. These are typically only a few KB in size, but can grow up to many MB for very high cardinality metrics. The index read cache is populated when there is a cache miss.\nThe index read cache should be configured on the querier and ruler, using the flags with the -store.index-cache-read prefix.\nQuery Results Cache The query results cache contains protobuf \u0026amp; snappy encoded query results. These query results can potentially be very large, and as such the maximum value size in memcached should be increased beyond the default 1M. The cache is populated when there is a cache miss. Items stay in the cache indefinitely.\nThe query results cache should be configured on the query-frontend using flags with -frontend.cache prefix.\nIndex Write Cache The index write cache is used to avoid re-writing index and chunk data which has already been stored in the back-end database, aka “deduplication”. This can reduce write load on your backend-database by around 12x.\nYou should not use in-process caching for the index write cache - most of the deduplication comes from replication between ingesters.\nThe index write cache contains row and column keys written to the index. If an entry is in the index write cache it will not be written to the index. As such, entries are only written to the index write cache after being successfully written to the index. Data stays in the index indefinitely or until it is evicted by newer entries.\nThe index write cache should be configures on the ingesters using flags with the -store.index-cache-write prefix.\n","excerpt":"Correctly configured caching is important for a production-ready Cortex cluster. Cortex has many …","ref":"/docs/guides/caching/","title":"Caching in Cortex"},{"body":" [this is a work in progress]\nRemote API Cortex supports Prometheus\u0026rsquo; remote_read and remote_write APIs.\nThe API for writes accepts a HTTP POST request with a body containing a request encoded with Protocol Buffers and compressed with Snappy. The HTTP path for writes is /api/prom/push. The definition of the protobuf message can be found in the Cortex codebase or in the Prometheus codebase. The HTTP request should contain the header X-Prometheus-Remote-Write-Version set to 0.1.0.\nThe API for reads also accepts HTTP/protobuf/snappy, and the path is /api/prom/read.\nSee the Prometheus documentation for more information on the Prometheus remote write format.\nAlerts \u0026amp; Rules API Cortex supports the Prometheus\u0026rsquo; alerts and rules api endpoints. This is supported in the Ruler service and can be enabled using the experimental.ruler.enable-api flag.\nGET /api/prom/api/v1/rules - List of alerting and recording rules that are currently loaded\nGET /api/prom/api/v1/alerts - List of all active alerts\nConfigs API The configs service provides an API-driven multi-tenant approach to handling various configuration files for prometheus. The service hosts an API where users can read and write Prometheus rule files, Alertmanager configuration files, and Alertmanager templates to a database.\nEach tenant will have it\u0026rsquo;s own set of rule files, Alertmanager config, and templates. A POST operation will effectively replace the existing copy with the configs provided in the request body.\nConfigs Format At the current time of writing, the API is part-way through a migration from a single Configs service that handled all three sets of data to a split API (Tracking issue). All APIs take and return all sets of data.\nThe following schema is used both when retrieving the current configs from the API and when setting new configs via the API.\nSchema: { \u0026#34;id\u0026#34;: 99, \u0026#34;rule_format_version\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;alertmanager_config\u0026#34;: \u0026#34;\u0026lt;standard alertmanager.yaml config\u0026gt;\u0026#34;, \u0026#34;rules_files\u0026#34;: { \u0026#34;rules.yaml\u0026#34;: \u0026#34;\u0026lt;standard rules.yaml config\u0026gt;\u0026#34;, \u0026#34;rules2.yaml\u0026#34;: \u0026#34;\u0026lt;standard rules.yaml config\u0026gt;\u0026#34; }, \u0026#34;template_files\u0026#34;: { \u0026#34;templates.tmpl\u0026#34;: \u0026#34;\u0026lt;standard template file\u0026gt;\u0026#34;, \u0026#34;templates2.tmpl\u0026#34;: \u0026#34;\u0026lt;standard template file\u0026gt;\u0026#34; } } Formatting id - should be incremented every time data is updated; Cortex will use the config with the highest number.\nrule_format_version - allows compatibility for tenants with config in Prometheus V1 format. Pass \u0026ldquo;1\u0026rdquo; or \u0026ldquo;2\u0026rdquo; according to which Prometheus version you want to match.\nconfig.alertmanager_config - The contents of the alertmanager config file should be as described here, encoded as a single string to fit within the overall JSON payload.\nconfig.rules_files - The contents of a rules file should be as described here, encoded as a single string to fit within the overall JSON payload.\nconfig.template_files - The contents of a template file should be as described here, encoded as a single string to fit within the overall JSON payload.\nEndpoints Manage Alertmanager GET /api/prom/configs/alertmanager - Get current Alertmanager config\n Normal Response Codes: OK(200) Error Response Codes: Unauthorized(401), NotFound(404) POST /api/prom/configs/alertmanager - Replace current Alertmanager config\n Normal Response Codes: NoContent(204) Error Response Codes: Unauthorized(401), BadRequest(400) The POST request body is expected to be like the following example:\n{ \u0026#34;rule_format_version\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;alertmanager_config\u0026#34;: \u0026#34;global:\\n resolve_timeout: 10s\\nroute: \\n receiver: webhook\\nreceivers:\\n - name: webhook\\n webhook_configs: \\n - url: http://example.com\u0026#34;, \u0026#34;rules_files\u0026#34;: { \u0026#34;rules.yaml\u0026#34;: \u0026#34;groups:\\n- name: demo-service-alerts\\n interval: 1s\\n rules:\\n - alert: SomethingIsUp\\n expr: up == 1\\n\u0026#34; } } POST /api/prom/configs/alertmanager/validate - Validate Alertmanager config\nNormal Response: OK(200)\n{ \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } Error Response: BadRequest(400)\n{ \u0026#34;status\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;error\u0026#34;: \u0026#34;error message\u0026#34; } Manage Rules GET /api/prom/configs/rules - Get current rule files\n Normal Response Codes: OK(200) Error Response Codes: Unauthorized(400), NotFound(404) POST /api/prom/configs/rules - Replace current rule files\n Normal Response Codes: NoContent(204) Error Response Codes: Unauthorized(401), BadRequest(400) Manage Templates GET /api/prom/configs/templates - Get current templates\n Normal Response Codes: OK(200) Error Response Codes: Unauthorized(401), NotFound(404) POST /api/prom/configs/templates - Replace current templates\n Normal Response Codes: NoContent(204) Error Response Codes: Unauthorized(401), BadRequest(400) Deactivate/Restore Configs DELETE /api/prom/configs/deactivate - Disable configs for a tenant\n Normal Response Codes: OK(200) Error Response Codes: Unauthorized(401), NotFound(404) POST /api/prom/configs/restore - Re-enable configs for a tenant\n Normal Response Codes OK(200) Error Response Codes: Unauthorized(401), NotFound(404) These API endpoints will disable/enable the current Rule and Alertmanager configuration for a tenant.\nNote that setting a new config will effectively \u0026ldquo;re-enable\u0026rdquo; the Rules and Alertmanager configuration for a tenant.\nIngester Shutdown POST /shutdown - Shutdown all operations of an ingester. Shutdown operations performed are similar to when an ingester is gracefully shutting down, including flushing of chunks if no other ingester is in PENDING state. Ingester does not terminate after calling this endpoint.\n Normal Response Codes: NoContent(204) Error Response Codes: Unauthorized(401) Testing APIs POST /push - Push samples directly to ingesters. Accepts requests in Prometheus remote write format. Indended for performance testing and debugging.\n","excerpt":"[this is a work in progress]\nRemote API Cortex supports Prometheus\u0026rsquo; remote_read and …","ref":"/docs/apis/","title":"Cortex APIs"},{"body":"The ingester holds several hours of sample data in memory. When we want to shut down an ingester, either for software version update or to drain a node for maintenance, this data must not be discarded.\nEach ingester goes through different states in its lifecycle. When working normally, the state is ACTIVE.\nOn start-up, an ingester first goes into state PENDING. After a short time, if nothing happens, it adds itself to the ring and goes into state ACTIVE.\nA running ingester is notified to shut down by Unix signal SIGINT. On receipt of this signal it goes into state LEAVING and looks for an ingester in state PENDING. If it finds one, that ingester goes into state JOINING and the leaver transfers all its in-memory data over to the joiner. On successful transfer the leaver removes itself from the ring and exits and the joiner changes to ACTIVE, taking over ownership of the leaver\u0026rsquo;s ring tokens.\nIf a leaving ingester does not find a pending ingester after several attempts, it will flush all of its chunks to the backing database, then remove itself from the ring and exit. This may take tens of minutes to complete.\nDuring hand-over, neither the leaving nor joining ingesters will accept new samples. Distributors are aware of this, and \u0026ldquo;spill\u0026rdquo; the samples to the next ingester in the ring. This creates a set of extra \u0026ldquo;spilled\u0026rdquo; chunks which will idle out and flush after hand-over is complete. The sudden increase in flush queue can be alarming!\nThe following metrics can be used to observe this process:\n cortex_member_ring_tokens_owned - how many tokens each ingester thinks it owns cortex_ring_tokens_owned - how many tokens each ingester is seen to own by other components cortex_ring_member_ownership_percent same as cortex_ring_tokens_owned but expressed as a percentage cortex_ring_members - how many ingesters can be seen in each state, by other components cortex_ingester_sent_chunks - number of chunks sent by leaving ingester cortex_ingester_received_chunks - number of chunks received by joining ingester You can see the current state of the ring via http browser request to /ring on a distributor.\n","excerpt":"The ingester holds several hours of sample data in memory. When we want to shut down an ingester, …","ref":"/docs/guides/ingester-handover/","title":"Ingester Hand-over"},{"body":" Currently the ingesters running in the chunks storage mode, store all their data in memory. If there is a crash, there could be loss of data. WAL helps fill this gap in reliability.\nTo use WAL, there are some changes that needs to be made in the deployment.\nChanges to deployment Since ingesters need to have the same persistent volume across restarts/rollout, all the ingesters should be run on statefulset with fixed volumes.\n Following flags needs to be set\n --ingester.wal-dir to the directory where the WAL data should be stores and/or recovered from. Note that this should be on the mounted volume. --ingester.wal-enabled to true which enables writing to WAL during ingestion. --ingester.checkpoint-enabled to true to enable checkpointing of in-memory chunks to disk. This is optional which helps in speeding up the replay process. --ingester.checkpoint-duration to the interval at which checkpoints should be created. Default is 30m, and depending on the number of series, it can be brought down to 15m if there are less series per ingester (say 1M). --ingester.recover-from-wal to true to recover data from an existing WAL. The data is recovered even if WAL is disabled and this is set to true. The WAL dir needs to be set for this. If you are going to enable WAL, it is advisable to always set this to true. --ingester.tokens-file-path should be set to the filepath where the tokens should be stored. Note that this should be on the mounted volume. Why this is required is described below. Changes in lifecycle when WAL is enabled Flushing of data to chunk store during rollouts or scale down is disabled. This is because during a rollout of statefulset there are no ingesters that are simultaneously leaving and joining, rather the same ingester is shut down and brought back again with updated config. Hence flushing is skipped and the data is recovered from the WAL.\n As there are no transfers between ingesters, the tokens are stored and recovered from disk between rollout/restarts. This is not a new thing but it is effective when using statefulsets.\n Migrating from stateless deployments The ingester deployment without WAL and statefulset with WAL should be scaled down and up respectively in sync without transfer of data between them to ensure that any ingestion after migration is reliable immediately.\nLet\u0026rsquo;s take an example of 4 ingesters. The migration would look something like this:\n Bring up one stateful ingester ingester-0 and wait till it\u0026rsquo;s ready (accepting read and write requests). Scale down old ingester deployment to 3 and wait till the leaving ingester flushes all the data to chunk store. Once that ingester has disappeared from kc get pods ..., add another stateful ingester and wait till it\u0026rsquo;s ready. This assures not transfer. Now you have ingester-0 ingester-1. Repeat step 2 to reduce remove another ingester from old deployment. Repeat step 3 to add another stateful ingester. Now you have ingester-0 ingester-1 ingester-2. Repeat step 4 and 5, and now you will finally have ingester-0 ingester-1 ingester-2 ingester-3. How to scale up/down Scale up Scaling up is same as what you would do without WAL or statefulsets. Nothing to change here.\nScale down Since Kubernetes doesn\u0026rsquo;t differentiate between rollout and scale down when sending a signal, the flushing of chunks is disabled by default. Hence the only thing to take care during scale down is flushing of chunks.\nThere are 2 ways to do it, with the latter being a fallback option.\nFirst option Consider you have 4 ingesters ingester-0 ingester-1 ingester-2 ingester-3 and you want to scale down to 2 ingesters, the ingesters which will be shutdown according to statefulset rules are ingester-3 and then ingester-2.\nHence before actually scaling down in Kubernetes, port forward those ingesters and hit the /shutdown endpoint. This will flush the chunks and shut down the ingesters (while also removing itself from the ring).\nAfter hitting the endpoint for ingester-2 ingester-3, scale down the ingesters to 2.\nPS: Given you have to scale down 1 ingester at a time, you can pipeline the shutdown and scaledown process instead of hitting shutdown endpoint for all to-be-scaled-down ingesters at the same time.\nFallback option\nThere is a flush mode ingester in progress, and with recent discussions there will be a separate target called flusher in it\u0026rsquo;s place.\nYou can run it as a kubernetes job which will * Attach to the volume of the scaled down ingester * Recover from the WAL * And flush all the chunks.\nThis job is to be run for all the ingesters that you missed hitting the shutdown endpoint as a first option.\nMore info about the flusher target will be added once it\u0026rsquo;s upstream.\n","excerpt":"Currently the ingesters running in the chunks storage mode, store all their data in memory. If there …","ref":"/docs/guides/ingesters-with-wal/","title":"Ingesters with WAL"},{"body":"","excerpt":"","ref":"/docs/guides/","title":"Guides"},{"body":" Cortex uses Jaeger to implement distributed tracing. We have found Jaeger invaluable for troubleshooting the behavior of Cortex in production.\nDependencies In order to send traces you will need to set up a Jaeger deployment. A deployment includes either the jaeger all-in-one binary, or else a distributed system of agents, collectors, and queriers. If running on Kubernetes, Jaeger Kubernetes is an excellent resource.\nConfiguration In order to configure Cortex to send traces you must do two things: 1. Set the JAEGER_AGENT_HOST environment variable in all components to point to your Jaeger agent. This defaults to localhost. 1. Enable sampling in the appropriate components: * The Ingester and Ruler self-initiate traces and should have sampling explicitly enabled. * Sampling for the Distributor and Query Frontend can be enabled in Cortex or in an upstream service such as your frontdoor.\nTo enable sampling in Cortex components you can specify either JAEGER_SAMPLER_MANAGER_HOST_PORT for remote sampling, or JAEGER_SAMPLER_TYPE and JAEGER_SAMPLER_PARAM to manually set sampling configuration. See the Jaeger Client Go documentation for the full list of environment variables you can configure.\nNote that you must specify one of JAEGER_AGENT_HOST or JAEGER_SAMPLER_MANAGER_HOST_PORT in each component for Jaeger to be enabled, even if you plan to use the default values.\n","excerpt":"Cortex uses Jaeger to implement distributed tracing. We have found Jaeger invaluable for …","ref":"/docs/guides/tracing/","title":"Tracing"},{"body":"","excerpt":"","ref":"/docs/configuration/","title":"Configuration"},{"body":"","excerpt":"","ref":"/docs/operations/","title":"Operating Cortex"},{"body":" This guide covers how to run a single local Cortex instance - with the chunks storage engine - storing time series chunks and index in Cassandra.\nIn this guide we\u0026rsquo;re going to:\n Setup a locally running Cassandra Configure Cortex to store chunks and index on Cassandra Configure Prometheus to send series to Cortex Configure Grafana to visualise metrics Setup a locally running Cassandra Run Cassandra with the following command:\ndocker run -d --name cassandra --rm -p 9042:9042 cassandra:3.11 Use Docker to execute the Cassandra Query Language (CQL) shell in the container:\ndocker exec -it \u0026lt;container_id\u0026gt; cqlsh Create a new Cassandra keyspace for Cortex metrics:\nA keyspace is an object that is used to hold column families, user defined types. A keyspace is like RDBMS database which contains column families, indexes, user defined types.\nCREATE KEYSPACE cortex WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1}; Configure Cortex to store chunks and index on Cassandra Now, we have to configure Cortex to store the chunks and index in Cassandra. Create a config file called single-process-config.yaml, then add the content below. Make sure to replace the following placeholders: - LOCALHOST: Addresses of your Cassandra instance. This can accept multiple addresses by passing them as comma separated values. - KEYSPACE: The name of the Cassandra keyspace used to store the metrics.\nsingle-process-config.yaml\n# Configuration for running Cortex in single-process mode. # This should not be used in production. It is only for getting started # and development. # Disable the requirement that every request to Cortex has a # X-Scope-OrgID header. `fake` will be substituted in instead. auth_enabled: false server: http_listen_port: 9009 # Configure the server to allow messages up to 100MB. grpc_server_max_recv_msg_size: 104857600 grpc_server_max_send_msg_size: 104857600 grpc_server_max_concurrent_streams: 1000 distributor: shard_by_all_labels: true pool: health_check_ingesters: true ingester_client: grpc_client_config: # Configure the client to allow messages up to 100MB. max_recv_msg_size: 104857600 max_send_msg_size: 104857600 use_gzip_compression: true ingester: lifecycler: # The address to advertise for this ingester. Will be autodiscovered by # looking up address on eth0 or en0; can be specified if this fails. address: 127.0.0.1 # We want to start immediately and flush on shutdown. join_after: 0 claim_on_rollout: false final_sleep: 0s num_tokens: 512 # Use an in memory ring store, so we don't need to launch a Consul. ring: kvstore: store: inmemory replication_factor: 1 # Use cassandra as storage -for both index store and chunks store. schema: configs: - from: 2019-07-29 store: cassandra object_store: cassandra schema: v10 index: prefix: index_ period: 168h chunks: prefix: chunk_ period: 168h storage: cassandra: addresses: LOCALHOST # configure cassandra addresses here. keyspace: KEYSPACE # configure desired keyspace here. The latest tag is not published for the Cortex docker image. Visit quay.io/repository/cortexproject/cortex to find the latest stable version tag and use it in the command bellow (currently it is v0.6.1).\nRun Cortex using the latest stable version:\ndocker run -d --name=cortex -v $(pwd)/single-process-config.yaml:/etc/single-process-config.yaml -p 9009:9009 quay.io/cortexproject/cortex:v0.6.1 -config.file=/etc/single-process-config.yaml In case you prefer to run the master version, please follow this documentation on how to build Cortex from source.\nConfigure Prometheus to send series to Cortex Now that Cortex is up, it should be running on http://localhost:9009.\nAdd the following section to your Prometheus configuration file. This will configure the remote write to send metrics to Cortex.\nremote_write: - url: http://localhost:9009/api/prom/push Configure Grafana to visualise metrics Run grafana to visualise metrics from Cortex:\ndocker run -d --name=grafana -p 3000:3000 grafana/grafana Add a data source in Grafana by selecting Prometheus as the data source type and use the Cortex URL to query metrics: http://localhost:9009/api/prom.\nFinally, You can monitor Cortex\u0026rsquo;s reads \u0026amp; writes by creating the dashboard. You can follow this documentation to do so.\n","excerpt":"This guide covers how to run a single local Cortex instance - with the chunks storage engine - …","ref":"/docs/guides/cassandra/","title":"Running Cortex with Cassandra"},{"body":" Welcome! We\u0026rsquo;re excited that you\u0026rsquo;re interested in contributing. Below are some basic guidelines.\nWorkflow Cortex follows a standard GitHub pull request workflow. If you\u0026rsquo;re unfamiliar with this workflow, read the very helpful Understanding the GitHub flow guide from GitHub.\nYou are welcome to create draft PRs at any stage of readiness - this can be helpful to ask for assistance or to develop an idea. But before a piece of work is finished it should:\n Be organised into one or more commits, each of which has a commit message that describes all changes made in that commit (\u0026lsquo;why\u0026rsquo; more than \u0026lsquo;what\u0026rsquo; - we can read the diffs to see the code that changed). Each commit should build towards the whole - don\u0026rsquo;t leave in back-tracks and mistakes that you later corrected. Have tests for new functionality or tests that would have caught the bug being fixed. Include a CHANGELOG message if users of Cortex need to hear about what you did. If you have made any changes to flags or config, run make doc and commit the changed files to update the config file documentation. Formatting Cortex projects uses goimports tool (go get golang.org/x/tools/cmd/goimports to install) to format the Go files, and sort imports. We use goimports with -local github.com/cortexproject/cortex parameter, to put Cortex internal imports into a separate group. We try to keep imports sorted into three groups: imports from standard library, imports of 3rd party packages and internal Cortex imports. Goimports will fix the order, but will keep existing newlines between imports in the groups. We try to avoid extra newlines like that.\nYou\u0026rsquo;re using an IDE you may find useful the following settings for the Cortex project:\n VSCode Developer Certificates of Origin (DCOs) Before submitting your work in a pull request, make sure that all commits are signed off with a Developer Certificate of Origin (DCO). Here\u0026rsquo;s an example:\ngit commit -s -m \u0026#34;Here is my signed commit\u0026#34; You can find further instructions here.\nBuilding Cortex To build:\nmake (By default, the build runs in a Docker container, using an image built with all the tools required. The source code is mounted from where you run make into the build container as a Docker volume.)\nTo run the test suite:\nmake test Dependency management We uses Go modules to manage dependencies on external packages. This requires a working Go environment with version 1.11 or greater, git and bzr installed.\nTo add or update a new dependency, use the go get command:\n# Pick the latest tagged release. go get example.com/some/module/pkg # Pick a specific version. go get example.com/some/module/pkg@vX.Y.Z Tidy up the go.mod and go.sum files:\ngo mod tidy go mod vendor git add go.mod go.sum vendor git commit You have to commit the changes to go.mod and go.sum before submitting the pull request.\nDocumentation The Cortex documentation is compiled into a website published at cortexmetrics.io. Please see \u0026ldquo;How to run the website locally\u0026rdquo; for instructions.\nNote: if you attempt to view pages on Github, it\u0026rsquo;s likely that you might find broken links or pages. That is expected and should not be addressed unless it is causing issues with the site that occur as part of the build.\n","excerpt":"Welcome! We\u0026rsquo;re excited that you\u0026rsquo;re interested in contributing. Below are some basic …","ref":"/docs/contributing/","title":"Contributing"},{"body":" This document defines project governance for the project.\nVoting The Cortex project employs voting to ensure no single member can dominate the project. Any maintainer may cast a vote. To avoid having a single company dominate the project, at most two votes from maintainers working for the same company will count.\nFor formal votes, a specific statement of what is being voted on should be added to the relevant github issue or PR, and a link to that issue or PR added to the maintainers meeting agenda document. Maintainers should indicate their yes/no vote on that issue or PR, and after a suitable period of time, the votes will be tallied and the outcome noted.\nChanges in Maintainership New maintainers are proposed by an existing maintainer and are elected by a 2\u0026frasl;3 majority vote.\nMaintainers can be removed by a 2\u0026frasl;3 majority vote.\nApproving PRs PRs may be merged after receiving at least two positive votes. If the PR author is a maintainer, this counts as a vote.\nGithub Project Administration Maintainers will be added to the collaborators list of the Cortex repository with \u0026ldquo;Write\u0026rdquo; access.\nAfter 6 months a maintainer will be given \u0026ldquo;Admin\u0026rdquo; access to the Cortex repository.\nChanges in Governance All changes in Governance require a 2\u0026frasl;3 majority vote.\nOther Changes Unless specified above, all other changes to the project require a 2\u0026frasl;3 majority vote. Additionally, any maintainer may request that any change require a 2\u0026frasl;3 majority vote.\n","excerpt":"This document defines project governance for the project.\nVoting The Cortex project employs voting …","ref":"/docs/governance/","title":"Governance"},{"body":" master / unreleased [CHANGE] Utilize separate protos for rule state and storage. Experimental ruler API will not be functional until the rollout is complete. #2226 [CHANGE] Frontend worker in querier now starts after all Querier module dependencies are started. This fixes issue where frontend worker started to send queries to querier before it was ready to serve them (mostly visible when using experimental blocks storage). #2246 [CHANGE] Lifecycler component now enters Failed state on errors, and doesn\u0026rsquo;t exit the process. (Important if you\u0026rsquo;re vendoring Cortex and use Lifecycler) #2251 [FEATURE] Flusher target to flush the WAL. -flusher.wal-dir for the WAL directory to recover from. -flusher.concurrent-flushes for number of concurrent flushes. -flusher.flush-op-timeout is duration after which a flush should timeout. [ENHANCEMENT] Experimental TSDB: Add support for local filesystem backend. #2245 0.7.0-rc.0 / 2020-03-09 Cortex 0.7.0 introduces some breaking changes. You\u0026rsquo;re encouraged to read all the [CHANGE] entries below before upgrading your Cortex cluster. In particular:\n Cleaned up some configuration options in preparation for the Cortex 1.0.0 release: Removed CLI flags support to configure the schema (see how to migrate from flags to schema file) Renamed CLI flag -config-yaml to -schema-config-file Removed CLI flag -store.min-chunk-age in favor of -querier.query-store-after. The corresponding YAML config option ingestermaxquerylookback has been renamed to query_ingesters_within Deprecated CLI flag -frontend.cache-split-interval in favor of -querier.split-queries-by-interval Renamed the YAML config option defaul_validity to default_validity Removed the YAML config option config_store (in the alertmanager YAML config) in favor of store Removed the YAML config root block configdb in favor of configs. This change is also reflected in the following CLI flags renaming: -database.* -\u0026gt; -configs.database.* -database.migrations -\u0026gt; -configs.database.migrations-dir Removed the fluentd-based billing infrastructure including the CLI flags: -distributor.enable-billing -billing.max-buffered-events -billing.retry-delay -billing.ingester Removed support for using denormalised tokens in the ring. Before upgrading, make sure your Cortex cluster is already running v0.6.0 or an earlier version with -ingester.normalise-tokens=true Full changelog [CHANGE] Removed support for flags to configure schema. Further, the flag for specifying the config file (-config-yaml) has been deprecated. Please use -schema-config-file. See the Schema Configuration documentation for more details on how to configure the schema using the YAML file. #2221 [CHANGE] Config file changed to remove top level config_store field in favor of a nested configdb field. #2125 [CHANGE] Removed unnecessary frontend.cache-split-interval in favor of querier.split-queries-by-interval both to reduce configuration complexity and guarantee alignment of these two configs. Starting from now, -querier.cache-results may only be enabled in conjunction with -querier.split-queries-by-interval (previously the cache interval default was 24h so if you want to preserve the same behaviour you should set -querier.split-queries-by-interval=24h). #2040 [CHANGE] Renamed Configs configuration options. #2187 configuration options -database.* -\u0026gt; -configs.database.* -database.migrations -\u0026gt; -configs.database.migrations-dir config file configdb.uri: -\u0026gt; configs.database.uri: configdb.migrationsdir: -\u0026gt; configs.database.migrations_dir: configdb.passwordfile: -\u0026gt; configs.database.password_file: [CHANGE] Moved -store.min-chunk-age to the Querier config as -querier.query-store-after, allowing the store to be skipped during query time if the metrics wouldn\u0026rsquo;t be found. The YAML config option ingestermaxquerylookback has been renamed to query_ingesters_within to match its CLI flag. #1893 [CHANGE] Renamed the cache configuration setting defaul_validity to default_validity. #2140 [CHANGE] Remove fluentd-based billing infrastructure and flags such as -distributor.enable-billing. #1491 [CHANGE] Removed remaining support for using denormalised tokens in the ring. If you\u0026rsquo;re still running ingesters with denormalised tokens (Cortex 0.4 or earlier, with -ingester.normalise-tokens=false), such ingesters will now be completely invisible to distributors and need to be either switched to Cortex 0.6.0 or later, or be configured to use normalised tokens. #2034 [CHANGE] The frontend http server will now send 502 in case of deadline exceeded and 499 if the user requested cancellation. #2156 [CHANGE] We now enforce queries to be up to -querier.max-query-into-future into the future (defaults to 10m). #1929 -store.min-chunk-age has been removed -querier.query-store-after has been added in it\u0026rsquo;s place. [CHANGE] Removed unused /validate_expr endpoint. #2152 [CHANGE] Updated Prometheus dependency to v2.16.0. This Prometheus version uses Active Query Tracker to limit concurrent queries. In order to keep -querier.max-concurrent working, Active Query Tracker is enabled by default, and is configured to store its data to active-query-tracker directory (relative to current directory when Cortex started). This can be changed by using -querier.active-query-tracker-dir option. Purpose of Active Query Tracker is to log queries that were running when Cortex crashes. This logging happens on next Cortex start. #2088 [CHANGE] Default to BigChunk encoding; may result in slightly higher disk usage if many timeseries have a constant value, but should generally result in fewer, bigger chunks. #2207 [CHANGE] WAL replays are now done while the rest of Cortex is starting, and more specifically, when HTTP server is running. This makes it possible to scrape metrics during WAL replays. Applies to both chunks and experimental blocks storage. #2222 [CHANGE] Cortex now has /ready probe for all services, not just ingester and querier as before. In single-binary mode, /ready reports 204 only if all components are running properly. #2166 [CHANGE] If you are vendoring Cortex and use its components in your project, be aware that many Cortex components no longer start automatically when they are created. You may want to review PR and attached document. #2166 [CHANGE] Experimental TSDB: the querier in-memory index cache used by the experimental blocks storage shifted from per-tenant to per-querier. The -experimental.tsdb.bucket-store.index-cache-size-bytes now configures the per-querier index cache max size instead of a per-tenant cache and its default has been increased to 1GB. #2189 [CHANGE] Experimental TSDB: TSDB head compaction interval and concurrency is now configurable (defaults to 1 min interval and 5 concurrent head compactions). New options: -experimental.tsdb.head-compaction-interval and -experimental.tsdb.head-compaction-concurrency. #2172 [CHANGE] Experimental TSDB: switched the blocks storage index header to the binary format. This change is expected to have no visible impact, except lower startup times and memory usage in the queriers. It\u0026rsquo;s possible to switch back to the old JSON format via the flag -experimental.tsdb.bucket-store.binary-index-header-enabled=false. #2223 [CHANGE] Experimental Memberlist KV store can now be used in single-binary Cortex. Attempts to use it previously would fail with panic. This change also breaks existing binary protocol used to exchange gossip messages, so this version will not be able to understand gossiped Ring when used in combination with the previous version of Cortex. Easiest way to upgrade is to shutdown old Cortex installation, and restart it with new version. Incremental rollout works too, but with reduced functionality until all components run the same version. #2016 [FEATURE] Added a read-only local alertmanager config store using files named corresponding to their tenant id. #2125 [FEATURE] Added flag -experimental.ruler.enable-api to enable the ruler api which implements the Prometheus API /api/v1/rules and /api/v1/alerts endpoints under the configured -http.prefix. #1999 [FEATURE] Added sharding support to compactor when using the experimental TSDB blocks storage. #2113 [FEATURE] Added ability to override YAML config file settings using environment variables. #2147 -config.expand-env [FEATURE] Added flags to disable Alertmanager notifications methods. #2187 -configs.notifications.disable-email -configs.notifications.disable-webhook [FEATURE] Add /config HTTP endpoint which exposes the current Cortex configuration as YAML. #2165 [FEATURE] Allow Prometheus remote write directly to ingesters. #1491 [FEATURE] Introduced new standalone service query-tee that can be used for testing purposes to send the same Prometheus query to multiple backends (ie. two Cortex clusters ingesting the same metrics) and compare the performances. #2203 [FEATURE] Fan out parallelizable queries to backend queriers concurrently. #1878 querier.parallelise-shardable-queries (bool) Requires a shard-compatible schema (v10+) This causes the number of traces to increase accordingly. The query-frontend now requires a schema config to determine how/when to shard queries, either from a file or from flags (i.e. by the config-yaml CLI flag). This is the same schema config the queriers consume. The schema is only required to use this option. It\u0026rsquo;s also advised to increase downstream concurrency controls as well: querier.max-outstanding-requests-per-tenant querier.max-query-parallelism querier.max-concurrent server.grpc-max-concurrent-streams (for both query-frontends and queriers) [FEATURE] Added user sub rings to distribute users to a subset of ingesters. #1947 -experimental.distributor.user-subring-size [FEATURE] Add flag -experimental.tsdb.stripe-size to expose TSDB stripe size option. #2185 [FEATURE] Experimental Delete Series: Added support for Deleting Series with Prometheus style API. Needs to be enabled first by setting -purger.enable to true. Deletion only supported when using boltdb and filesystem as index and object store respectively. Support for other stores to follow in separate PRs #2103 [ENHANCEMENT] Alertmanager: Expose Per-tenant alertmanager metrics #2124 [ENHANCEMENT] Add status label to cortex_alertmanager_configs metric to gauge the number of valid and invalid configs. #2125 [ENHANCEMENT] Cassandra Authentication: added the custom_authenticators config option that allows users to authenticate with cassandra clusters using password authenticators that are not approved by default in gocql #2093 [ENHANCEMENT] Cassandra Storage: added max_retries, retry_min_backoff and retry_max_backoff configuration options to enable retrying recoverable errors. #2054 [ENHANCEMENT] Allow to configure HTTP and gRPC server listen address, maximum number of simultaneous connections and connection keepalive settings. -server.http-listen-address -server.http-conn-limit -server.grpc-listen-address -server.grpc-conn-limit -server.grpc.keepalive.max-connection-idle -server.grpc.keepalive.max-connection-age -server.grpc.keepalive.max-connection-age-grace -server.grpc.keepalive.time -server.grpc.keepalive.timeout [ENHANCEMENT] PostgreSQL: Bump up github.com/lib/pq from v1.0.0 to v1.3.0 to support PostgreSQL SCRAM-SHA-256 authentication. #2097 [ENHANCEMENT] Cassandra Storage: User no longer need CREATE privilege on \u0026lt;all keyspaces\u0026gt; if given keyspace exists. #2032 [ENHANCEMENT] Cassandra Storage: added password_file configuration options to enable reading Cassandra password from file. #2096 [ENHANCEMENT] Configs API: Allow GET/POST configs in YAML format. #2181 [ENHANCEMENT] Background cache writes are batched to improve parallelism and observability. #2135 [ENHANCEMENT] Add automatic repair for checkpoint and WAL. #2105 [ENHANCEMENT] Support lastEvaluation and evaluationTime in /api/v1/rules endpoints and make order of groups stable. #2196 [ENHANCEMENT] Skip expired requests in query-frontend scheduling. #2082 [ENHANCEMENT] Experimental TSDB: Export TSDB Syncer metrics from Compactor component, they are prefixed with cortex_compactor_. #2023 [ENHANCEMENT] Experimental TSDB: Added dedicated flag -experimental.tsdb.bucket-store.tenant-sync-concurrency to configure the maximum number of concurrent tenants for which blocks are synched. #2026 [ENHANCEMENT] Experimental TSDB: Expose metrics for objstore operations (prefixed with cortex_\u0026lt;component\u0026gt;_thanos_objstore_, component being one of ingester, querier and compactor). #2027 [ENHANCEMENT] Experimental TSDB: Added support for Azure Storage to be used for block storage, in addition to S3 and GCS. #2083 [ENHANCEMENT] Experimental TSDB: Reduced memory allocations in the ingesters when using the experimental blocks storage. #2057 [ENHANCEMENT] Experimental Memberlist KV: expose -memberlist.gossip-to-dead-nodes-time and -memberlist.dead-node-reclaim-time options to control how memberlist library handles dead nodes and name reuse. #2131 [BUGFIX] Alertmanager: fixed panic upon applying a new config, caused by duplicate metrics registration in the NewPipelineBuilder function. #211 [BUGFIX] Azure Blob ChunkStore: Fixed issue causing invalid chunk checksum errors. #2074 [BUGFIX] The gauge cortex_overrides_last_reload_successful is now only exported by components that use a RuntimeConfigManager. Previously, for components that do not initialize a RuntimeConfigManager (such as the compactor) the gauge was initialized with 0 (indicating error state) and then never updated, resulting in a false-negative permanent error state. #2092 [BUGFIX] Fixed WAL metric names, added the cortex_ prefix. [BUGFIX] Restored histogram cortex_configs_request_duration_seconds #2138 [BUGFIX] Fix wrong syntax for url in config-file-reference. #2148 [BUGFIX] Fixed some 5xx status code returned by the query-frontend when they should actually be 4xx. #2122 [BUGFIX] Experimental TSDB: fixed /all_user_stats and /api/prom/user_stats endpoints when using the experimental TSDB blocks storage. #2042 [BUGFIX] Experimental TSDB: fixed ruler to correctly work with the experimental TSDB blocks storage. #2101 Changes to denormalised tokens in the ring Cortex 0.4.0 is the last version that can write denormalised tokens. Cortex 0.5.0 and above always write normalised tokens.\nCortex 0.6.0 is the last version that can read denormalised tokens. Starting with Cortex 0.7.0 only normalised tokens are supported, and ingesters writing denormalised tokens to the ring (running Cortex 0.4.0 or earlier with -ingester.normalise-tokens=false) are ignored by distributors. Such ingesters should either switch to using normalised tokens, or be upgraded to Cortex 0.5.0 or later.\n0.6.1 / 2020-02-05 [BUGFIX] Fixed parsing of the WAL configuration when specified in the YAML config file. #2071 0.6.0 / 2020-01-28 Note that the ruler flags need to be changed in this upgrade. You\u0026rsquo;re moving from a single node ruler to something that might need to be sharded. Further, if you\u0026rsquo;re using the configs service, we\u0026rsquo;ve upgraded the migration library and this requires some manual intervention. See full instructions below to upgrade your PostgreSQL.\n [CHANGE] The frontend component now does not cache results if it finds a Cache-Control header and if one of its values is no-store. #1974 [CHANGE] Flags changed with transition to upstream Prometheus rules manager: -ruler.client-timeout is now ruler.configs.client-timeout in order to match ruler.configs.url. -ruler.group-timeouthas been removed. -ruler.num-workers has been removed. -ruler.rule-path has been added to specify where the prometheus rule manager will sync rule files. -ruler.storage.type has beem added to specify the rule store backend type, currently only the configdb. -ruler.poll-interval has been added to specify the interval in which to poll new rule groups. -ruler.evaluation-interval default value has changed from 15s to 1m to match the default evaluation interval in Prometheus. Ruler sharding requires a ring which can be configured via the ring flags prefixed by ruler.ring.. #1987 [CHANGE] Use relative links from /ring page to make it work when used behind reverse proxy. #1896 [CHANGE] Deprecated -distributor.limiter-reload-period flag. #1766 [CHANGE] Ingesters now write only normalised tokens to the ring, although they can still read denormalised tokens used by other ingesters. -ingester.normalise-tokens is now deprecated, and ignored. If you want to switch back to using denormalised tokens, you need to downgrade to Cortex 0.4.0. Previous versions don\u0026rsquo;t handle claiming tokens from normalised ingesters correctly. #1809 [CHANGE] Overrides mechanism has been renamed to \u0026ldquo;runtime config\u0026rdquo;, and is now separate from limits. Runtime config is simply a file that is reloaded by Cortex every couple of seconds. Limits and now also multi KV use this mechanism.\nNew arguments were introduced: -runtime-config.file (defaults to empty) and -runtime-config.reload-period (defaults to 10 seconds), which replace previously used -limits.per-user-override-config and -limits.per-user-override-period options. Old options are still used if -runtime-config.file is not specified. This change is also reflected in YAML configuration, where old limits.per_tenant_override_config and limits.per_tenant_override_period fields are replaced with runtime_config.file and runtime_config.period respectively. #1749 [CHANGE] Cortex now rejects data with duplicate labels. Previously, such data was accepted, with duplicate labels removed with only one value left. #1964 [CHANGE] Changed the default value for -distributor.ha-tracker.prefix from collectors/ to ha-tracker/ in order to not clash with other keys (ie. ring) stored in the same key-value store. #1940 [FEATURE] Experimental: Write-Ahead-Log added in ingesters for more data reliability against ingester crashes. #1103 --ingester.wal-enabled: Setting this to true enables writing to WAL during ingestion. --ingester.wal-dir: Directory where the WAL data should be stored and/or recovered from. --ingester.checkpoint-enabled: Set this to true to enable checkpointing of in-memory chunks to disk. --ingester.checkpoint-duration: This is the interval at which checkpoints should be created. --ingester.recover-from-wal: Set this to true to recover data from an existing WAL. For more information, please checkout the \u0026ldquo;Ingesters with WAL\u0026rdquo; guide. [FEATURE] The distributor can now drop labels from samples (similar to the removal of the replica label for HA ingestion) per user via the distributor.drop-label flag. #1726 [FEATURE] Added flag debug.mutex-profile-fraction to enable mutex profiling #1969 [FEATURE] Added global ingestion rate limiter strategy. Deprecated -distributor.limiter-reload-period flag. #1766 [FEATURE] Added support for Microsoft Azure blob storage to be used for storing chunk data. #1913 [FEATURE] Added readiness probe endpoint/ready to queriers. #1934 [FEATURE] Added \u0026ldquo;multi\u0026rdquo; KV store that can interact with two other KV stores, primary one for all reads and writes, and secondary one, which only receives writes. Primary/secondary store can be modified in runtime via runtime-config mechanism (previously \u0026ldquo;overrides\u0026rdquo;). #1749 [FEATURE] Added support to store ring tokens to a file and read it back on startup, instead of generating/fetching the tokens to/from the ring. This feature can be enabled with the flag -ingester.tokens-file-path. #1750 [FEATURE] Experimental TSDB: Added /series API endpoint support with TSDB blocks storage. #1830 [FEATURE] Experimental TSDB: Added TSDB blocks compactor component, which iterates over users blocks stored in the bucket and compact them according to the configured block ranges. #1942 [FEATURE] Experimental: Implements gRPC streaming for ingesters when using the experimental TSDB blocks storage. #1845 [ENHANCEMENT] metric cortex_ingester_flush_reasons gets a new reason value: Spread, when -ingester.spread-flushes option is enabled. #1978 [ENHANCEMENT] Added password and enable_tls options to redis cache configuration. Enables usage of Microsoft Azure Cache for Redis service. #1923 [ENHANCEMENT] Upgraded Kubernetes API version for deployments from extensions/v1beta1 to apps/v1. #1941 [ENHANCEMENT] Add ability to configure gRPC keepalive settings. #2066 [ENHANCEMENT] Experimental TSDB: Open existing TSDB on startup to prevent ingester from becoming ready before it can accept writes. The max concurrency is set via --experimental.tsdb.max-tsdb-opening-concurrency-on-startup. #1917 [ENHANCEMENT] Experimental TSDB: Querier now exports aggregate metrics from Thanos bucket store and in memory index cache (many metrics to list, but all have cortex_querier_bucket_store_ or cortex_querier_blocks_index_cache_ prefix). #1996 [ENHANCEMENT] Experimental TSDB: Improved multi-tenant bucket store. #1991 Allowed to configure the blocks sync interval via -experimental.tsdb.bucket-store.sync-interval (0 disables the sync) Limited the number of tenants concurrently synched by -experimental.tsdb.bucket-store.block-sync-concurrency Renamed cortex_querier_sync_seconds metric to cortex_querier_blocks_sync_seconds Track cortex_querier_blocks_sync_seconds metric for the initial sync too [BUGFIX] Fixed unnecessary CAS operations done by the HA tracker when the jitter is enabled. #1861 [BUGFIX] Fixed ingesters getting stuck in a LEAVING state after coming up from an ungraceful exit. #1921 [BUGFIX] Reduce memory usage when ingester Push() errors. #1922 [BUGFIX] Table Manager: Fixed calculation of expected tables and creation of tables from next active schema considering grace period. #1976 [BUGFIX] Fixed leaked goroutines in the querier. #2070 [BUGFIX] Experimental TSDB: Fixed ingesters consistency during hand-over when using experimental TSDB blocks storage. #1854 #1818 [BUGFIX] Experimental TSDB: Fixed metrics when using experimental TSDB blocks storage. #1981 #1982 #1990 #1983 [BUGFIX] Experimental memberlist: Use the advertised address when sending packets to other peers of the Gossip memberlist. #1857 Upgrading PostgreSQL (if you\u0026rsquo;re using configs service) Reference: https://github.com/golang-migrate/migrate/tree/master/database/postgres#upgrading-from-v1\n Install the migrate package cli tool: https://github.com/golang-migrate/migrate/tree/master/cmd/migrate#installation Drop the schema_migrations table: DROP TABLE schema_migrations;. Run the migrate command:\nmigrate -path \u0026lt;absolute_path_to_cortex\u0026gt;/cmd/cortex/migrations -database postgres://localhost:5432/database force 2 Known issues The cortex_prometheus_rule_group_last_evaluation_timestamp_seconds metric, tracked by the ruler, is not unregistered for rule groups not being used anymore. This issue will be fixed in the next Cortex release (see 2033).\n Write-Ahead-Log (WAL) does not have automatic repair of corrupt checkpoint or WAL segments, which is possible if ingester crashes abruptly or the underlying disk corrupts. Currently the only way to resolve this is to manually delete the affected checkpoint and/or WAL segments. Automatic repair will be added in the future releases.\n 0.4.0 / 2019-12-02 [CHANGE] The frontend component has been refactored to be easier to re-use. When upgrading the frontend, cache entries will be discarded and re-created with the new protobuf schema. #1734 [CHANGE] Removed direct DB/API access from the ruler. -ruler.configs.url has been now deprecated. #1579 [CHANGE] Removed Delta encoding. Any old chunks with Delta encoding cannot be read anymore. If ingester.chunk-encoding is set to Delta the ingester will fail to start. #1706 [CHANGE] Setting -ingester.max-transfer-retries to 0 now disables hand-over when ingester is shutting down. Previously, zero meant infinite number of attempts. #1771 [CHANGE] dynamo has been removed as a valid storage name to make it consistent for all components. aws and aws-dynamo remain as valid storage names. [CHANGE/FEATURE] The frontend split and cache intervals can now be configured using the respective flag --querier.split-queries-by-interval and --frontend.cache-split-interval. If --querier.split-queries-by-interval is not provided request splitting is disabled by default. --querier.split-queries-by-day is still accepted for backward compatibility but has been deprecated. You should now use --querier.split-queries-by-interval. We recommend a to use a multiple of 24 hours. [FEATURE] Global limit on the max series per user and metric #1760 -ingester.max-global-series-per-user -ingester.max-global-series-per-metric Requires -distributor.replication-factor and -distributor.shard-by-all-labels set for the ingesters too [FEATURE] Flush chunks with stale markers early with ingester.max-stale-chunk-idle. #1759 [FEATURE] EXPERIMENTAL: Added new KV Store backend based on memberlist library. Components can gossip about tokens and ingester states, instead of using Consul or Etcd. #1721 [FEATURE] EXPERIMENTAL: Use TSDB in the ingesters \u0026amp; flush blocks to S3/GCS ala Thanos. This will let us use an Object Store more efficiently and reduce costs. #1695 [FEATURE] Allow Query Frontend to log slow queries with frontend.log-queries-longer-than. #1744 [FEATURE] Add HTTP handler to trigger ingester flush \u0026amp; shutdown - used when running as a stateful set with the WAL enabled. #1746 [FEATURE] EXPERIMENTAL: Added GCS support to TSDB blocks storage. #1772 [ENHANCEMENT] Reduce memory allocations in the write path. #1706 [ENHANCEMENT] Consul client now follows recommended practices for blocking queries wrt returned Index value. #1708 [ENHANCEMENT] Consul client can optionally rate-limit itself during Watch (used e.g. by ring watchers) and WatchPrefix (used by HA feature) operations. Rate limiting is disabled by default. New flags added: --consul.watch-rate-limit, and --consul.watch-burst-size. #1708 [ENHANCEMENT] Added jitter to HA deduping heartbeats, configure using distributor.ha-tracker.update-timeout-jitter-max #1534 [ENHANCEMENT] Add ability to flush chunks with stale markers early. #1759 [BUGFIX] Stop reporting successful actions as 500 errors in KV store metrics. #1798 [BUGFIX] Fix bug where duplicate labels can be returned through metadata APIs. #1790 [BUGFIX] Fix reading of old, v3 chunk data. #1779 [BUGFIX] Now support IAM roles in service accounts in AWS EKS. #1803 [BUGFIX] Fixed duplicated series returned when querying both ingesters and store with the experimental TSDB blocks storage. #1778 In this release we updated the following dependencies:\n gRPC v1.25.0 (resulted in a drop of 30% CPU usage when compression is on) jaeger-client v2.20.0 aws-sdk-go to v1.25.22 0.3.0 / 2019-10-11 This release adds support for Redis as an alternative to Memcached, and also includes many optimisations which reduce CPU and memory usage.\n [CHANGE] Gauge metrics were renamed to drop the _total suffix. #1685 In Alertmanager, alertmanager_configs_total is now alertmanager_configs In Ruler, scheduler_configs_total is now scheduler_configs scheduler_groups_total is now scheduler_groups. [CHANGE] --alertmanager.configs.auto-slack-root flag was dropped as auto Slack root is not supported anymore. #1597 [CHANGE] In table-manager, default DynamoDB capacity was reduced from 3,000 units to 1,000 units. We recommend you do not run with the defaults: find out what figures are needed for your environment and set that via -dynamodb.periodic-table.write-throughput and -dynamodb.chunk-table.write-throughput. [FEATURE] Add Redis support for caching #1612 [FEATURE] Allow spreading chunk writes across multiple S3 buckets #1625 [FEATURE] Added /shutdown endpoint for ingester to shutdown all operations of the ingester. #1746 [ENHANCEMENT] Upgraded Prometheus to 2.12.0 and Alertmanager to 0.19.0. #1597 [ENHANCEMENT] Cortex is now built with Go 1.13 #1675, #1676, #1679 [ENHANCEMENT] Many optimisations, mostly impacting ingester and querier: #1574, #1624, #1638, #1644, #1649, #1654, #1702 Full list of changes: https://github.com/cortexproject/cortex/compare/v0.2.0...v0.3.0\n0.2.0 / 2019-09-05 This release has several exciting features, the most notable of them being setting -ingester.spread-flushes to potentially reduce your storage space by upto 50%.\n [CHANGE] Flags changed due to changes upstream in Prometheus Alertmanager #929: alertmanager.mesh.listen-address is now cluster.listen-address alertmanager.mesh.peer.host and alertmanager.mesh.peer.service can be replaced by cluster.peer alertmanager.mesh.hardware-address, alertmanager.mesh.nickname, alertmanager.mesh.password, and alertmanager.mesh.peer.refresh-interval all disappear. [CHANGE] \u0026ndash;claim-on-rollout flag deprecated; feature is now always on #1566 [CHANGE] Retention period must now be a multiple of periodic table duration #1564 [CHANGE] The value for the name label for the chunks memcache in all cortex_cache_ metrics is now chunksmemcache (before it was memcache) #1569 [FEATURE] Makes the ingester flush each timeseries at a specific point in the max-chunk-age cycle with -ingester.spread-flushes. This means multiple replicas of a chunk are very likely to contain the same contents which cuts chunk storage space by up to 66%. #1578 [FEATURE] Make minimum number of chunk samples configurable per user #1620 [FEATURE] Honor HTTPS for custom S3 URLs #1603 [FEATURE] You can now point the query-frontend at a normal Prometheus for parallelisation and caching #1441 [FEATURE] You can now specify http_config on alert receivers #929 [FEATURE] Add option to use jump hashing to load balance requests to memcached #1554 [FEATURE] Add status page for HA tracker to distributors #1546 [FEATURE] The distributor ring page is now easier to read with alternate rows grayed out #1621 0.1.0 / 2019-08-07 [CHANGE] HA Tracker flags were renamed to provide more clarity #1465 distributor.accept-ha-labels is now distributor.ha-tracker.enable distributor.accept-ha-samples is now distributor.ha-tracker.enable-for-all-users ha-tracker.replica is now distributor.ha-tracker.replica ha-tracker.cluster is now distributor.ha-tracker.cluster [FEATURE] You can specify \u0026ldquo;heap ballast\u0026rdquo; to reduce Go GC Churn #1489 [BUGFIX] HA Tracker no longer always makes a request to Consul/Etcd when a request is not from the active replica #1516 [BUGFIX] Queries are now correctly cancelled by the query-frontend #1508 ","excerpt":"master / unreleased [CHANGE] Utilize separate protos for rule state and storage. Experimental ruler …","ref":"/docs/changelog/","title":"Changelog"},{"body":"Cortex follows the CNCF Code of Conduct.\n","excerpt":"Cortex follows the CNCF Code of Conduct.","ref":"/docs/code-of-conduct/","title":"Code of Conduct"},{"body":"","excerpt":"","ref":"/index.json","title":""},{"body":" Horizontally scalable, highly available, multi-tenant, long term Prometheus. Learn More Releases Companies using Cortex\n Long term storage Durably store data for longer than the lifetime of any single machine, and use this data for long term capacity planning. Blazin\u0026rsquo; fast PromQL Cortex makes your PromQL queries blazin' fast through aggressive parallelization and caching. A global view of data Cortex gives you a global view of Prometheus time series data that includes data in long-term storage, greatly expanding the usefulness of PromQL for analytical purposes. Horizontally scalable Cortex runs across multiple machines in a cluster, exceeding the throughput and storage of a single machine. This enables you to send the metrics from multiple Prometheus servers to a single Cortex cluster. We are a Cloud Native Computing Foundation Sandbox project.\n Join the community ! Join users and companies that are using Cortex in production.\n Slack Issues Twitter ","excerpt":"Horizontally scalable, highly available, multi-tenant, long term Prometheus. Learn More Releases …","ref":"/","title":"Cortex"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"}]