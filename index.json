[{"body":" The blocks storage is an experimental Cortex storage engine based on Prometheus TSDB: it stores each tenant\u0026rsquo;s time series into their own TSDB which write out their series to a on-disk Block (defaults to 2h block range periods). Each Block is composed by chunk files - containing the timestamp-value pairs for multiple series - and an index, which indexes metric names and labels to time series in the chunk files.\nThe supported backends for the blocks storage are:\n Amazon S3 Google Cloud Storage Internally, this storage engine is based on Thanos, but no Thanos knowledge is required in order to run it.\nThe rest of the document assumes you have read the Cortex architecture documentation.\nHow it works When the blocks storage is used, each ingester creates a per-tenant TSDB and ships the TSDB Blocks - which by default are cut every 2 hours - to the long-term storage.\nQueriers periodically iterate over the storage bucket to discover recently uploaded Blocks and - for each Block - download a subset of the block index - called \u0026ldquo;index header\u0026rdquo; - which is kept in memory and used to provide fast lookups.\nThe write path Ingesters receive incoming samples from the distributors. Each push request belongs to a tenant, and the ingester append the received samples to the specific per-tenant TSDB. The received samples are both kept in-memory and written to a write-ahead log (WAL) stored on the local disk and used to recover the in-memory series in case the ingester abruptly terminates. The per-tenant TSDB is lazily created in each ingester upon the first push request is received for that tenant.\nThe in-memory samples are periodically flushed to disk - and the WAL truncated - when a new TSDB Block is cut, which by default occurs every 2 hours. Each new Block cut is then uploaded to the long-term storage and kept in the ingester for some more time, in order to give queriers enough time to discover the new Block from the storage and download its index header.\nIn order to effectively use the WAL and being able to recover the in-memory series upon ingester abruptly termination, the WAL needs to be stored to a persistent local disk which can survive in the event of an ingester failure (ie. AWS EBS volume or GCP persistent disk when running in the cloud). For example, if you\u0026rsquo;re running the Cortex cluster in Kubernetes, you may use a StatefulSet with a persistent volume claim for the ingesters.\nThe read path Queriers - at startup - iterate over the entire storage bucket to discover all tenants Blocks and - for each of them - download the index header. During this initial synchronization phase, a querier is not ready to handle incoming queries yet and its /ready readiness probe endpoint will fail.\nQueriers also periodically re-iterate over the storage bucket to discover newly uploaded Blocks (by the ingesters) and find out Blocks deleted in the meanwhile, as effect of an optional retention policy.\nThe blocks chunks and the entire index is never fully downloaded by the queriers. In the read path, a querier lookups the series label names and values using the in-memory index header and then download the required segments of the index and chunks for the matching series directly from the long-term storage using byte-range requests.\nThe index header is also stored to the local disk, in order to avoid to re-download it on subsequent restarts of a querier. For this reason, it\u0026rsquo;s recommended - but not required - to run the querier with a persistent local disk. For example, if you\u0026rsquo;re running the Cortex cluster in Kubernetes, you may use a StatefulSet with a persistent volume claim for the queriers.\nSharding and Replication The series sharding and replication doesn\u0026rsquo;t change based on the storage engine, so the general overview provided by the \u0026ldquo;Cortex architecture\u0026rdquo; documentation applies to the blocks storage as well.\nIt\u0026rsquo;s important to note that - differently than the chunks storage - time series are effectively written N times to the long-term storage, where N is the replication factor (typically 3). This may lead to a storage utilization N times more than the chunks storage, but is actually mitigated by the compactor service (see \u0026ldquo;vertical compaction\u0026rdquo;).\nCompactor The compactor is an optional - but highly recommended - service which compacts multiple Blocks of a given tenant into a single optimized larger Block. The compactor has two main benefits:\n Vertically compact Blocks uploaded by all ingesters for the same time range Horizontally compact Blocks with small time ranges into a single larger Block The vertical compaction compacts all the Blocks of a tenant uploaded by any ingester for the same Block range period (defaults to 2 hours) into a single Block, de-duplicating samples that are originally written to N Blocks as effect of the replication.\nThe horizontal compaction triggers after the vertical compaction and compacts several Blocks belonging to adjacent small range periods (2 hours) into a single larger Block. Despite the total block chunks size doesn\u0026rsquo;t change after this compaction, it may have a significative impact on the reduction of the index size and its index header kept in memory by queriers.\nThe compactor is stateless.\nConfiguration The general configuration documentation also applied to a Cortex cluster running the blocks storage, with few differences:\n storage_config tsdb_config compactor_config storage_config The storage_config block configures the storage engine.\nstorage:# The storage engine to use. Use \u0026#34;tsdb\u0026#34; for the blocks storage.# CLI flag: -store.engineengine:tsdb tsdb_config The tsdb_config block configures the blocks storage engine (based on TSDB).\ntsdb:# Backend storage to use. Either \u0026#34;s3\u0026#34; or \u0026#34;gcs\u0026#34;.# CLI flag: -experimental.tsdb.backendbackend:\u0026lt;string\u0026gt; # Local directory to store TSDBs in the ingesters.# CLI flag: -experimental.tsdb.dir[dir:\u0026lt;string\u0026gt;|default=\u0026#34;tsdb\u0026#34;]# TSDB blocks range period.# CLI flag: -experimental.tsdb.block-ranges-period[block_ranges_period:\u0026lt;listofduration\u0026gt;|default=[2h]]# TSDB blocks retention in the ingester before a block is removed. This# should be larger than the block_ranges_period and large enough to give# ingesters enough time to discover newly uploaded blocks.# CLI flag: -experimental.tsdb.retention-period duration[retention_period:\u0026lt;duration\u0026gt;|default=6h]# The frequency at which the ingester shipper look for unshipped TSDB blocks# and start uploading them to the long-term storage.# CLI flag: -experimental.tsdb.ship-interval duration[ship_interval:\u0026lt;duration\u0026gt;|default=30s]# The bucket store configuration applies to queriers and configure how queriers# iteract with the long-term storage backend.bucket_store:# Directory to store synched TSDB index headers.# CLI flag: -experimental.tsdb.bucket-store.sync-dir[sync_dir:\u0026lt;string\u0026gt;|default=\u0026#34;tsdb-sync\u0026#34;]# Size - in bytes - of a per-tenant in-memory index cache used to speed up# blocks index lookups.# CLI flag: -experimental.tsdb.bucket-store.index-cache-size-bytes[index_cache_size_bytes:\u0026lt;int\u0026gt;|default=262144000]# Max size - in bytes - of a per-tenant chunk pool, used to reduce memory# allocations.# CLI flag: -experimental.tsdb.bucket-store.max-chunk-pool-bytes[max_chunk_pool_bytes:\u0026lt;int\u0026gt;|default=2147483648]# Max number of samples per query when loading series from the long-term# storage. 0 disables the limit.# CLI flag: -experimental.tsdb.bucket-store.max-sample-count[max_sample_count:\u0026lt;int\u0026gt;|default=0]# Max number of concurrent queries to execute against the long-term storage# on a per-tenant basis.# CLI flag: -experimental.tsdb.bucket-store.max-concurrent[max_concurrent:\u0026lt;int\u0026gt;|default=20]# Number of Go routines, per tenant, to use when syncing blocks from the# long-term storage.# CLI flag: -experimental.tsdb.bucket-store.block-sync-concurrency[block_sync_concurrency:\u0026lt;int\u0026gt;|default=20s]# Configures the S3 storage backend.# Required only when \u0026#34;s3\u0026#34; backend has been selected.s3:# S3 bucket name.# CLI flag: -experimental.tsdb.s3.bucket-name stringbucket_name:\u0026lt;string\u0026gt; # S3 access key ID. If empty, fallbacks to AWS SDK default logic.# CLI flag: -experimental.tsdb.s3.access-key-id string[access_key_id:\u0026lt;string\u0026gt;]# S3 secret access key. If empty, fallbacks to AWS SDK default logic.# CLI flag: -experimental.tsdb.s3.secret-access-key string[secret_access_key:\u0026lt;string\u0026gt;]# S3 endpoint without schema. By defaults it use the AWS S3 endpoint.# CLI flag: -experimental.tsdb.s3.endpoint[endpoint:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# If enabled, use http:// for the S3 endpoint instead of https://.# This could be useful in local dev/test environments while using# an S3-compatible backend storage, like Minio.# CLI flag: -experimental.tsdb.s3.insecure[insecure:\u0026lt;boolean\u0026gt;|default=false]# Configures the GCS storage backend.# Required only when \u0026#34;gcs\u0026#34; backend has been selected.gcs:# GCS bucket name.# CLI flag: -experimental.tsdb.gcs.bucket-namebucket_name:\u0026lt;string\u0026gt; # JSON representing either a Google Developers Console client_credentials.json# file or a Google Developers service account key file. If empty, fallbacks to# Google SDK default logic.# CLI flag: -experimental.tsdb.gcs.service-account string[service_account:\u0026lt;string\u0026gt;] compactor_config The compactor_config block configures the optional compactor service.\ncompactor:# List of compaction time ranges.# CLI flag: -compactor.block-ranges[block_ranges:\u0026lt;listofduration\u0026gt;|default=[2h,12h,24h]]# Number of Go routines to use when syncing block metadata from the long-term# storage.# CLI flag: -compactor.block-sync-concurrency[block_sync_concurrency:\u0026lt;int\u0026gt;|default=20]# Minimum age of fresh (non-compacted) blocks before they are being processed,# in order to skip blocks that are still uploading from ingesters. Malformed# blocks older than the maximum of consistency-delay and 30m will be removed.# CLI flag: -compactor.consistency-delay[consistency_delay:\u0026lt;duration\u0026gt;|default=30m]# Directory in which to cache downloaded blocks to compact and to store the# newly created block during the compaction process.# CLI flag: -compactor.data-dir[data_dir:\u0026lt;string\u0026gt;|default=\u0026#34;./data\u0026#34;]# The frequency at which the compactor should look for new blocks eligible for# compaction and trigger their compaction.# CLI flag: -compactor.compaction-interval[compaction_interval:\u0026lt;duration\u0026gt;|default=1h]# How many times to retry a failed compaction during a single compaction# interval.# CLI flag: -compactor.compaction-retries[compaction_retries:\u0026lt;int\u0026gt;|default=3] Known issues Horizontal scalability The compactor currently doesn\u0026rsquo;t support horizontal scalability and only 1 replica of the compactor should run at any given time within a Cortex cluster.\nMigrating from the chunks to the blocks storage Currently, no smooth migration path is provided to migrate from chunks to blocks storage. For this reason, the blocks storage can only be enabled in new Cortex clusters.\n","excerpt":"The blocks storage is an experimental Cortex storage engine based on Prometheus TSDB: it stores each …","ref":"/docs/operations/blocks-storage/","title":"Blocks storage (experimental)"},{"body":" Cortex can be configured using a YAML file - specified using the -config.file flag - or CLI flags. In case you combine both, CLI flags take precedence over the YAML config file.\nReference To specify which configuration file to load, pass the -config.file flag at the command line. The file is written in YAML format, defined by the scheme below. Brackets indicate that a parameter is optional.\nGeneric placeholders are defined as follows:\n \u0026lt;boolean\u0026gt;: a boolean that can take the values true or false \u0026lt;int\u0026gt;: any integer matching the regular expression [1-9]+[0-9]* \u0026lt;duration\u0026gt;: a duration matching the regular expression [0-9]+(ns|us|µs|ms|[smh]) \u0026lt;string\u0026gt;: a regular string \u0026lt;url\u0026gt;: an URL \u0026lt;prefix\u0026gt;: a CLI flag prefix based on the context (look at the parent configuration block to see which CLI flags prefix should be used) Supported contents and default values of the config file:\n# The Cortex service to run. Supported values are: all, distributor, ingester,# querier, query-frontend, table-manager, ruler, alertmanager, configs.# CLI flag: -target[target:\u0026lt;string\u0026gt;|default=\u0026#34;all\u0026#34;]# Set to false to disable auth.# CLI flag: -auth.enabled[auth_enabled:\u0026lt;boolean\u0026gt;|default=true]# HTTP path prefix for Cortex API.# CLI flag: -http.prefix[http_prefix:\u0026lt;string\u0026gt;|default=\u0026#34;/api/prom\u0026#34;]# The server_config configures the HTTP and gRPC server of the launched# service(s).[server:\u0026lt;server_config\u0026gt;]# The distributor_config configures the Cortex distributor.[distributor:\u0026lt;distributor_config\u0026gt;]# The querier_config configures the Cortex querier.[querier:\u0026lt;querier_config\u0026gt;]# The ingester_client_config configures how the Cortex distributors connect to# the ingesters.[ingester_client:\u0026lt;ingester_client_config\u0026gt;]# The ingester_config configures the Cortex ingester.[ingester:\u0026lt;ingester_config\u0026gt;]# The storage_config configures where Cortex stores the data (chunks storage# engine).[storage:\u0026lt;storage_config\u0026gt;]# The chunk_store_config configures how Cortex stores the data (chunks storage# engine).[chunk_store:\u0026lt;chunk_store_config\u0026gt;]# The limits_config configures default and per-tenant limits imposed by Cortex# services (ie. distributor, ingester, ...).[limits:\u0026lt;limits_config\u0026gt;]# The frontend_worker_config configures the worker - running within the Cortex# ingester - picking up and executing queries enqueued by the query-frontend.[frontend_worker:\u0026lt;frontend_worker_config\u0026gt;]# The query_frontend_config configures the Cortex query-frontend.[frontend:\u0026lt;query_frontend_config\u0026gt;]# The queryrange_config configures the query splitting and caching in the Cortex# query-frontend.[query_range:\u0026lt;queryrange_config\u0026gt;]# The table_manager_config configures the Cortex table-manager.[table_manager:\u0026lt;table_manager_config\u0026gt;]# The ruler_config configures the Cortex ruler.[ruler:\u0026lt;ruler_config\u0026gt;]# The configdb_config configures the config database storing rules and alerts,# and used by the \u0026#39;configs\u0026#39; service to expose APIs to manage them.[configdb:\u0026lt;configdb_config\u0026gt;]# The configstore_config configures the config database storing rules and# alerts, and is used by the Cortex alertmanager.# The CLI flags prefix for this block config is: alertmanager[config_store:\u0026lt;configstore_config\u0026gt;]# The alertmanager_config configures the Cortex alertmanager.[alertmanager:\u0026lt;alertmanager_config\u0026gt;]runtime_config:# How often to check runtime config file.# CLI flag: -runtime-config.reload-period[period:\u0026lt;duration\u0026gt;|default=10s]# File with the configuration that can be updated in runtime.# CLI flag: -runtime-config.file[file:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;] server_config The server_config configures the HTTP and gRPC server of the launched service(s).\n# HTTP server listen port.# CLI flag: -server.http-listen-port[http_listen_port:\u0026lt;int\u0026gt;|default=80]# gRPC server listen port.# CLI flag: -server.grpc-listen-port[grpc_listen_port:\u0026lt;int\u0026gt;|default=9095]# Register the intrumentation handlers (/metrics etc).# CLI flag: -server.register-instrumentation[register_instrumentation:\u0026lt;boolean\u0026gt;|default=true]# Timeout for graceful shutdowns# CLI flag: -server.graceful-shutdown-timeout[graceful_shutdown_timeout:\u0026lt;duration\u0026gt;|default=30s]# Read timeout for HTTP server# CLI flag: -server.http-read-timeout[http_server_read_timeout:\u0026lt;duration\u0026gt;|default=30s]# Write timeout for HTTP server# CLI flag: -server.http-write-timeout[http_server_write_timeout:\u0026lt;duration\u0026gt;|default=30s]# Idle timeout for HTTP server# CLI flag: -server.http-idle-timeout[http_server_idle_timeout:\u0026lt;duration\u0026gt;|default=2m0s]# Limit on the size of a gRPC message this server can receive (bytes).# CLI flag: -server.grpc-max-recv-msg-size-bytes[grpc_server_max_recv_msg_size:\u0026lt;int\u0026gt;|default=4194304]# Limit on the size of a gRPC message this server can send (bytes).# CLI flag: -server.grpc-max-send-msg-size-bytes[grpc_server_max_send_msg_size:\u0026lt;int\u0026gt;|default=4194304]# Limit on the number of concurrent streams for gRPC calls (0 = unlimited)# CLI flag: -server.grpc-max-concurrent-streams[grpc_server_max_concurrent_streams:\u0026lt;int\u0026gt;|default=100]# Only log messages with the given severity or above. Valid levels: [debug,# info, warn, error]# CLI flag: -log.level[log_level:\u0026lt;string\u0026gt;|default=\u0026#34;info\u0026#34;]# Base path to serve all API routes from (e.g. /v1/)# CLI flag: -server.path-prefix[http_path_prefix:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;] distributor_config The distributor_config configures the Cortex distributor.\n# Report number of ingested samples to billing system.# CLI flag: -distributor.enable-billing[enable_billing:\u0026lt;boolean\u0026gt;|default=false]billing:# Maximum number of billing events to buffer in memory# CLI flag: -billing.max-buffered-events[maxbufferedevents:\u0026lt;int\u0026gt;|default=1024]# How often to retry sending events to the billing ingester.# CLI flag: -billing.retry-delay[retrydelay:\u0026lt;duration\u0026gt;|default=500ms]# points to the billing ingester sidecar (should be on localhost)# CLI flag: -billing.ingester[ingesterhostport:\u0026lt;string\u0026gt;|default=\u0026#34;localhost:24225\u0026#34;]pool:# How frequently to clean up clients for ingesters that have gone away.# CLI flag: -distributor.client-cleanup-period[client_cleanup_period:\u0026lt;duration\u0026gt;|default=15s]# Run a health check on each ingester client during periodic cleanup.# CLI flag: -distributor.health-check-ingesters[health_check_ingesters:\u0026lt;boolean\u0026gt;|default=false]ha_tracker:# Enable the distributors HA tracker so that it can accept samples from# Prometheus HA replicas gracefully (requires labels).# CLI flag: -distributor.ha-tracker.enable[enable_ha_tracker:\u0026lt;boolean\u0026gt;|default=false]# Update the timestamp in the KV store for a given cluster/replica only after# this amount of time has passed since the current stored timestamp.# CLI flag: -distributor.ha-tracker.update-timeout[ha_tracker_update_timeout:\u0026lt;duration\u0026gt;|default=15s]# Maximum jitter applied to the update timeout, in order to spread the HA# heartbeats over time.# CLI flag: -distributor.ha-tracker.update-timeout-jitter-max[ha_tracker_update_timeout_jitter_max:\u0026lt;duration\u0026gt;|default=5s]# If we don\u0026#39;t receive any samples from the accepted replica for a cluster in# this amount of time we will failover to the next replica we receive a sample# from. This value must be greater than the update timeout# CLI flag: -distributor.ha-tracker.failover-timeout[ha_tracker_failover_timeout:\u0026lt;duration\u0026gt;|default=30s]kvstore:# Backend storage to use for the ring. Supported values are: consul, etcd,# inmemory, multi, memberlist (experimental).# CLI flag: -distributor.ha-tracker.store[store:\u0026lt;string\u0026gt;|default=\u0026#34;consul\u0026#34;]# The prefix for the keys in the store. Should end with a /.# CLI flag: -distributor.ha-tracker.prefix[prefix:\u0026lt;string\u0026gt;|default=\u0026#34;ha-tracker/\u0026#34;]# The consul_config configures the consul client.# The CLI flags prefix for this block config is: distributor.ha-tracker[consul:\u0026lt;consul_config\u0026gt;]# The etcd_config configures the etcd client.# The CLI flags prefix for this block config is: distributor.ha-tracker[etcd:\u0026lt;etcd_config\u0026gt;]# The memberlist_config configures the Gossip memberlist.# The CLI flags prefix for this block config is: distributor.ha-tracker[memberlist:\u0026lt;memberlist_config\u0026gt;]multi:# Primary backend storage used by multi-client.# CLI flag: -distributor.ha-tracker.multi.primary[primary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Secondary backend storage used by multi-client.# CLI flag: -distributor.ha-tracker.multi.secondary[secondary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Mirror writes to secondary store.# CLI flag: -distributor.ha-tracker.multi.mirror-enabled[mirror_enabled:\u0026lt;boolean\u0026gt;|default=false]# Timeout for storing value to secondary store.# CLI flag: -distributor.ha-tracker.multi.mirror-timeout[mirror_timeout:\u0026lt;duration\u0026gt;|default=2s]# remote_write API max receive message size (bytes).# CLI flag: -distributor.max-recv-msg-size[max_recv_msg_size:\u0026lt;int\u0026gt;|default=104857600]# Timeout for downstream ingesters.# CLI flag: -distributor.remote-timeout[remote_timeout:\u0026lt;duration\u0026gt;|default=2s]# Time to wait before sending more than the minimum successful query requests.# CLI flag: -distributor.extra-query-delay[extra_queue_delay:\u0026lt;duration\u0026gt;|default=0s]# Distribute samples based on all labels, as opposed to solely by user and# metric name.# CLI flag: -distributor.shard-by-all-labels[shard_by_all_labels:\u0026lt;boolean\u0026gt;|default=false]ring:kvstore:# Backend storage to use for the ring. Supported values are: consul, etcd,# inmemory, multi, memberlist (experimental).# CLI flag: -distributor.ring.store[store:\u0026lt;string\u0026gt;|default=\u0026#34;consul\u0026#34;]# The prefix for the keys in the store. Should end with a /.# CLI flag: -distributor.ring.prefix[prefix:\u0026lt;string\u0026gt;|default=\u0026#34;collectors/\u0026#34;]# The consul_config configures the consul client.# The CLI flags prefix for this block config is: distributor.ring[consul:\u0026lt;consul_config\u0026gt;]# The etcd_config configures the etcd client.# The CLI flags prefix for this block config is: distributor.ring[etcd:\u0026lt;etcd_config\u0026gt;]# The memberlist_config configures the Gossip memberlist.# The CLI flags prefix for this block config is: distributor.ring[memberlist:\u0026lt;memberlist_config\u0026gt;]multi:# Primary backend storage used by multi-client.# CLI flag: -distributor.ring.multi.primary[primary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Secondary backend storage used by multi-client.# CLI flag: -distributor.ring.multi.secondary[secondary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Mirror writes to secondary store.# CLI flag: -distributor.ring.multi.mirror-enabled[mirror_enabled:\u0026lt;boolean\u0026gt;|default=false]# Timeout for storing value to secondary store.# CLI flag: -distributor.ring.multi.mirror-timeout[mirror_timeout:\u0026lt;duration\u0026gt;|default=2s]# Period at which to heartbeat to the ring.# CLI flag: -distributor.ring.heartbeat-period[heartbeat_period:\u0026lt;duration\u0026gt;|default=5s]# The heartbeat timeout after which distributors are considered unhealthy# within the ring.# CLI flag: -distributor.ring.heartbeat-timeout[heartbeat_timeout:\u0026lt;duration\u0026gt;|default=1m0s] ingester_config The ingester_config configures the Cortex ingester.\nlifecycler:ring:kvstore:# Backend storage to use for the ring. Supported values are: consul, etcd,# inmemory, multi, memberlist (experimental).# CLI flag: -ring.store[store:\u0026lt;string\u0026gt;|default=\u0026#34;consul\u0026#34;]# The prefix for the keys in the store. Should end with a /.# CLI flag: -ring.prefix[prefix:\u0026lt;string\u0026gt;|default=\u0026#34;collectors/\u0026#34;]# The consul_config configures the consul client.[consul:\u0026lt;consul_config\u0026gt;]# The etcd_config configures the etcd client.[etcd:\u0026lt;etcd_config\u0026gt;]# The memberlist_config configures the Gossip memberlist.[memberlist:\u0026lt;memberlist_config\u0026gt;]multi:# Primary backend storage used by multi-client.# CLI flag: -multi.primary[primary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Secondary backend storage used by multi-client.# CLI flag: -multi.secondary[secondary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Mirror writes to secondary store.# CLI flag: -multi.mirror-enabled[mirror_enabled:\u0026lt;boolean\u0026gt;|default=false]# Timeout for storing value to secondary store.# CLI flag: -multi.mirror-timeout[mirror_timeout:\u0026lt;duration\u0026gt;|default=2s]# The heartbeat timeout after which ingesters are skipped for reads/writes.# CLI flag: -ring.heartbeat-timeout[heartbeat_timeout:\u0026lt;duration\u0026gt;|default=1m0s]# The number of ingesters to write to and read from.# CLI flag: -distributor.replication-factor[replication_factor:\u0026lt;int\u0026gt;|default=3]# Number of tokens for each ingester.# CLI flag: -ingester.num-tokens[num_tokens:\u0026lt;int\u0026gt;|default=128]# Period at which to heartbeat to consul.# CLI flag: -ingester.heartbeat-period[heartbeat_period:\u0026lt;duration\u0026gt;|default=5s]# Observe tokens after generating to resolve collisions. Useful when using# gossiping ring.# CLI flag: -ingester.observe-period[observe_period:\u0026lt;duration\u0026gt;|default=0s]# Period to wait for a claim from another member; will join automatically# after this.# CLI flag: -ingester.join-after[join_after:\u0026lt;duration\u0026gt;|default=0s]# Minimum duration to wait before becoming ready. This is to work around race# conditions with ingesters exiting and updating the ring.# CLI flag: -ingester.min-ready-duration[min_ready_duration:\u0026lt;duration\u0026gt;|default=1m0s]# Name of network interface to read address from.# CLI flag: -ingester.lifecycler.interface[interface_names:\u0026lt;listofstring\u0026gt;|default=[eth0en0]]# Duration to sleep for before exiting, to ensure metrics are scraped.# CLI flag: -ingester.final-sleep[final_sleep:\u0026lt;duration\u0026gt;|default=30s]# File path where tokens are stored. If empty, tokens are not stored at# shutdown and restored at startup.# CLI flag: -ingester.tokens-file-path[tokens_file_path:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Number of times to try and transfer chunks before falling back to flushing.# Negative value or zero disables hand-over.# CLI flag: -ingester.max-transfer-retries[max_transfer_retries:\u0026lt;int\u0026gt;|default=10]# Period with which to attempt to flush chunks.# CLI flag: -ingester.flush-period[flushcheckperiod:\u0026lt;duration\u0026gt;|default=1m0s]# Period chunks will remain in memory after flushing.# CLI flag: -ingester.retain-period[retainperiod:\u0026lt;duration\u0026gt;|default=5m0s]# Maximum chunk idle time before flushing.# CLI flag: -ingester.max-chunk-idle[maxchunkidle:\u0026lt;duration\u0026gt;|default=5m0s]# Maximum chunk idle time for chunks terminating in stale markers before# flushing. 0 disables it and a stale series is not flushed until the# max-chunk-idle timeout is reached.# CLI flag: -ingester.max-stale-chunk-idle[maxstalechunkidle:\u0026lt;duration\u0026gt;|default=0s]# Timeout for individual flush operations.# CLI flag: -ingester.flush-op-timeout[flushoptimeout:\u0026lt;duration\u0026gt;|default=1m0s]# Maximum chunk age before flushing.# CLI flag: -ingester.max-chunk-age[maxchunkage:\u0026lt;duration\u0026gt;|default=12h0m0s]# Range of time to subtract from MaxChunkAge to spread out flushes# CLI flag: -ingester.chunk-age-jitter[chunkagejitter:\u0026lt;duration\u0026gt;|default=20m0s]# Number of concurrent goroutines flushing to dynamodb.# CLI flag: -ingester.concurrent-flushes[concurrentflushes:\u0026lt;int\u0026gt;|default=50]# If true, spread series flushes across the whole period of MaxChunkAge# CLI flag: -ingester.spread-flushes[spreadflushes:\u0026lt;boolean\u0026gt;|default=false]# Period with which to update the per-user ingestion rates.# CLI flag: -ingester.rate-update-period[rateupdateperiod:\u0026lt;duration\u0026gt;|default=15s] querier_config The querier_config configures the Cortex querier.\n# The maximum number of concurrent queries.# CLI flag: -querier.max-concurrent[maxconcurrent:\u0026lt;int\u0026gt;|default=20]# The timeout for a query.# CLI flag: -querier.timeout[timeout:\u0026lt;duration\u0026gt;|default=2m0s]# Use iterators to execute query, as opposed to fully materialising the series# in memory.# CLI flag: -querier.iterators[iterators:\u0026lt;boolean\u0026gt;|default=false]# Use batch iterators to execute query, as opposed to fully materialising the# series in memory. Takes precedent over the -querier.iterators flag.# CLI flag: -querier.batch-iterators[batchiterators:\u0026lt;boolean\u0026gt;|default=false]# Use streaming RPCs to query ingester.# CLI flag: -querier.ingester-streaming[ingesterstreaming:\u0026lt;boolean\u0026gt;|default=false]# Maximum number of samples a single query can load into memory.# CLI flag: -querier.max-samples[maxsamples:\u0026lt;int\u0026gt;|default=50000000]# Maximum lookback beyond which queries are not sent to ingester. 0 means all# queries are sent to ingester.# CLI flag: -querier.query-ingesters-within[ingestermaxquerylookback:\u0026lt;duration\u0026gt;|default=0s]# The default evaluation interval or step size for subqueries.# CLI flag: -querier.default-evaluation-interval[defaultevaluationinterval:\u0026lt;duration\u0026gt;|default=1m0s] query_frontend_config The query_frontend_config configures the Cortex query-frontend.\n# Maximum number of outstanding requests per tenant per frontend; requests# beyond this error with HTTP 429.# CLI flag: -querier.max-outstanding-requests-per-tenant[max_outstanding_per_tenant:\u0026lt;int\u0026gt;|default=100]# Compress HTTP responses.# CLI flag: -querier.compress-http-responses[compress_responses:\u0026lt;boolean\u0026gt;|default=false]# URL of downstream Prometheus.# CLI flag: -frontend.downstream-url[downstream:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Log queries that are slower than the specified duration. 0 to disable.# CLI flag: -frontend.log-queries-longer-than[log_queries_longer_than:\u0026lt;duration\u0026gt;|default=0s] queryrange_config The queryrange_config configures the query splitting and caching in the Cortex query-frontend.\n# Split queries by an interval and execute in parallel, 0 disables it. You# should use an a multiple of 24 hours (same as the storage bucketing scheme),# to avoid queriers downloading and processing the same chunks.# CLI flag: -querier.split-queries-by-interval[split_queries_by_interval:\u0026lt;duration\u0026gt;|default=0s]# Deprecated: Split queries by day and execute in parallel.# CLI flag: -querier.split-queries-by-day[split_queries_by_day:\u0026lt;boolean\u0026gt;|default=false]# Mutate incoming queries to align their start and end with their step.# CLI flag: -querier.align-querier-with-step[align_queries_with_step:\u0026lt;boolean\u0026gt;|default=false]results_cache:cache:# Enable in-memory cache.# CLI flag: -frontend.cache.enable-fifocache[enable_fifocache:\u0026lt;boolean\u0026gt;|default=false]# The default validity of entries for caches unless overridden.# CLI flag: -frontend.default-validity[defaul_validity:\u0026lt;duration\u0026gt;|default=0s]background:# How many goroutines to use to write back to memcache.# CLI flag: -frontend.memcache.write-back-goroutines[writeback_goroutines:\u0026lt;int\u0026gt;|default=10]# How many chunks to buffer for background write back.# CLI flag: -frontend.memcache.write-back-buffer[writeback_buffer:\u0026lt;int\u0026gt;|default=10000]# The memcached_config block configures how data is stored in Memcached (ie.# expiration).# The CLI flags prefix for this block config is: frontend[memcached:\u0026lt;memcached_config\u0026gt;]# The memcached_client_config configures the client used to connect to# Memcached.# The CLI flags prefix for this block config is: frontend[memcached_client:\u0026lt;memcached_client_config\u0026gt;]# The redis_config configures the Redis backend cache.# The CLI flags prefix for this block config is: frontend[redis:\u0026lt;redis_config\u0026gt;]# The fifo_cache_config configures the local in-memory cache.# The CLI flags prefix for this block config is: frontend[fifocache:\u0026lt;fifo_cache_config\u0026gt;]# Most recent allowed cacheable result, to prevent caching very recent results# that might still be in flux.# CLI flag: -frontend.max-cache-freshness[max_freshness:\u0026lt;duration\u0026gt;|default=1m0s]# The maximum interval expected for each request, results will be cached per# single interval.# CLI flag: -frontend.cache-split-interval[cache_split_interval:\u0026lt;duration\u0026gt;|default=24h0m0s]# Cache query results.# CLI flag: -querier.cache-results[cache_results:\u0026lt;boolean\u0026gt;|default=false]# Maximum number of retries for a single request; beyond this, the downstream# error is returned.# CLI flag: -querier.max-retries-per-request[max_retries:\u0026lt;int\u0026gt;|default=5] ruler_config The ruler_config configures the Cortex ruler.\nexternalurl:# URL of alerts return path.# CLI flag: -ruler.external.url[url:\u0026lt;url\u0026gt;|default=]# How frequently to evaluate rules# CLI flag: -ruler.evaluation-interval[evaluationinterval:\u0026lt;duration\u0026gt;|default=1m0s]# How frequently to poll for rule changes# CLI flag: -ruler.poll-interval[pollinterval:\u0026lt;duration\u0026gt;|default=1m0s]storeconfig:# Method to use for backend rule storage (configdb)# CLI flag: -ruler.storage.type[type:\u0026lt;string\u0026gt;|default=\u0026#34;configdb\u0026#34;]# The configstore_config configures the config database storing rules and# alerts, and is used by the Cortex alertmanager.# The CLI flags prefix for this block config is: ruler[configdb:\u0026lt;configstore_config\u0026gt;]# file path to store temporary rule files for the prometheus rule managers# CLI flag: -ruler.rule-path[rulepath:\u0026lt;string\u0026gt;|default=\u0026#34;/rules\u0026#34;]alertmanagerurl:# URL of the Alertmanager to send notifications to.# CLI flag: -ruler.alertmanager-url[url:\u0026lt;url\u0026gt;|default=]# Use DNS SRV records to discover alertmanager hosts.# CLI flag: -ruler.alertmanager-discovery[alertmanagerdiscovery:\u0026lt;boolean\u0026gt;|default=false]# How long to wait between refreshing alertmanager hosts.# CLI flag: -ruler.alertmanager-refresh-interval[alertmanagerrefreshinterval:\u0026lt;duration\u0026gt;|default=1m0s]# If enabled requests to alertmanager will utilize the V2 API.# CLI flag: -ruler.alertmanager-use-v2[alertmanangerenablev2api:\u0026lt;boolean\u0026gt;|default=false]# Capacity of the queue for notifications to be sent to the Alertmanager.# CLI flag: -ruler.notification-queue-capacity[notificationqueuecapacity:\u0026lt;int\u0026gt;|default=10000]# HTTP timeout duration when sending notifications to the Alertmanager.# CLI flag: -ruler.notification-timeout[notificationtimeout:\u0026lt;duration\u0026gt;|default=10s]# Distribute rule evaluation using ring backend# CLI flag: -ruler.enable-sharding[enablesharding:\u0026lt;boolean\u0026gt;|default=false]# Time to spend searching for a pending ruler when shutting down.# CLI flag: -ruler.search-pending-for[searchpendingfor:\u0026lt;duration\u0026gt;|default=5m0s]lifecyclerconfig:ring:kvstore:# Backend storage to use for the ring. Supported values are: consul, etcd,# inmemory, multi, memberlist (experimental).# CLI flag: -ruler.store[store:\u0026lt;string\u0026gt;|default=\u0026#34;consul\u0026#34;]# The prefix for the keys in the store. Should end with a /.# CLI flag: -ruler.prefix[prefix:\u0026lt;string\u0026gt;|default=\u0026#34;collectors/\u0026#34;]# The consul_config configures the consul client.# The CLI flags prefix for this block config is: ruler[consul:\u0026lt;consul_config\u0026gt;]# The etcd_config configures the etcd client.# The CLI flags prefix for this block config is: ruler[etcd:\u0026lt;etcd_config\u0026gt;]# The memberlist_config configures the Gossip memberlist.# The CLI flags prefix for this block config is: ruler[memberlist:\u0026lt;memberlist_config\u0026gt;]multi:# Primary backend storage used by multi-client.# CLI flag: -ruler.multi.primary[primary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Secondary backend storage used by multi-client.# CLI flag: -ruler.multi.secondary[secondary:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Mirror writes to secondary store.# CLI flag: -ruler.multi.mirror-enabled[mirror_enabled:\u0026lt;boolean\u0026gt;|default=false]# Timeout for storing value to secondary store.# CLI flag: -ruler.multi.mirror-timeout[mirror_timeout:\u0026lt;duration\u0026gt;|default=2s]# The heartbeat timeout after which ingesters are skipped for reads/writes.# CLI flag: -ruler.ring.heartbeat-timeout[heartbeat_timeout:\u0026lt;duration\u0026gt;|default=1m0s]# The number of ingesters to write to and read from.# CLI flag: -ruler.distributor.replication-factor[replication_factor:\u0026lt;int\u0026gt;|default=3]# Number of tokens for each ingester.# CLI flag: -ruler.num-tokens[num_tokens:\u0026lt;int\u0026gt;|default=128]# Period at which to heartbeat to consul.# CLI flag: -ruler.heartbeat-period[heartbeat_period:\u0026lt;duration\u0026gt;|default=5s]# Observe tokens after generating to resolve collisions. Useful when using# gossiping ring.# CLI flag: -ruler.observe-period[observe_period:\u0026lt;duration\u0026gt;|default=0s]# Period to wait for a claim from another member; will join automatically# after this.# CLI flag: -ruler.join-after[join_after:\u0026lt;duration\u0026gt;|default=0s]# Minimum duration to wait before becoming ready. This is to work around race# conditions with ingesters exiting and updating the ring.# CLI flag: -ruler.min-ready-duration[min_ready_duration:\u0026lt;duration\u0026gt;|default=1m0s]# Name of network interface to read address from.# CLI flag: -ruler.lifecycler.interface[interface_names:\u0026lt;listofstring\u0026gt;|default=[eth0en0]]# Duration to sleep for before exiting, to ensure metrics are scraped.# CLI flag: -ruler.final-sleep[final_sleep:\u0026lt;duration\u0026gt;|default=30s]# File path where tokens are stored. If empty, tokens are not stored at# shutdown and restored at startup.# CLI flag: -ruler.tokens-file-path[tokens_file_path:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Period with which to attempt to flush rule groups.# CLI flag: -ruler.flush-period[flushcheckperiod:\u0026lt;duration\u0026gt;|default=1m0s] alertmanager_config The alertmanager_config configures the Cortex alertmanager.\n# Base path for data storage.# CLI flag: -alertmanager.storage.path[datadir:\u0026lt;string\u0026gt;|default=\u0026#34;data/\u0026#34;]# How long to keep data for.# CLI flag: -alertmanager.storage.retention[retention:\u0026lt;duration\u0026gt;|default=120h0m0s]externalurl:# The URL under which Alertmanager is externally reachable (for example, if# Alertmanager is served via a reverse proxy). Used for generating relative# and absolute links back to Alertmanager itself. If the URL has a path# portion, it will be used to prefix all HTTP endpoints served by# Alertmanager. If omitted, relevant URL components will be derived# automatically.# CLI flag: -alertmanager.web.external-url[url:\u0026lt;url\u0026gt;|default=]# How frequently to poll Cortex configs# CLI flag: -alertmanager.configs.poll-interval[pollinterval:\u0026lt;duration\u0026gt;|default=15s]# Listen address for cluster.# CLI flag: -cluster.listen-address[clusterbindaddr:\u0026lt;string\u0026gt;|default=\u0026#34;0.0.0.0:9094\u0026#34;]# Explicit address to advertise in cluster.# CLI flag: -cluster.advertise-address[clusteradvertiseaddr:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Initial peers (may be repeated).# CLI flag: -cluster.peer[peers:\u0026lt;listofstring\u0026gt;|default=]# Time to wait between peers to send notifications.# CLI flag: -cluster.peer-timeout[peertimeout:\u0026lt;duration\u0026gt;|default=15s]# Filename of fallback config to use if none specified for instance.# CLI flag: -alertmanager.configs.fallback[fallbackconfigfile:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Root of URL to generate if config is http://internal.monitor# CLI flag: -alertmanager.configs.auto-webhook-root[autowebhookroot:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;] table_manager_config The table_manager_config configures the Cortex table-manager.\n# If true, disable all changes to DB capacity# CLI flag: -table-manager.throughput-updates-disabled[throughput_updates_disabled:\u0026lt;boolean\u0026gt;|default=false]# If true, enables retention deletes of DB tables# CLI flag: -table-manager.retention-deletes-enabled[retention_deletes_enabled:\u0026lt;boolean\u0026gt;|default=false]# Tables older than this retention period are deleted. Note: This setting is# destructive to data!(default: 0, which disables deletion)# CLI flag: -table-manager.retention-period[retention_period:\u0026lt;duration\u0026gt;|default=0s]# How frequently to poll DynamoDB to learn our capacity.# CLI flag: -dynamodb.poll-interval[dynamodb_poll_interval:\u0026lt;duration\u0026gt;|default=2m0s]# DynamoDB periodic tables grace period (duration which table will be# created/deleted before/after it\u0026#39;s needed).# CLI flag: -dynamodb.periodic-table.grace-period[creation_grace_period:\u0026lt;duration\u0026gt;|default=10m0s]index_tables_provisioning:# Enables on demand throughput provisioning for the storage provider (if# supported). Applies only to tables which are not autoscaled# CLI flag: -dynamodb.periodic-table.enable-ondemand-throughput-mode[provisioned_throughput_on_demand_mode:\u0026lt;boolean\u0026gt;|default=false]# DynamoDB table default write throughput.# CLI flag: -dynamodb.periodic-table.write-throughput[provisioned_write_throughput:\u0026lt;int\u0026gt;|default=1000]# DynamoDB table default read throughput.# CLI flag: -dynamodb.periodic-table.read-throughput[provisioned_read_throughput:\u0026lt;int\u0026gt;|default=300]# Enables on demand throughput provisioning for the storage provider (if# supported). Applies only to tables which are not autoscaled# CLI flag: -dynamodb.periodic-table.inactive-enable-ondemand-throughput-mode[inactive_throughput_on_demand_mode:\u0026lt;boolean\u0026gt;|default=false]# DynamoDB table write throughput for inactive tables.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput[inactive_write_throughput:\u0026lt;int\u0026gt;|default=1]# DynamoDB table read throughput for inactive tables.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput[inactive_read_throughput:\u0026lt;int\u0026gt;|default=300]write_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.periodic-table.write-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.periodic-table.write-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]inactive_write_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]# Number of last inactive tables to enable write autoscale.# CLI flag: -dynamodb.periodic-table.inactive-write-throughput.scale-last-n[inactive_write_scale_lastn:\u0026lt;int\u0026gt;|default=4]read_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.periodic-table.read-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.periodic-table.read-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]inactive_read_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]# Number of last inactive tables to enable read autoscale.# CLI flag: -dynamodb.periodic-table.inactive-read-throughput.scale-last-n[inactive_read_scale_lastn:\u0026lt;int\u0026gt;|default=4]chunk_tables_provisioning:# Enables on demand throughput provisioning for the storage provider (if# supported). Applies only to tables which are not autoscaled# CLI flag: -dynamodb.chunk-table.enable-ondemand-throughput-mode[provisioned_throughput_on_demand_mode:\u0026lt;boolean\u0026gt;|default=false]# DynamoDB table default write throughput.# CLI flag: -dynamodb.chunk-table.write-throughput[provisioned_write_throughput:\u0026lt;int\u0026gt;|default=1000]# DynamoDB table default read throughput.# CLI flag: -dynamodb.chunk-table.read-throughput[provisioned_read_throughput:\u0026lt;int\u0026gt;|default=300]# Enables on demand throughput provisioning for the storage provider (if# supported). Applies only to tables which are not autoscaled# CLI flag: -dynamodb.chunk-table.inactive-enable-ondemand-throughput-mode[inactive_throughput_on_demand_mode:\u0026lt;boolean\u0026gt;|default=false]# DynamoDB table write throughput for inactive tables.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput[inactive_write_throughput:\u0026lt;int\u0026gt;|default=1]# DynamoDB table read throughput for inactive tables.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput[inactive_read_throughput:\u0026lt;int\u0026gt;|default=300]write_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.chunk-table.write-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.chunk-table.write-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]inactive_write_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]# Number of last inactive tables to enable write autoscale.# CLI flag: -dynamodb.chunk-table.inactive-write-throughput.scale-last-n[inactive_write_scale_lastn:\u0026lt;int\u0026gt;|default=4]read_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.chunk-table.read-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.chunk-table.read-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]inactive_read_scale:# Should we enable autoscale for the table.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.enabled[enabled:\u0026lt;boolean\u0026gt;|default=false]# AWS AutoScaling role ARN# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.role-arn[role_arn:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# DynamoDB minimum provision capacity.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.min-capacity[min_capacity:\u0026lt;int\u0026gt;|default=3000]# DynamoDB maximum provision capacity.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.max-capacity[max_capacity:\u0026lt;int\u0026gt;|default=6000]# DynamoDB minimum seconds between each autoscale up.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.out-cooldown[out_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB minimum seconds between each autoscale down.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.in-cooldown[in_cooldown:\u0026lt;int\u0026gt;|default=1800]# DynamoDB target ratio of consumed capacity to provisioned capacity.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale.target-value[target:\u0026lt;float\u0026gt;|default=80]# Number of last inactive tables to enable read autoscale.# CLI flag: -dynamodb.chunk-table.inactive-read-throughput.scale-last-n[inactive_read_scale_lastn:\u0026lt;int\u0026gt;|default=4] storage_config The storage_config configures where Cortex stores the data (chunks storage engine).\n# The storage engine to use: chunks or tsdb. Be aware tsdb is experimental and# shouldn\u0026#39;t be used in production.# CLI flag: -store.engine[engine:\u0026lt;string\u0026gt;|default=\u0026#34;chunks\u0026#34;]aws:dynamodbconfig:dynamodb:# DynamoDB endpoint URL with escaped Key and Secret encoded. If only# region is specified as a host, proper endpoint will be deduced. Use# inmemory:///\u0026lt;table-name\u0026gt; to use a mock in-memory implementation.# CLI flag: -dynamodb.url[url:\u0026lt;url\u0026gt;|default=]# DynamoDB table management requests per second limit.# CLI flag: -dynamodb.api-limit[apilimit:\u0026lt;float\u0026gt;|default=2]# DynamoDB rate cap to back off when throttled.# CLI flag: -dynamodb.throttle-limit[throttlelimit:\u0026lt;float\u0026gt;|default=10]applicationautoscaling:# ApplicationAutoscaling endpoint URL with escaped Key and Secret encoded.# CLI flag: -applicationautoscaling.url[url:\u0026lt;url\u0026gt;|default=]metrics:# Use metrics-based autoscaling, via this query URL# CLI flag: -metrics.url[url:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Queue length above which we will scale up capacity# CLI flag: -metrics.target-queue-length[targetqueuelen:\u0026lt;int\u0026gt;|default=100000]# Scale up capacity by this multiple# CLI flag: -metrics.scale-up-factor[scaleupfactor:\u0026lt;float\u0026gt;|default=1.3]# Ignore throttling below this level (rate per second)# CLI flag: -metrics.ignore-throttle-below[minthrottling:\u0026lt;float\u0026gt;|default=1]# query to fetch ingester queue length# CLI flag: -metrics.queue-length-query[queuelengthquery:\u0026lt;string\u0026gt;|default=\u0026#34;sum(avg_over_time(cortex_ingester_flush_queue_length{job=\\\u0026#34;cortex/ingester\\\u0026#34;}[2m]))\u0026#34;]# query to fetch throttle rates per table# CLI flag: -metrics.write-throttle-query[throttlequery:\u0026lt;string\u0026gt;|default=\u0026#34;sum(rate(cortex_dynamo_throttled_total{operation=\\\u0026#34;DynamoDB.BatchWriteItem\\\u0026#34;}[1m])) by (table) \u0026gt; 0\u0026#34;]# query to fetch write capacity usage per table# CLI flag: -metrics.usage-query[usagequery:\u0026lt;string\u0026gt;|default=\u0026#34;sum(rate(cortex_dynamo_consumed_capacity_total{operation=\\\u0026#34;DynamoDB.BatchWriteItem\\\u0026#34;}[15m])) by (table) \u0026gt; 0\u0026#34;]# query to fetch read capacity usage per table# CLI flag: -metrics.read-usage-query[readusagequery:\u0026lt;string\u0026gt;|default=\u0026#34;sum(rate(cortex_dynamo_consumed_capacity_total{operation=\\\u0026#34;DynamoDB.QueryPages\\\u0026#34;}[1h])) by (table) \u0026gt; 0\u0026#34;]# query to fetch read errors per table# CLI flag: -metrics.read-error-query[readerrorquery:\u0026lt;string\u0026gt;|default=\u0026#34;sum(increase(cortex_dynamo_failures_total{operation=\\\u0026#34;DynamoDB.QueryPages\\\u0026#34;,error=\\\u0026#34;ProvisionedThroughputExceededException\\\u0026#34;}[1m])) by (table) \u0026gt; 0\u0026#34;]# Number of chunks to group together to parallelise fetches (zero to# disable)# CLI flag: -dynamodb.chunk.gang.size[chunkgangsize:\u0026lt;int\u0026gt;|default=10]# Max number of chunk-get operations to start in parallel# CLI flag: -dynamodb.chunk.get.max.parallelism[chunkgetmaxparallelism:\u0026lt;int\u0026gt;|default=32]s3:# S3 endpoint URL with escaped Key and Secret encoded. If only region is# specified as a host, proper endpoint will be deduced. Use# inmemory:///\u0026lt;bucket-name\u0026gt; to use a mock in-memory implementation.# CLI flag: -s3.url[url:\u0026lt;url\u0026gt;|default=]# Comma separated list of bucket names to evenly distribute chunks over.# Overrides any buckets specified in s3.url flag# CLI flag: -s3.buckets[bucketnames:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Set this to `true` to force the request to use path-style addressing.# CLI flag: -s3.force-path-style[s3forcepathstyle:\u0026lt;boolean\u0026gt;|default=false]azure:# Name of the blob container used to store chunks. Defaults to `cortex`. This# container must be created before running cortex.# CLI flag: -azure.container-name[container_name:\u0026lt;string\u0026gt;|default=\u0026#34;cortex\u0026#34;]# The Microsoft Azure account name to be used# CLI flag: -azure.account-name[account_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# The Microsoft Azure account key to use.# CLI flag: -azure.account-key[account_key:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Preallocated buffer size for downloads (default is 512KB)# CLI flag: -azure.download-buffer-size[download_buffer_size:\u0026lt;int\u0026gt;|default=512000]# Preallocated buffer size for up;oads (default is 256KB)# CLI flag: -azure.upload-buffer-size[upload_buffer_size:\u0026lt;int\u0026gt;|default=256000]# Number of buffers used to used to upload a chunk. (defaults to 1)# CLI flag: -azure.download-buffer-count[upload_buffer_count:\u0026lt;int\u0026gt;|default=1]# Timeout for requests made against azure blob storage. Defaults to 30# seconds.# CLI flag: -azure.request-timeout[request_timeout:\u0026lt;duration\u0026gt;|default=30s]# Number of retries for a request which times out.# CLI flag: -azure.max-retries[max_retries:\u0026lt;int\u0026gt;|default=5]# Minimum time to wait before retrying a request.# CLI flag: -azure.min-retry-delay[min_retry_delay:\u0026lt;duration\u0026gt;|default=10ms]# Maximum time to wait before retrying a request.# CLI flag: -azure.max-retry-delay[max_retry_delay:\u0026lt;duration\u0026gt;|default=500ms]bigtable:# Bigtable project ID.# CLI flag: -bigtable.project[project:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Bigtable instance ID.# CLI flag: -bigtable.instance[instance:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]grpc_client_config:# gRPC client max receive message size (bytes).# CLI flag: -bigtable.grpc-max-recv-msg-size[max_recv_msg_size:\u0026lt;int\u0026gt;|default=104857600]# gRPC client max send message size (bytes).# CLI flag: -bigtable.grpc-max-send-msg-size[max_send_msg_size:\u0026lt;int\u0026gt;|default=16777216]# Use compression when sending messages.# CLI flag: -bigtable.grpc-use-gzip-compression[use_gzip_compression:\u0026lt;boolean\u0026gt;|default=false]# Rate limit for gRPC client; 0 means disabled.# CLI flag: -bigtable.grpc-client-rate-limit[rate_limit:\u0026lt;float\u0026gt;|default=0]# Rate limit burst for gRPC client.# CLI flag: -bigtable.grpc-client-rate-limit-burst[rate_limit_burst:\u0026lt;int\u0026gt;|default=0]# Enable backoff and retry when we hit ratelimits.# CLI flag: -bigtable.backoff-on-ratelimits[backoff_on_ratelimits:\u0026lt;boolean\u0026gt;|default=false]backoff_config:# Minimum delay when backing off.# CLI flag: -bigtable.backoff-min-period[minbackoff:\u0026lt;duration\u0026gt;|default=100ms]# Maximum delay when backing off.# CLI flag: -bigtable.backoff-max-period[maxbackoff:\u0026lt;duration\u0026gt;|default=10s]# Number of times to backoff and retry before failing.# CLI flag: -bigtable.backoff-retries[maxretries:\u0026lt;int\u0026gt;|default=10]# If enabled, once a tables info is fetched, it is cached.# CLI flag: -bigtable.table-cache.enabled[tablecacheenabled:\u0026lt;boolean\u0026gt;|default=true]# Duration to cache tables before checking again.# CLI flag: -bigtable.table-cache.expiration[tablecacheexpiration:\u0026lt;duration\u0026gt;|default=30m0s]gcs:# Name of GCS bucket to put chunks in.# CLI flag: -gcs.bucketname[bucket_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# The size of the buffer that GCS client for each PUT request. 0 to disable# buffering.# CLI flag: -gcs.chunk-buffer-size[chunk_buffer_size:\u0026lt;int\u0026gt;|default=0]# The duration after which the requests to GCS should be timed out.# CLI flag: -gcs.request-timeout[request_timeout:\u0026lt;duration\u0026gt;|default=0s]cassandra:# Comma-separated hostnames or IPs of Cassandra instances.# CLI flag: -cassandra.addresses[addresses:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Port that Cassandra is running on# CLI flag: -cassandra.port[port:\u0026lt;int\u0026gt;|default=9042]# Keyspace to use in Cassandra.# CLI flag: -cassandra.keyspace[keyspace:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Consistency level for Cassandra.# CLI flag: -cassandra.consistency[consistency:\u0026lt;string\u0026gt;|default=\u0026#34;QUORUM\u0026#34;]# Replication factor to use in Cassandra.# CLI flag: -cassandra.replication-factor[replication_factor:\u0026lt;int\u0026gt;|default=1]# Instruct the cassandra driver to not attempt to get host info from the# system.peers table.# CLI flag: -cassandra.disable-initial-host-lookup[disable_initial_host_lookup:\u0026lt;boolean\u0026gt;|default=false]# Use SSL when connecting to cassandra instances.# CLI flag: -cassandra.ssl[SSL:\u0026lt;boolean\u0026gt;|default=false]# Require SSL certificate validation.# CLI flag: -cassandra.host-verification[host_verification:\u0026lt;boolean\u0026gt;|default=true]# Path to certificate file to verify the peer.# CLI flag: -cassandra.ca-path[CA_path:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Enable password authentication when connecting to cassandra.# CLI flag: -cassandra.auth[auth:\u0026lt;boolean\u0026gt;|default=false]# Username to use when connecting to cassandra.# CLI flag: -cassandra.username[username:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Password to use when connecting to cassandra.# CLI flag: -cassandra.password[password:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Timeout when connecting to cassandra.# CLI flag: -cassandra.timeout[timeout:\u0026lt;duration\u0026gt;|default=600ms]# Initial connection timeout, used during initial dial to server.# CLI flag: -cassandra.connect-timeout[connect_timeout:\u0026lt;duration\u0026gt;|default=600ms]boltdb:# Location of BoltDB index files.# CLI flag: -boltdb.dir[directory:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]filesystem:# Directory to store chunks in.# CLI flag: -local.chunk-directory[directory:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Cache validity for active index entries. Should be no higher than# -ingester.max-chunk-idle.# CLI flag: -store.index-cache-validity[indexcachevalidity:\u0026lt;duration\u0026gt;|default=5m0s]index_queries_cache_config:# Cache config for index entry reading. Enable in-memory cache.# CLI flag: -store.index-cache-read.cache.enable-fifocache[enable_fifocache:\u0026lt;boolean\u0026gt;|default=false]# Cache config for index entry reading. The default validity of entries for# caches unless overridden.# CLI flag: -store.index-cache-read.default-validity[defaul_validity:\u0026lt;duration\u0026gt;|default=0s]background:# Cache config for index entry reading. How many goroutines to use to write# back to memcache.# CLI flag: -store.index-cache-read.memcache.write-back-goroutines[writeback_goroutines:\u0026lt;int\u0026gt;|default=10]# Cache config for index entry reading. How many chunks to buffer for# background write back.# CLI flag: -store.index-cache-read.memcache.write-back-buffer[writeback_buffer:\u0026lt;int\u0026gt;|default=10000]# The memcached_config block configures how data is stored in Memcached (ie.# expiration).# The CLI flags prefix for this block config is: store.index-cache-read[memcached:\u0026lt;memcached_config\u0026gt;]# The memcached_client_config configures the client used to connect to# Memcached.# The CLI flags prefix for this block config is: store.index-cache-read[memcached_client:\u0026lt;memcached_client_config\u0026gt;]# The redis_config configures the Redis backend cache.# The CLI flags prefix for this block config is: store.index-cache-read[redis:\u0026lt;redis_config\u0026gt;]# The fifo_cache_config configures the local in-memory cache.# The CLI flags prefix for this block config is: store.index-cache-read[fifocache:\u0026lt;fifo_cache_config\u0026gt;] chunk_store_config The chunk_store_config configures how Cortex stores the data (chunks storage engine).\nchunk_cache_config:# Cache config for chunks. Enable in-memory cache.# CLI flag: -cache.enable-fifocache[enable_fifocache:\u0026lt;boolean\u0026gt;|default=false]# Cache config for chunks. The default validity of entries for caches unless# overridden.# CLI flag: -default-validity[defaul_validity:\u0026lt;duration\u0026gt;|default=0s]background:# Cache config for chunks. How many goroutines to use to write back to# memcache.# CLI flag: -memcache.write-back-goroutines[writeback_goroutines:\u0026lt;int\u0026gt;|default=10]# Cache config for chunks. How many chunks to buffer for background write# back.# CLI flag: -memcache.write-back-buffer[writeback_buffer:\u0026lt;int\u0026gt;|default=10000]# The memcached_config block configures how data is stored in Memcached (ie.# expiration).[memcached:\u0026lt;memcached_config\u0026gt;]# The memcached_client_config configures the client used to connect to# Memcached.[memcached_client:\u0026lt;memcached_client_config\u0026gt;]# The redis_config configures the Redis backend cache.[redis:\u0026lt;redis_config\u0026gt;]# The fifo_cache_config configures the local in-memory cache.[fifocache:\u0026lt;fifo_cache_config\u0026gt;]write_dedupe_cache_config:# Cache config for index entry writing. Enable in-memory cache.# CLI flag: -store.index-cache-write.cache.enable-fifocache[enable_fifocache:\u0026lt;boolean\u0026gt;|default=false]# Cache config for index entry writing. The default validity of entries for# caches unless overridden.# CLI flag: -store.index-cache-write.default-validity[defaul_validity:\u0026lt;duration\u0026gt;|default=0s]background:# Cache config for index entry writing. How many goroutines to use to write# back to memcache.# CLI flag: -store.index-cache-write.memcache.write-back-goroutines[writeback_goroutines:\u0026lt;int\u0026gt;|default=10]# Cache config for index entry writing. How many chunks to buffer for# background write back.# CLI flag: -store.index-cache-write.memcache.write-back-buffer[writeback_buffer:\u0026lt;int\u0026gt;|default=10000]# The memcached_config block configures how data is stored in Memcached (ie.# expiration).# The CLI flags prefix for this block config is: store.index-cache-write[memcached:\u0026lt;memcached_config\u0026gt;]# The memcached_client_config configures the client used to connect to# Memcached.# The CLI flags prefix for this block config is: store.index-cache-write[memcached_client:\u0026lt;memcached_client_config\u0026gt;]# The redis_config configures the Redis backend cache.# The CLI flags prefix for this block config is: store.index-cache-write[redis:\u0026lt;redis_config\u0026gt;]# The fifo_cache_config configures the local in-memory cache.# The CLI flags prefix for this block config is: store.index-cache-write[fifocache:\u0026lt;fifo_cache_config\u0026gt;]# Minimum time between chunk update and being saved to the store.# CLI flag: -store.min-chunk-age[min_chunk_age:\u0026lt;duration\u0026gt;|default=0s]# Cache index entries older than this period. 0 to disable.# CLI flag: -store.cache-lookups-older-than[cache_lookups_older_than:\u0026lt;duration\u0026gt;|default=0s]# Limit how long back data can be queried# CLI flag: -store.max-look-back-period[max_look_back_period:\u0026lt;duration\u0026gt;|default=0s] ingester_client_config The ingester_client_config configures how the Cortex distributors connect to the ingesters.\ngrpc_client_config:# gRPC client max receive message size (bytes).# CLI flag: -ingester.client.grpc-max-recv-msg-size[max_recv_msg_size:\u0026lt;int\u0026gt;|default=104857600]# gRPC client max send message size (bytes).# CLI flag: -ingester.client.grpc-max-send-msg-size[max_send_msg_size:\u0026lt;int\u0026gt;|default=16777216]# Use compression when sending messages.# CLI flag: -ingester.client.grpc-use-gzip-compression[use_gzip_compression:\u0026lt;boolean\u0026gt;|default=false]# Rate limit for gRPC client; 0 means disabled.# CLI flag: -ingester.client.grpc-client-rate-limit[rate_limit:\u0026lt;float\u0026gt;|default=0]# Rate limit burst for gRPC client.# CLI flag: -ingester.client.grpc-client-rate-limit-burst[rate_limit_burst:\u0026lt;int\u0026gt;|default=0]# Enable backoff and retry when we hit ratelimits.# CLI flag: -ingester.client.backoff-on-ratelimits[backoff_on_ratelimits:\u0026lt;boolean\u0026gt;|default=false]backoff_config:# Minimum delay when backing off.# CLI flag: -ingester.client.backoff-min-period[minbackoff:\u0026lt;duration\u0026gt;|default=100ms]# Maximum delay when backing off.# CLI flag: -ingester.client.backoff-max-period[maxbackoff:\u0026lt;duration\u0026gt;|default=10s]# Number of times to backoff and retry before failing.# CLI flag: -ingester.client.backoff-retries[maxretries:\u0026lt;int\u0026gt;|default=10] frontend_worker_config The frontend_worker_config configures the worker - running within the Cortex ingester - picking up and executing queries enqueued by the query-frontend.\n# Address of query frontend service.# CLI flag: -querier.frontend-address[address:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Number of simultaneous queries to process.# CLI flag: -querier.worker-parallelism[parallelism:\u0026lt;int\u0026gt;|default=10]# How often to query DNS.# CLI flag: -querier.dns-lookup-period[dnslookupduration:\u0026lt;duration\u0026gt;|default=10s]grpc_client_config:# gRPC client max receive message size (bytes).# CLI flag: -querier.frontend-client.grpc-max-recv-msg-size[max_recv_msg_size:\u0026lt;int\u0026gt;|default=104857600]# gRPC client max send message size (bytes).# CLI flag: -querier.frontend-client.grpc-max-send-msg-size[max_send_msg_size:\u0026lt;int\u0026gt;|default=16777216]# Use compression when sending messages.# CLI flag: -querier.frontend-client.grpc-use-gzip-compression[use_gzip_compression:\u0026lt;boolean\u0026gt;|default=false]# Rate limit for gRPC client; 0 means disabled.# CLI flag: -querier.frontend-client.grpc-client-rate-limit[rate_limit:\u0026lt;float\u0026gt;|default=0]# Rate limit burst for gRPC client.# CLI flag: -querier.frontend-client.grpc-client-rate-limit-burst[rate_limit_burst:\u0026lt;int\u0026gt;|default=0]# Enable backoff and retry when we hit ratelimits.# CLI flag: -querier.frontend-client.backoff-on-ratelimits[backoff_on_ratelimits:\u0026lt;boolean\u0026gt;|default=false]backoff_config:# Minimum delay when backing off.# CLI flag: -querier.frontend-client.backoff-min-period[minbackoff:\u0026lt;duration\u0026gt;|default=100ms]# Maximum delay when backing off.# CLI flag: -querier.frontend-client.backoff-max-period[maxbackoff:\u0026lt;duration\u0026gt;|default=10s]# Number of times to backoff and retry before failing.# CLI flag: -querier.frontend-client.backoff-retries[maxretries:\u0026lt;int\u0026gt;|default=10] etcd_config The etcd_config configures the etcd client.\n# The etcd endpoints to connect to.# CLI flag: -\u0026lt;prefix\u0026gt;.etcd.endpoints[endpoints:\u0026lt;listofstring\u0026gt;|default=[]]# The dial timeout for the etcd connection.# CLI flag: -\u0026lt;prefix\u0026gt;.etcd.dial-timeout[dial_timeout:\u0026lt;duration\u0026gt;|default=10s]# The maximum number of retries to do for failed ops.# CLI flag: -\u0026lt;prefix\u0026gt;.etcd.max-retries[max_retries:\u0026lt;int\u0026gt;|default=10] consul_config The consul_config configures the consul client.\n# Hostname and port of Consul.# CLI flag: -\u0026lt;prefix\u0026gt;.consul.hostname[host:\u0026lt;string\u0026gt;|default=\u0026#34;localhost:8500\u0026#34;]# ACL Token used to interact with Consul.# CLI flag: -\u0026lt;prefix\u0026gt;.consul.acltoken[acltoken:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# HTTP timeout when talking to Consul# CLI flag: -\u0026lt;prefix\u0026gt;.consul.client-timeout[httpclienttimeout:\u0026lt;duration\u0026gt;|default=20s]# Enable consistent reads to Consul.# CLI flag: -\u0026lt;prefix\u0026gt;.consul.consistent-reads[consistentreads:\u0026lt;boolean\u0026gt;|default=true]# Rate limit when watching key or prefix in Consul, in requests per second. 0# disables the rate limit.# CLI flag: -\u0026lt;prefix\u0026gt;.consul.watch-rate-limit[watchkeyratelimit:\u0026lt;float\u0026gt;|default=0]# Burst size used in rate limit. Values less than 1 are treated as 1.# CLI flag: -\u0026lt;prefix\u0026gt;.consul.watch-burst-size[watchkeyburstsize:\u0026lt;int\u0026gt;|default=1] memberlist_config The memberlist_config configures the Gossip memberlist.\n# Name of the node in memberlist cluster. Defaults to hostname.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.nodename[node_name:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# The timeout for establishing a connection with a remote node, and for# read/write operations. Uses memberlist LAN defaults if 0.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.stream-timeout[stream_timeout:\u0026lt;duration\u0026gt;|default=0s]# Multiplication factor used when sending out messages (factor * log(N+1)).# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.retransmit-factor[retransmit_factor:\u0026lt;int\u0026gt;|default=0]# How often to use pull/push sync. Uses memberlist LAN defaults if 0.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.pullpush-interval[pull_push_interval:\u0026lt;duration\u0026gt;|default=0s]# How often to gossip. Uses memberlist LAN defaults if 0.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.gossip-interval[gossip_interval:\u0026lt;duration\u0026gt;|default=0s]# How many nodes to gossip to. Uses memberlist LAN defaults if 0.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.gossip-nodes[gossip_nodes:\u0026lt;int\u0026gt;|default=0]# Other cluster members to join. Can be specified multiple times. Memberlist# store is EXPERIMENTAL.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.join[join_members:\u0026lt;listofstring\u0026gt;|default=]# If this node fails to join memberlist cluster, abort.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.abort-if-join-fails[abort_if_cluster_join_fails:\u0026lt;boolean\u0026gt;|default=true]# How long to keep LEFT ingesters in the ring.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.left-ingesters-timeout[left_ingesters_timeout:\u0026lt;duration\u0026gt;|default=5m0s]# Timeout for leaving memberlist cluster.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.leave-timeout[leave_timeout:\u0026lt;duration\u0026gt;|default=5s]# IP address to listen on for gossip messages. Multiple addresses may be# specified. Defaults to 0.0.0.0# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.bind-addr[bind_addr:\u0026lt;listofstring\u0026gt;|default=]# Port to listen on for gossip messages.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.bind-port[bind_port:\u0026lt;int\u0026gt;|default=7946]# Timeout used when connecting to other nodes to send packet.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.packet-dial-timeout[packet_dial_timeout:\u0026lt;duration\u0026gt;|default=5s]# Timeout for writing \u0026#39;packet\u0026#39; data.# CLI flag: -\u0026lt;prefix\u0026gt;.memberlist.packet-write-timeout[packet_write_timeout:\u0026lt;duration\u0026gt;|default=5s] limits_config The limits_config configures default and per-tenant limits imposed by Cortex services (ie. distributor, ingester, \u0026hellip;).\n# Per-user ingestion rate limit in samples per second.# CLI flag: -distributor.ingestion-rate-limit[ingestion_rate:\u0026lt;float\u0026gt;|default=25000]# Whether the ingestion rate limit should be applied individually to each# distributor instance (local), or evenly shared across the cluster (global).# CLI flag: -distributor.ingestion-rate-limit-strategy[ingestion_rate_strategy:\u0026lt;string\u0026gt;|default=\u0026#34;local\u0026#34;]# Per-user allowed ingestion burst size (in number of samples).# CLI flag: -distributor.ingestion-burst-size[ingestion_burst_size:\u0026lt;int\u0026gt;|default=50000]# Flag to enable, for all users, handling of samples with external labels# identifying replicas in an HA Prometheus setup.# CLI flag: -distributor.ha-tracker.enable-for-all-users[accept_ha_samples:\u0026lt;boolean\u0026gt;|default=false]# Prometheus label to look for in samples to identify a Prometheus HA cluster.# CLI flag: -distributor.ha-tracker.cluster[ha_cluster_label:\u0026lt;string\u0026gt;|default=\u0026#34;cluster\u0026#34;]# Prometheus label to look for in samples to identify a Prometheus HA replica.# CLI flag: -distributor.ha-tracker.replica[ha_replica_label:\u0026lt;string\u0026gt;|default=\u0026#34;__replica__\u0026#34;]# This flag can be used to specify label names that to drop during sample# ingestion within the distributor and can be repeated in order to drop multiple# labels.# CLI flag: -distributor.drop-label[drop_labels:\u0026lt;listofstring\u0026gt;|default=]# Maximum length accepted for label names# CLI flag: -validation.max-length-label-name[max_label_name_length:\u0026lt;int\u0026gt;|default=1024]# Maximum length accepted for label value. This setting also applies to the# metric name# CLI flag: -validation.max-length-label-value[max_label_value_length:\u0026lt;int\u0026gt;|default=2048]# Maximum number of label names per series.# CLI flag: -validation.max-label-names-per-series[max_label_names_per_series:\u0026lt;int\u0026gt;|default=30]# Reject old samples.# CLI flag: -validation.reject-old-samples[reject_old_samples:\u0026lt;boolean\u0026gt;|default=false]# Maximum accepted sample age before rejecting.# CLI flag: -validation.reject-old-samples.max-age[reject_old_samples_max_age:\u0026lt;duration\u0026gt;|default=336h0m0s]# Duration which table will be created/deleted before/after it\u0026#39;s needed; we# won\u0026#39;t accept sample from before this time.# CLI flag: -validation.create-grace-period[creation_grace_period:\u0026lt;duration\u0026gt;|default=10m0s]# Enforce every sample has a metric name.# CLI flag: -validation.enforce-metric-name[enforce_metric_name:\u0026lt;boolean\u0026gt;|default=true]# The maximum number of series that a query can return.# CLI flag: -ingester.max-series-per-query[max_series_per_query:\u0026lt;int\u0026gt;|default=100000]# The maximum number of samples that a query can return.# CLI flag: -ingester.max-samples-per-query[max_samples_per_query:\u0026lt;int\u0026gt;|default=1000000]# The maximum number of active series per user, per ingester. 0 to disable.# CLI flag: -ingester.max-series-per-user[max_series_per_user:\u0026lt;int\u0026gt;|default=5000000]# The maximum number of active series per metric name, per ingester. 0 to# disable.# CLI flag: -ingester.max-series-per-metric[max_series_per_metric:\u0026lt;int\u0026gt;|default=50000]# The maximum number of active series per user, across the cluster. 0 to# disable. Supported only if -distributor.shard-by-all-labels is true.# CLI flag: -ingester.max-global-series-per-user[max_global_series_per_user:\u0026lt;int\u0026gt;|default=0]# The maximum number of active series per metric name, across the cluster. 0 to# disable.# CLI flag: -ingester.max-global-series-per-metric[max_global_series_per_metric:\u0026lt;int\u0026gt;|default=0]# Minimum number of samples in an idle chunk to flush it to the store. Use with# care, if chunks are less than this size they will be discarded.# CLI flag: -ingester.min-chunk-length[min_chunk_length:\u0026lt;int\u0026gt;|default=0]# Maximum number of chunks that can be fetched in a single query.# CLI flag: -store.query-chunk-limit[max_chunks_per_query:\u0026lt;int\u0026gt;|default=2000000]# Limit to length of chunk store queries, 0 to disable.# CLI flag: -store.max-query-length[max_query_length:\u0026lt;duration\u0026gt;|default=0s]# Maximum number of queries will be scheduled in parallel by the frontend.# CLI flag: -querier.max-query-parallelism[max_query_parallelism:\u0026lt;int\u0026gt;|default=14]# Cardinality limit for index queries.# CLI flag: -store.cardinality-limit[cardinality_limit:\u0026lt;int\u0026gt;|default=100000]# File name of per-user overrides. [deprecated, use -runtime-config.file# instead]# CLI flag: -limits.per-user-override-config[per_tenant_override_config:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Period with which to reload the overrides. [deprecated, use# -runtime-config.reload-period instead]# CLI flag: -limits.per-user-override-period[per_tenant_override_period:\u0026lt;duration\u0026gt;|default=10s] redis_config The redis_config configures the Redis backend cache.\n# Redis service endpoint to use when caching chunks. If empty, no redis will be# used.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.endpoint[endpoint:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Maximum time to wait before giving up on redis requests.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.timeout[timeout:\u0026lt;duration\u0026gt;|default=100ms]# How long keys stay in the redis.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.expiration[expiration:\u0026lt;duration\u0026gt;|default=0s]# Maximum number of idle connections in pool.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.max-idle-conns[max_idle_conns:\u0026lt;int\u0026gt;|default=80]# Maximum number of active connections in pool.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.max-active-conns[max_active_conns:\u0026lt;int\u0026gt;|default=0]# Password to use when connecting to redis.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.password[password:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# Enables connecting to redis with TLS.# CLI flag: -\u0026lt;prefix\u0026gt;.redis.enable-tls[enable_tls:\u0026lt;boolean\u0026gt;|default=false] memcached_config The memcached_config block configures how data is stored in Memcached (ie. expiration).\n# How long keys stay in the memcache.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.expiration[expiration:\u0026lt;duration\u0026gt;|default=0s]# How many keys to fetch in each batch.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.batchsize[batch_size:\u0026lt;int\u0026gt;|default=0]# Maximum active requests to memcache.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.parallelism[parallelism:\u0026lt;int\u0026gt;|default=100] memcached_client_config The memcached_client_config configures the client used to connect to Memcached.\n# Hostname for memcached service to use when caching chunks. If empty, no# memcached will be used.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.hostname[host:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# SRV service used to discover memcache servers.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.service[service:\u0026lt;string\u0026gt;|default=\u0026#34;memcached\u0026#34;]# Maximum time to wait before giving up on memcached requests.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.timeout[timeout:\u0026lt;duration\u0026gt;|default=100ms]# Maximum number of idle connections in pool.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.max-idle-conns[max_idle_conns:\u0026lt;int\u0026gt;|default=16]# Period with which to poll DNS for memcache servers.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.update-interval[update_interval:\u0026lt;duration\u0026gt;|default=1m0s]# Use consistent hashing to distribute to memcache servers.# CLI flag: -\u0026lt;prefix\u0026gt;.memcached.consistent-hash[consistent_hash:\u0026lt;boolean\u0026gt;|default=false] fifo_cache_config The fifo_cache_config configures the local in-memory cache.\n# The number of entries to cache.# CLI flag: -\u0026lt;prefix\u0026gt;.fifocache.size[size:\u0026lt;int\u0026gt;|default=0]# The expiry duration for the cache.# CLI flag: -\u0026lt;prefix\u0026gt;.fifocache.duration[validity:\u0026lt;duration\u0026gt;|default=0s] configdb_config The configdb_config configures the config database storing rules and alerts, and used by the \u0026lsquo;configs\u0026rsquo; service to expose APIs to manage them.\n# URI where the database can be found (for dev you can use memory://)# CLI flag: -database.uri[uri:\u0026lt;string\u0026gt;|default=\u0026#34;postgres://postgres@configs-db.weave.local/configs?sslmode=disable\u0026#34;]# Path where the database migration files can be found# CLI flag: -database.migrations[migrationsdir:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;]# File containing password (username goes in URI)# CLI flag: -database.password-file[passwordfile:\u0026lt;string\u0026gt;|default=\u0026#34;\u0026#34;] configstore_config The configstore_config configures the config database storing rules and alerts, and is used by the Cortex alertmanager.\nconfigsapiurl:# URL of configs API server.# CLI flag: -\u0026lt;prefix\u0026gt;.configs.url[url:\u0026lt;url\u0026gt;|default=]# Timeout for requests to Weave Cloud configs service.# CLI flag: -\u0026lt;prefix\u0026gt;.configs.client-timeout[clienttimeout:\u0026lt;duration\u0026gt;|default=5s]","excerpt":"Cortex can be configured using a YAML file - specified using the -config.file flag - or CLI flags. …","ref":"/docs/configuration/configuration-file/","title":"Configuration file"},{"body":" \nCortex provides horizontally scalable, highly available, multi-tenant, long term storage for Prometheus.\n Horizontally scalable: Cortex can run across multiple machines in a cluster, exceeding the throughput and storage of a single machine. This enables you to send the metrics from multiple Prometheus servers to a single Cortex cluster and run \u0026ldquo;globally aggregated\u0026rdquo; queries across all data in a single place. Highly available: When run in a cluster, Cortex can replicate data between machines. This allows you to survive machine failure without gaps in your graphs. Multi-tenant: Cortex can isolate data and queries from multiple different independent Prometheus sources in a single cluster, allowing untrusted parties to share the same cluster. Long term storage: Cortex supports Amazon DynamoDB, Google Bigtable, Cassandra, S3 and GCS for long term storage of metric data. This allows you to durably store data for longer than the lifetime of any single machine, and use this data for long term capacity planning. Cortex is a CNCF sandbox project used in several production systems including Weave Cloud and Grafana Cloud. Cortex is primarily used as a remote write destination for Prometheus, exposing a Prometheus-compatible query API.\nDocumentation Read the getting started guide if you\u0026rsquo;re new to the project. Before deploying Cortex with a permanent storage backend you should read:\n An overview of Cortex\u0026rsquo;s architecture A general guide to running Cortex Information regarding configuring Cortex For a guide to contributing to Cortex, see the contributor guidelines.\nFurther reading To learn more about Cortex, consult the following documents \u0026amp; talks:\n May 2019 KubeCon talks; \u0026ldquo;Cortex: Intro\u0026rdquo; (video, slides, blog post) and \u0026ldquo;Cortex: Deep Dive\u0026rdquo; (video, slides) Feb 2019 blog post \u0026amp; podcast; \u0026ldquo;Prometheus Scalability with Bryan Boreham\u0026rdquo; (podcast) Feb 2019 blog post; \u0026ldquo;How Aspen Mesh Runs Cortex in Production\u0026ldquo; Dec 2018 KubeCon talk; \u0026ldquo;Cortex: Infinitely Scalable Prometheus\u0026rdquo; (video, slides) Dec 2018 CNCF blog post; \u0026ldquo;Cortex: a multi-tenant, horizontally scalable Prometheus-as-a-Service\u0026ldquo; Nov 2018 CloudNative London meetup talk; \u0026ldquo;Cortex: Horizontally Scalable, Highly Available Prometheus\u0026rdquo; (slides) Nov 2018 CNCF TOC Presentation; \u0026ldquo;Horizontally Scalable, Multi-tenant Prometheus\u0026rdquo; (slides) Sept 2018 blog post; \u0026ldquo;What is Cortex?\u0026ldquo; Aug 2018 PromCon panel; \u0026ldquo;Prometheus Long-Term Storage Approaches\u0026rdquo; (video) Jul 2018 design doc; \u0026ldquo;Cortex Query Optimisations\u0026ldquo; Aug 2017 PromCon talk; \u0026ldquo;Cortex: Prometheus as a Service, One Year On\u0026rdquo; (videos, slides, write up part 1, part 2, part 3) Jun 2017 Prometheus London meetup talk; \u0026ldquo;Cortex: open-source, horizontally-scalable, distributed Prometheus\u0026rdquo; (video) Dec 2016 KubeCon talk; \u0026ldquo;Weave Cortex: Multi-tenant, horizontally scalable Prometheus as a Service\u0026rdquo; (video, slides) Aug 2016 PromCon talk; \u0026ldquo;Project Frankenstein: Multitenant, Scale-Out Prometheus\u0026rdquo;: (video, slides) Jun 2016 design document; \u0026ldquo;Project Frankenstein: A Multi Tenant, Scale Out Prometheus\u0026ldquo; Getting Help If you have any questions about Cortex:\n Ask a question on the Cortex Slack channel. To invite yourself to the CNCF Slack, visit http://slack.cncf.io/. File an issue. Send an email to cortex-users@lists.cncf.io Your feedback is always welcome.\nHosted Cortex (Prometheus as a service) There are several commercial services where you can use Cortex on-demand:\nWeave Cloud Weave Cloud from Weaveworks lets you deploy, manage, and monitor container-based applications. Sign up at https://cloud.weave.works and follow the instructions there. Additional help can also be found in the Weave Cloud documentation.\nInstrumenting Your App: Best Practices\nGrafana Cloud To use Cortex as part of Grafana Cloud, sign up for Grafana Cloud by clicking \u0026ldquo;Log In\u0026rdquo; in the top right and then \u0026ldquo;Sign Up Now\u0026rdquo;. Cortex is included as part of the Starter and Basic Hosted Grafana plans.\n","excerpt":"Cortex provides horizontally scalable, highly available, multi-tenant, long term storage for …","ref":"/docs/","title":"Documentation"},{"body":" This document assumes you have read the architecture document.\nIn addition to the general advice in this document, please see these platform-specific notes:\n AWS Planning Tenants If you will run Cortex as a multi-tenant system, you need to give each tenant a unique ID - this can be any string. Managing tenants and allocating IDs must be done outside of Cortex. You must also configure Authentication and Authorisation.\nStorage Cortex requires a scalable storage back-end. Commercial cloud options are DynamoDB and Bigtable: the advantage is you don\u0026rsquo;t have to know how to manage them, but the downside is they have specific costs. Alternatively you can choose Cassandra, which you will have to install and manage.\nComponents Every Cortex installation will need Distributor, Ingester and Querier. Alertmanager, Ruler and Query-frontend are optional.\nOther dependencies Cortex needs a KV store to track sharding of data between processes. This can be either Etcd or Consul.\nIf you want to configure recording and alerting rules (i.e. if you will run the Ruler and Alertmanager components) then a Postgres database is required to store configs.\nMemcached is not essential but highly recommended.\nIngester replication factor The standard replication factor is three, so that we can drop one replica and be unconcerned, as we still have two copies of the data left for redundancy. This is configurable: you can run with more redundancy or less, depending on your risk appetite.\nSchema Schema periodic table The periodic table from argument (-dynamodb.periodic-table.from=\u0026lt;date\u0026gt; if using command line flags, the from field for the first schema entry if using YAML) should be set to the date the oldest metrics you will be sending to Cortex. Generally that means set it to the date you are first deploying this instance. If you use an example date from years ago table-manager will create hundreds of tables. You can also avoid creating too many tables by setting a reasonable retention in the table-manager (-table-manager.retention-period=\u0026lt;duration\u0026gt;).\nSchema version Choose schema version 9 in most cases; version 10 if you expect hundreds of thousands of timeseries under a single name. Anything older than v9 is much less efficient.\nChunk encoding Standard choice would be Bigchunk, which is the most flexible chunk encoding. You may get better compression from Varbit, if many of your timeseries do not change value from one day to the next.\nSizing You will want to estimate how many nodes are required, how many of each component to run, and how much storage space will be required. In practice, these will vary greatly depending on the metrics being sent to Cortex.\nSome key parameters are:\n The number of active series. If you have Prometheus already you can query prometheus_tsdb_head_series to see this number. Sampling rate, e.g. a new sample for each series every 15 seconds. Multiply this by the number of active series to get the total rate at which samples will arrive at Cortex. The rate at which series are added and removed. This can be very high if you monitor objects that come and go - for example if you run thousands of batch jobs lasting a minute or so and capture metrics with a unique ID for each one. Read how to analyse this on Prometheus How compressible the time-series data are. If a metric stays at the same value constantly, then Cortex can compress it very well, so 12 hours of data sampled every 15 seconds would be around 2KB. On the other hand if the value jumps around a lot it might take 10KB. There are not currently any tools available to analyse this. How long you want to retain data for, e.g. 1 month or 2 years. Other parameters which can become important if you have particularly high values:\n Number of different series under one metric name. Number of labels per series. Rate and complexity of queries. Now, some rules of thumb:\n Each million series in an ingester takes 15GB of RAM. Total number of series in ingesters is number of active series times the replication factor. This is with the default of 12-hour chunks - RAM required will reduce if you set -ingester.max-chunk-age lower (trading off more back-end database IO) Each million series (including churn) consumes 15GB of chunk storage and 4GB of index, per day (so multiply by the retention period). Each 100,000 samples/sec arriving takes 1 CPU in distributors. Distributors don\u0026rsquo;t need much RAM. If you turn on compression between distributors and ingesters (for example to save on inter-zone bandwidth charges at AWS) they will use significantly more CPU (approx 100% more for distributor and 50% more for ingester).\nCaching Cortex can retain data in-process or in Memcached to speed up various queries by caching:\n individual chunks index lookups for one label on one day the results of a whole query You should always include Memcached in your Cortex install so results from one process can be re-used by another. In-process caching can cut fetch times slightly and reduce the load on Memcached.\nIngesters can also be configured to use Memcached to avoid re-writing index and chunk data which has already been stored in the back-end database. Again, highly recommended.\nOrchestration Because Cortex is designed to run multiple instances of each component (ingester, querier, etc.), you probably want to automate the placement and shepherding of these instances. Most users choose Kubernetes to do this, but this is not mandatory.\nConfiguration Resource requests If using Kubernetes, each container should specify resource requests so that the scheduler can place them on a node with sufficient capacity.\nFor example an ingester might request:\n resources: requests: cpu: 4 memory: 10Gi The specific values here should be adjusted based on your own experiences running Cortex - they are very dependent on rate of data arriving and other factors such as series churn.\nTake extra care with ingesters Ingesters hold hours of timeseries data in memory; you can configure Cortex to replicate the data but you should take steps to avoid losing all replicas at once: - Don\u0026rsquo;t run multiple ingesters on the same node. - Don\u0026rsquo;t run ingesters on preemptible/spot nodes. - Spread out ingesters across racks / availability zones / whatever applies in your datacenters.\nYou can ask Kubernetes to avoid running on the same node like this:\n affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: name operator: In values: - ingester topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; Give plenty of time for an ingester to hand over or flush data to store when shutting down; for Kubernetes this looks like:\n terminationGracePeriodSeconds: 2400 Ask Kubernetes to limit rolling updates to one ingester at a time, and signal the old one to stop before the new one is ready:\n strategy: rollingUpdate: maxSurge: 0 maxUnavailable: 1 Ingesters provide an http hook to signal readiness when all is well; this is valuable because it stops a rolling update at the first problem:\n readinessProbe: httpGet: path: /ready port: 80 We do not recommend configuring a liveness probe on ingesters - killing them is a last resort and should not be left to a machine.\nRemote writing Prometheus To configure your Prometheus instances for remote writes take a look at the Prometheus Remote Write Config. We recommend to tune the following parameters of the queue_config:\nremote_write:-queue_config:capacity:5000max_shards:20min_shards:5max_samples_per_send:1000 Please take note that these values are tweaked for our use cases and may be necessary to adapt depending on your workload. Take a look at the remote write tuning docs.\nIf you experience a rather high delay for your metrics to appear in Cortex (15s+) you can try increasing the min_shards in your remote write config. Sometimes Prometheus does not increase the number of shards even though it hasn\u0026rsquo;t caught up the lag. You can monitor the delay with this Prometheus query:\ntime() - sum by (statefulset_kubernetes_io_pod_name) (prometheus_remote_storage_queue_highest_sent_timestamp_seconds) Optimising Optimising Storage These ingester options reduce the chance of storing multiple copies of the same data:\n -ingester.spread-flushes=true -ingester.chunk-age-jitter=0 Add a chunk cache via -memcached.hostname to allow writes to be de-duplicated.\nAs recommended under Chunk encoding, use Bigchunk:\n -ingester.chunk-encoding=3 # bigchunk ","excerpt":"This document assumes you have read the architecture document.\nIn addition to the general advice in …","ref":"/docs/guides/running-in-production/","title":"Running Cortex in Production"},{"body":"All Cortex components take the tenant ID from a header X-Scope-OrgID on each request. They trust this value completely: if you need to protect your Cortex installation from accidental or malicious calls then you must add an additional layer of protection.\nTypically this means you run Cortex behind a reverse proxy, and ensure that all callers, both machines sending data over the remote_write interface and humans sending queries from GUIs, supply credentials which identify them and confirm they are authorised.\nWhen configuring the remote_write API in Prometheus there is no way to add extra headers. The user and password fields of http Basic auth, or Bearer token, can be used to convey tenant ID and/or credentials.\n","excerpt":"All Cortex components take the tenant ID from a header X-Scope-OrgID on each request. They trust …","ref":"/docs/guides/auth/","title":"Authentication and Authorisation"},{"body":" General Notes Cortex has evolved over several years, and the command-line options sometimes reflect this heritage. In some cases the default value for options is not the recommended value, and in some cases names do not reflect the true meaning. We do intend to clean this up, but it requires a lot of care to avoid breaking existing installations. In the meantime we regret the inconvenience.\nDuration arguments should be specified with a unit like 5s or 3h. Valid time units are \u0026ldquo;ms\u0026rdquo;, \u0026ldquo;s\u0026rdquo;, \u0026ldquo;m\u0026rdquo;, \u0026ldquo;h\u0026rdquo;.\nQuerier -querier.max-concurrent The maximum number of top-level PromQL queries that will execute at the same time, per querier process. If using the query frontend, this should be set to at least (querier.worker-parallelism * number of query frontend replicas). Otherwise queries may queue in the queriers and not the frontend, which will affect QoS.\n -querier.query-parallelism This refers to database queries against the store (e.g. Bigtable or DynamoDB). This is the max subqueries run in parallel per higher-level query.\n -querier.timeout The timeout for a top-level PromQL query.\n -querier.max-samples Maximum number of samples a single query can load into memory, to avoid blowing up on enormous queries.\nThe next three options only apply when the querier is used together with the Query Frontend:\n -querier.frontend-address Address of query frontend service, used by workers to find the frontend which will give them queries to execute.\n -querier.dns-lookup-period How often the workers will query DNS to re-check where the frontend is.\n -querier.worker-parallelism Number of simultaneous queries to process, per worker process. See note on -querier.max-concurrent\nQuerier and Ruler The ingester query API was improved over time, but defaults to the old behaviour for backwards-compatibility. For best results both of these next two flags should be set to true:\n -querier.batch-iterators This uses iterators to execute query, as opposed to fully materialising the series in memory, and fetches multiple results per loop.\n -querier.ingester-streaming Use streaming RPCs to query ingester, to reduce memory pressure in the ingester.\n -querier.iterators This is similar to -querier.batch-iterators but less efficient. If both iterators and batch-iterators are true, batch-iterators will take precedence.\n -promql.lookback-delta Time since the last sample after which a time series is considered stale and ignored by expression evaluations.\nQuery Frontend -querier.align-querier-with-step If set to true, will cause the query frontend to mutate incoming queries and align their start and end parameters to the step parameter of the query. This improves the cacheability of the query results.\n -querier.split-queries-by-day If set to true, will cause the query frontend to split multi-day queries into multiple single-day queries and execute them in parallel.\n -querier.cache-results If set to true, will cause the querier to cache query results. The cache will be used to answer future, overlapping queries. The query frontend calculates extra queries required to fill gaps in the cache.\n -frontend.max-cache-freshness When caching query results, it is desirable to prevent the caching of very recent results that might still be in flux. Use this parameter to configure the age of results that should be excluded.\n -memcached.{hostname, service, timeout} Use these flags to specify the location and timeout of the memcached cluster used to cache query results.\n -redis.{endpoint, timeout} Use these flags to specify the location and timeout of the Redis service used to cache query results.\nDistributor -distributor.shard-by-all-labels In the original Cortex design, samples were sharded amongst distributors by the combination of (userid, metric name). Sharding by metric name was designed to reduce the number of ingesters you need to hit on the read path; the downside was that you could hotspot the write path.\nIn hindsight, this seems like the wrong choice: we do many orders of magnitude more writes than reads, and ingester reads are in-memory and cheap. It seems the right thing to do is to use all the labels to shard, improving load balancing and support for very high cardinality metrics.\nSet this flag to true for the new behaviour.\nUpgrade notes: As this flag also makes all queries always read from all ingesters, the upgrade path is pretty trivial; just enable the flag. When you do enable it, you\u0026rsquo;ll see a spike in the number of active series as the writes are \u0026ldquo;reshuffled\u0026rdquo; amongst the ingesters, but over the next stale period all the old series will be flushed, and you should end up with much better load balancing. With this flag enabled in the queriers, reads will always catch all the data from all ingesters.\n -distributor.extra-query-delay This is used by a component with an embedded distributor (Querier and Ruler) to control how long to wait until sending more than the minimum amount of queries needed for a successful response.\n distributor.ha-tracker.enable-for-all-users Flag to enable, for all users, handling of samples with external labels identifying replicas in an HA Prometheus setup. This defaults to false, and is technically defined in the Distributor limits.\n distributor.ha-tracker.enable Enable the distributors HA tracker so that it can accept samples from Prometheus HA replicas gracefully (requires labels). Global (for distributors), this ensures that the necessary internal data structures for the HA handling are created. The option enable-for-all-users is still needed to enable ingestion of HA samples for all users.\n distributor.drop-label This flag can be used to specify label names that to drop during sample ingestion within the distributor and can be repeated in order to drop multiple labels.\n Ring/HA Tracker Store The KVStore client is used by both the Ring and HA Tracker. - {ring,distributor.ha-tracker}.prefix The prefix for the keys in the store. Should end with a /. For example with a prefix of foo/, the key bar would be stored under foo/bar. - {ring,distributor.ha-tracker}.store Backend storage to use for the ring (consul, etcd, inmemory, memberlist, multi).\nConsul By default these flags are used to configure Consul used for the ring. To configure Consul for the HA tracker, prefix these flags with distributor.ha-tracker.\n consul.hostname Hostname and port of Consul. consul.acltoken ACL token used to interact with Consul. consul.client-timeout HTTP timeout when talking to Consul. consul.consistent-reads Enable consistent reads to Consul. etcd By default these flags are used to configure etcd used for the ring. To configure etcd for the HA tracker, prefix these flags with distributor.ha-tracker.\n etcd.endpoints The etcd endpoints to connect to. etcd.dial-timeout The timeout for the etcd connection. etcd.max-retries The maximum number of retries to do for failed ops. memberlist (EXPERIMENTAL) Flags for configuring KV store based on memberlist library. This feature is experimental, please don\u0026rsquo;t use it yet.\n memberlist.nodename Name of the node in memberlist cluster. Defaults to hostname. memberlist.retransmit-factor Multiplication factor used when sending out messages (factor * log(N+1)). If not set, default value is used. memberlist.join Other cluster members to join. Can be specified multiple times. memberlist.abort-if-join-fails If this node fails to join memberlist cluster, abort. memberlist.left-ingesters-timeout How long to keep LEFT ingesters in the ring. Note: this is only used for gossiping, LEFT ingesters are otherwise invisible. memberlist.leave-timeout Timeout for leaving memberlist cluster. memberlist.gossip-interval How often to gossip with other cluster members. Uses memberlist LAN defaults if 0. memberlist.gossip-nodes How many nodes to gossip with in each gossip interval. Uses memberlist LAN defaults if 0. memberlist.pullpush-interval How often to use pull/push sync. Uses memberlist LAN defaults if 0. memberlist.bind-addr IP address to listen on for gossip messages. Multiple addresses may be specified. Defaults to 0.0.0.0. memberlist.bind-port Port to listen on for gossip messages. Defaults to 7946. memberlist.packet-dial-timeout Timeout used when connecting to other nodes to send packet. memberlist.packet-write-timeout Timeout for writing \u0026lsquo;packet\u0026rsquo; data. memberlist.transport-debug Log debug transport messages. Note: global log.level must be at debug level as well. Multi KV This is a special key-value implementation that uses two different KV stores (eg. consul, etcd or memberlist). One of them is always marked as primary, and all reads and writes go to primary store. Other one, secondary, is only used for writes. The idea is that operator can use multi KV store to migrate from primary to secondary store in runtime.\nFor example, migration from Consul to Etcd would look like this:\n Set ring.store to use multi store. Set -multi.primary=consul and -multi.secondary=etcd. All consul and etcd settings must still be specified. Start all Cortex microservices. They will still use Consul as primary KV, but they will also write share ring via etcd. Operator can now use \u0026ldquo;runtime config\u0026rdquo; mechanism to switch primary store to etcd. After all Cortex microservices have picked up new primary store, and everything looks correct, operator can now shut down Consul, and modify Cortex configuration to use -ring.store=etcd only. At this point, Consul can be shut down. Multi KV has following parameters:\n multi.primary - name of primary KV store. Same values as in ring.store are supported, except multi. multi.secondary - name of secondary KV store. multi.mirror-enabled - enable mirroring of values to secondary store, defaults to true multi.mirror-timeout - wait max this time to write to secondary store to finish. Default to 2 seconds. Errors writing to secondary store are not reported to caller, but are logged and also reported via cortex_multikv_mirror_write_errors_total metric. Multi KV also reacts on changes done via runtime configuration. It uses this section:\nmulti_kv_config:mirror-enabled:falseprimary:memberlist Note that runtime configuration values take precedence over command line options.\nHA Tracker HA tracking has two of its own flags: - distributor.ha-tracker.cluster Prometheus label to look for in samples to identify a Prometheus HA cluster. (default \u0026ldquo;cluster\u0026rdquo;) - distributor.ha-tracker.replica Prometheus label to look for in samples to identify a Prometheus HA replica. (default \u0026ldquo;__replica__\u0026rdquo;)\nIt\u0026rsquo;s reasonable to assume people probably already have a cluster label, or something similar. If not, they should add one along with __replica__ via external labels in their Prometheus config. If you stick to these default values your Prometheus config could look like this (POD_NAME is an environment variable which must be set by you):\nglobal:external_labels:cluster:clustername__replica__:$POD_NAME HA Tracking looks for the two labels (which can be overwritten per user)\nIt also talks to a KVStore and has it\u0026rsquo;s own copies of the same flags used by the Distributor to connect to for the ring. - distributor.ha-tracker.failover-timeout If we don\u0026rsquo;t receive any samples from the accepted replica for a cluster in this amount of time we will failover to the next replica we receive a sample from. This value must be greater than the update timeout (default 30s) - distributor.ha-tracker.store Backend storage to use for the ring (consul, etcd, inmemory). (default \u0026ldquo;consul\u0026rdquo;) - distributor.ha-tracker.update-timeout Update the timestamp in the KV store for a given cluster/replica only after this amount of time has passed since the current stored timestamp. (default 15s)\nIngester -ingester.max-chunk-age The maximum duration of a timeseries chunk in memory. If a timeseries runs for longer than this the current chunk will be flushed to the store and a new chunk created. (default 12h)\n -ingester.max-chunk-idle If a series doesn\u0026rsquo;t receive a sample for this duration, it is flushed and removed from memory.\n -ingester.max-stale-chunk-idle If a series receives a staleness marker, then we wait for this duration to get another sample before we close and flush this series, removing it from memory. You want it to be at least 2x the scrape interval as you don\u0026rsquo;t want a single failed scrape to cause a chunk flush.\n -ingester.chunk-age-jitter To reduce load on the database exactly 12 hours after starting, the age limit is reduced by a varying amount up to this. (default 20m)\n -ingester.spread-flushes Makes the ingester flush each timeseries at a specific point in the max-chunk-age cycle. This means multiple replicas of a chunk are very likely to contain the same contents which cuts chunk storage space by up to 66%. Set -ingester.chunk-age-jitter to 0 when using this option. If a chunk cache is configured (via -memcached.hostname) then duplicate chunk writes are skipped which cuts write IOPs.\n -ingester.join-after How long to wait in PENDING state during the hand-over process. (default 0s)\n -ingester.max-transfer-retries How many times a LEAVING ingester tries to find a PENDING ingester during the hand-over process. Each attempt takes a second or so. Negative value or zero disables hand-over process completely. (default 10)\n -ingester.normalise-tokens Deprecated. New ingesters always write \u0026ldquo;normalised\u0026rdquo; tokens to the ring. Normalised tokens consume less memory to encode and decode; as the ring is unmarshalled regularly, this significantly reduces memory usage of anything that watches the ring.\nCortex 0.4.0 is the last version that can write denormalised tokens. Cortex 0.5.0 and later will always write normalised tokens, although it can still read denormalised tokens written by older ingesters.\nIt\u0026rsquo;s perfectly OK to have a mix of ingesters running denormalised (\u0026lt;= 0.4.0) and normalised tokens (either by using -ingester.normalise-tokens in Cortex \u0026lt;= 0.4.0, or Cortex 0.5.0+) during upgrades.\n -ingester.chunk-encoding Pick one of the encoding formats for timeseries data, which have different performance characteristics. Bigchunk uses the Prometheus V2 code, and expands in memory to arbitrary length. Varbit, Delta and DoubleDelta use Prometheus V1 code, and are fixed at 1K per chunk. Defaults to DoubleDelta, but we recommend Bigchunk.\n -store.bigchunk-size-cap-bytes When using bigchunks, start a new bigchunk and flush the old one if the old one reaches this size. Use this setting to limit memory growth of ingesters with a lot of timeseries that last for days.\n -ingester-client.expected-timeseries When push requests arrive, pre-allocate this many slots to decode them. Tune this setting to reduce memory allocations and garbage. This should match the max_samples_per_send in your queue_config for Prometheus.\n -ingester-client.expected-samples-per-series When push requests arrive, pre-allocate this many slots to decode them. Tune this setting to reduce memory allocations and garbage. Under normal conditions, Prometheus scrapes should arrive with one sample per series.\n -ingester-client.expected-labels When push requests arrive, pre-allocate this many slots to decode them. Tune this setting to reduce memory allocations and garbage. The optimum value will depend on how many labels are sent with your timeseries samples.\n -store.chunk-cache-stubs Where you don\u0026rsquo;t want to cache every chunk written by ingesters, but you do want to take advantage of chunk write deduplication, this option will make ingesters write a placeholder to the cache for each chunk. Make sure you configure ingesters with a different cache to queriers, which need the whole value.\nRuntime Configuration file Cortex has a concept of \u0026ldquo;runtime config\u0026rdquo; file, which is simply a file that is reloaded while Cortex is running. It is used by some Cortex components to allow operator to change some aspects of Cortex configuration without restarting it. File is specified by using -runtime-config.file=\u0026lt;filename\u0026gt; flag and reload period (which defaults to 10 seconds) can be changed by -runtime-config.reload-period=\u0026lt;duration\u0026gt; flag. Previously this mechanism was only used by limits overrides, and flags were called -limits.per-user-override-config=\u0026lt;filename\u0026gt; and -limits.per-user-override-period=10s respectively. These are still used, if -runtime-config.file=\u0026lt;filename\u0026gt; is not specified.\nAt the moment, two components use runtime configuration: limits and multi KV store.\nExample runtime configuration file:\noverrides:tenant1:ingestion_rate:10000max_series_per_metric:100000max_series_per_query:100000tenant2:max_samples_per_query:1000000max_series_per_metric:100000max_series_per_query:100000multi_kv_config:mirror-enabled:falseprimary:memberlist When running Cortex on Kubernetes, store this file in a config map and mount it in each services\u0026rsquo; containers. When changing the values there is no need to restart the services, unless otherwise specified.\nIngester, Distributor \u0026amp; Querier limits. Cortex implements various limits on the requests it can process, in order to prevent a single tenant overwhelming the cluster. There are various default global limits which apply to all tenants which can be set on the command line. These limits can also be overridden on a per-tenant basis by using overrides field of runtime configuration file.\nThe overrides field is a map of tenant ID (same values as passed in the X-Scope-OrgID header) to the various limits. An example could look like:\noverrides:tenant1:ingestion_rate:10000max_series_per_metric:100000max_series_per_query:100000tenant2:max_samples_per_query:1000000max_series_per_metric:100000max_series_per_query:100000 Valid per-tenant limits are (with their corresponding flags for default values):\n ingestion_rate_strategy / -distributor.ingestion-rate-limit-strategy ingestion_rate / -distributor.ingestion-rate-limit ingestion_burst_size / -distributor.ingestion-burst-size The per-tenant rate limit (and burst size), in samples per second. It supports two strategies: local (default) and global.\nThe local strategy enforces the limit on a per distributor basis, actual effective rate limit will be N times higher, where N is the number of distributor replicas.\nThe global strategy enforces the limit globally, configuring a per-distributor local rate limiter as ingestion_rate / N, where N is the number of distributor replicas (it\u0026rsquo;s automatically adjusted if the number of replicas change). The ingestion_burst_size refers to the per-distributor local rate limiter (even in the case of the global strategy) and should be set at least to the maximum number of samples expected in a single push request. For this reason, the global strategy requires that push requests are evenly distributed across the pool of distributors; if you use a load balancer in front of the distributors you should be already covered, while if you have a custom setup (ie. an authentication gateway in front) make sure traffic is evenly balanced across distributors.\nThe global strategy requires the distributors to form their own ring, which is used to keep track of the current number of healthy distributor replicas. The ring is configured by distributor: { ring: {}} / -distributor.ring.*.\n max_label_name_length / -validation.max-length-label-name max_label_value_length / -validation.max-length-label-value max_label_names_per_series / -validation.max-label-names-per-series Also enforced by the distributor, limits on the on length of labels and their values, and the total number of labels allowed per series.\n reject_old_samples / -validation.reject-old-samples reject_old_samples_max_age / -validation.reject-old-samples.max-age creation_grace_period / -validation.create-grace-period Also enforce by the distributor, limits on how far in the past (and future) timestamps that we accept can be.\n max_series_per_user / -ingester.max-series-per-user max_series_per_metric / -ingester.max-series-per-metric Enforced by the ingesters; limits the number of active series a user (or a given metric) can have. When running with -distributor.shard-by-all-labels=false (the default), this limit will enforce the maximum number of series a metric can have \u0026lsquo;globally\u0026rsquo;, as all series for a single metric will be sent to the same replication set of ingesters. This is not the case when running with -distributor.shard-by-all-labels=true, so the actual limit will be N/RF times higher, where N is number of ingester replicas and RF is configured replication factor.\nAn active series is a series to which a sample has been written in the last -ingester.max-chunk-idle duration, which defaults to 5 minutes.\n max_global_series_per_user / -ingester.max-global-series-per-user max_global_series_per_metric / -ingester.max-global-series-per-metric Like max_series_per_user and max_series_per_metric, but the limit is enforced across the cluster. Each ingester is configured with a local limit based on the replication factor, the -distributor.shard-by-all-labels setting and the current number of healthy ingesters, and is kept updated whenever the number of ingesters change.\nRequires -distributor.replication-factor and -distributor.shard-by-all-labels set for the ingesters too.\n max_series_per_query / -ingester.max-series-per-query max_samples_per_query / -ingester.max-samples-per-query Limits on the number of timeseries and samples returns by a single ingester during a query.\nStorage s3.force-path-style Set this to true to force the request to use path-style addressing (http://s3.amazonaws.com/BUCKET/KEY). By default, the S3 client will use virtual hosted bucket addressing when possible (http://BUCKET.s3.amazonaws.com/KEY).\n","excerpt":"General Notes Cortex has evolved over several years, and the command-line options sometimes reflect …","ref":"/docs/configuration/arguments/","title":"Cortex Arguments"},{"body":" Cortex can be run as a single binary or as multiple independent microservices. The single-binary mode is easier to deploy and is aimed mainly at users wanting to try out Cortex or develop on it. The microservices mode is intended for production usage, as it allows you to independently scale different services and isolate failures. This document will focus on single-process Cortex. See the architecture doc For more information about the microservices.\nSeparately from single process vs microservices decision, Cortex can be configured to use local storage or cloud storage (DynamoDB, Bigtable, Cassandra, S3, GCS etc). This document will focus on using local storage. Local storage is explicitly not production ready at this time. Cortex can also make use of external memcacheds for caching and although these are not mandatory, they should be used in production.\nSingle instance, single process For simplicity \u0026amp; to get started, we\u0026rsquo;ll run it as a single process with no dependencies:\n$ go build ./cmd/cortex $ ./cortex -config.file=./docs/configuration/single-process-config.yaml This starts a single Cortex node storing chunks and index to your local filesystem in /tmp/cortex. It is not intended for production use.\nAdd the following to your Prometheus config (documentation/examples/prometheus.yml in Prometheus repo):\nremote_write:-url:http://localhost:9009/api/prom/push And start Prometheus with that config file:\n$ git clone https://github.com/prometheus/prometheus $ cd prometheus $ go build ./cmd/prometheus $ ./prometheus --config.file=./documentation/examples/prometheus.yml Your Prometheus instance will now start pushing data to Cortex. To query that data, start a Grafana instance:\n$ docker run -d --name=grafana -p 3000:3000 grafana/grafana In the Grafana UI (username/password admin/admin), add a Prometheus datasource for Cortex (http://host.docker.internal:9009/api/prom).\nTo clean up: press CTRL-C in both terminals (for Cortex and Promrtheus) and run docker rm -f grafana.\nHorizontally scale out Next we\u0026rsquo;re going to show how you can run a scale out Cortex cluster using Docker. We\u0026rsquo;ll need: - A built Cortex image. - A Docker network to put these containers on so they can resolve each other by name. - A single node Consul instance to coordinate the Cortex cluster.\n$ make ./cmd/cortex/.uptodate $ docker network create cortex $ docker run -d --name=consul --network=cortex -e CONSUL_BIND_INTERFACE=eth0 consul Next we\u0026rsquo;ll run a couple of Cortex instances pointed at that Consul. You\u0026rsquo;ll note with Cortex configuration can be specified in either a config file or overridden on the command line. See the arguments documentation for more information about Cortex configuration options.\n$ docker run -d --name=cortex1 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config.yaml:/etc/single-process-config.yaml \\ -p 9001:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 $ docker run -d --name=cortex2 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config.yaml:/etc/single-process-config.yaml \\ -p 9002:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 If you go to http://localhost:9001/ring (or http://localhost:9002/ring) you should see both Cortex nodes join the ring.\nTo demonstrate the correct operation of Cortex clustering, we\u0026rsquo;ll send samples to one of the instances and queries to another. In production, you\u0026rsquo;d want to load balance both pushes and queries evenly among all the nodes.\nPoint Prometheus at the first:\nremote_write:-url:http://localhost:9001/api/prom/push$ ./prometheus --config.file=./documentation/examples/prometheus.yml And Grafana at the second:\n$ docker run -d --name=grafana --network=cortex -p 3000:3000 grafana/grafana In the Grafana UI (username/password admin/admin), add a Prometheus datasource for Cortex (http://cortex2:9009/api/prom).\nTo clean up: CTRL-C the Prometheus process and run:\n$ docker rm -f cortex1 cortex2 consul grafana $ docker network remove cortex High availability with replication In this last demo we\u0026rsquo;ll show how Cortex can replicate data among three nodes, and demonstrate Cortex can tolerate a node failure without affecting reads and writes.\nFirst, create a network and run a new Consul and Grafana:\n$ docker network create cortex $ docker run -d --name=consul --network=cortex -e CONSUL_BIND_INTERFACE=eth0 consul $ docker run -d --name=grafana --network=cortex -p 3000:3000 grafana/grafana Finally, launch 3 Cortex nodes with replication factor 3:\n$ docker run -d --name=cortex1 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config.yaml:/etc/single-process-config.yaml \\ -p 9001:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 \\ -distributor.replication-factor=3 $ docker run -d --name=cortex2 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config.yaml:/etc/single-process-config.yaml \\ -p 9002:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 \\ -distributor.replication-factor=3 $ docker run -d --name=cortex3 --network=cortex \\ -v $(pwd)/docs/configuration/single-process-config.yaml:/etc/single-process-config.yaml \\ -p 9003:9009 \\ quay.io/cortexproject/cortex \\ -config.file=/etc/single-process-config.yaml \\ -ring.store=consul \\ -consul.hostname=consul:8500 \\ -distributor.replication-factor=3 Configure Prometheus to send data to the first replica:\nremote_write:-url:http://localhost:9001/api/prom/push$ ./prometheus --config.file=./documentation/examples/prometheus.yml In Grafana, add a datasource for the 3rd Cortex replica (http://cortex3:9009/api/prom) and verify the same data appears in both Prometheus and Cortex.\nTo show that Cortex can tolerate a node failure, hard kill one of the Cortex replicas:\n$ docker rm -f cortex2 You should see writes and queries continue to work without error.\nTo clean up: CTRL-C the Prometheus process and run:\n$ docker rm -f cortex1 cortex2 cortex3 consul grafana $ docker network remove cortex ","excerpt":"Cortex can be run as a single binary or as multiple independent microservices. The single-binary …","ref":"/docs/getting-started/","title":"Getting Started"},{"body":"You can use the Cortex query frontend with any Prometheus-API compatible service, including Prometheus and Thanos. Use this config file to get the benefits of query parallelisation and caching.\n# Disable the requirement that every request to Cortex has a# X-Scope-OrgID header. `fake` will be substituted in instead.auth_enabled:false# We only want to run the query-frontend module.target:query-frontend# We don\u0026#39;t want the usual /api/prom prefix.http_prefix:server:http_listen_port:9091query_range:split_queries_by_day:truealign_queries_with_step:truecache_results:trueresults_cache:max_freshness:1mcache:# We\u0026#39;re going to use the in-process \u0026#34;FIFO\u0026#34; cache, but you can enable# memcached below.enable_fifocache:truefifocache:size:1024validity:24h# If you want to use a memcached cluster, configure a headless service# in Kubernetes and Cortex will discover the individual instances using# a SRV DNS query. Cortex will then do client-side hashing to spread# the load evenly.# memcached:# memcached_client:# host: memcached.default.svc.cluster.local# service: memcached# consistent_hash: truefrontend:log_queries_longer_than:1scompress_responses:true","excerpt":"You can use the Cortex query frontend with any Prometheus-API compatible service, including …","ref":"/docs/configuration/prometheus-frontend/","title":"Prometheus Frontend"},{"body":" [this is a work in progress]\nSee also the Running in Production document.\nCredentials You can supply credentials to Cortex by setting environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY (and AWS_SESSION_TOKEN if you use MFA), or use a short-term token solution such as kiam.\nShould I use S3 or DynamoDB ? Note that the choices are: \u0026ldquo;chunks\u0026rdquo; of timeseries data in S3 and index in DynamoDB, or everything in DynamoDB. Using just S3 is not an option.\nBroadly S3 is much more expensive to read and write, while DynamoDB is much more expensive to store over months. S3 charges differently, so the cross-over will depend on the size of your chunks, and how long you keep them. Very roughly: for 3KB chunks if you keep them longer than 8 months then S3 is cheaper.\nDynamoDB capacity provisioning By default, the Cortex Tablemanager will provision tables with 1,000 units of write capacity and 300 read - these numbers are chosen to be high enough that most trial installations won\u0026rsquo;t see a bottleneck on storage, but do note that that AWS will charge you approximately $60 per day for this capacity.\nTo match your costs to requirements, observe the actual capacity utilisation via CloudWatch or Prometheus metrics, then adjust the Tablemanager provision via command-line options -dynamodb.chunk-table.write-throughput, read-throughput and similar with .periodic-table which controls the index table.\nTablemanager can even adjust the capacity dynamically, by watching metrics for DynamoDB throttling and ingester queue length. Here is an example set of command-line parameters from a fairly modest install:\n -target=table-manager -metrics.url=http://prometheus.monitoring.svc.cluster.local./api/prom/ -metrics.target-queue-length=100000 -dynamodb.url=dynamodb://us-east-1/ -dynamodb.use-periodic-tables=true -dynamodb.periodic-table.prefix=cortex_index_ -dynamodb.periodic-table.from=2019-05-02 -dynamodb.periodic-table.write-throughput=1000 -dynamodb.periodic-table.write-throughput.scale.enabled=true -dynamodb.periodic-table.write-throughput.scale.min-capacity=200 -dynamodb.periodic-table.write-throughput.scale.max-capacity=2000 -dynamodb.periodic-table.write-throughput.scale.out-cooldown=300 # 5 minutes between scale ups -dynamodb.periodic-table.inactive-enable-ondemand-throughput-mode=true -dynamodb.periodic-table.read-throughput=300 -dynamodb.periodic-table.tag=product_area=cortex -dynamodb.chunk-table.from=2019-05-02 -dynamodb.chunk-table.prefix=cortex_data_ -dynamodb.chunk-table.write-throughput=800 -dynamodb.chunk-table.write-throughput.scale.enabled=true -dynamodb.chunk-table.write-throughput.scale.min-capacity=200 -dynamodb.chunk-table.write-throughput.scale.max-capacity=1000 -dynamodb.chunk-table.write-throughput.scale.out-cooldown=300 # 5 minutes between scale ups -dynamodb.chunk-table.inactive-enable-ondemand-throughput-mode=true -dynamodb.chunk-table.read-throughput=300 -dynamodb.chunk-table.tag=product_area=cortex Several things to note here:\n -metrics.url points at a Prometheus server running within the cluster, scraping Cortex. Currently it is not possible to use Cortex itself as the target here. -metrics.target-queue-length: when the ingester queue is below this level, Tablemanager will not scale up. When the queue is growing above this level, Tablemanager will scale up whatever table is being throttled. The plain throughput values are used when the tables are first created. Scale-up to any level up to this value will be very quick, but if you go higher than this initial value, AWS may take tens of minutes to finish scaling. In the config above they are set. ondemand-throughput-mode tells AWS to charge for what you use, as opposed to continuous provisioning. This mode is cost-effective for older data, which is never written and only read sporadically. ","excerpt":"[this is a work in progress]\nSee also the Running in Production document.\nCredentials You can supply …","ref":"/docs/guides/aws/","title":"Running Cortex at AWS"},{"body":" Cortex consists of multiple horizontally scalable microservices. Each microservice uses the most appropriate technique for horizontal scaling; most are stateless and can handle requests for any users while some (namely the ingesters) are semi-stateful and depend on consistent hashing. This document provides a basic overview of Cortex\u0026rsquo;s architecture.\nThe role of Prometheus Prometheus instances scrape samples from various targets and then push them to Cortex (using Prometheus\u0026rsquo; remote write API). That remote write API emits batched Snappy-compressed Protocol Buffer messages inside the body of an HTTP PUT request.\nCortex requires that each HTTP request bear a header specifying a tenant ID for the request. Request authentication and authorization are handled by an external reverse proxy.\nIncoming samples (writes from Prometheus) are handled by the distributor while incoming reads (PromQL queries) are handled by the querier or optionally by the query frontend.\nStorage Cortex currently supports two storage engines to store and query the time series:\n Chunks (default, stable) Blocks (experimental) The two engines mostly share the same Cortex architecture with few differences outlined in the rest of the document.\nChunks storage (default) The chunks storage stores each single time series into a separate object called Chunk. Each Chunk contains the samples for a given period (defaults to 12 hours). Chunks are then indexed by time range and labels, in order to provide a fast lookup across many (over millions) Chunks.\nFor this reason, the chunks storage consists of:\n An index for the Chunks. This index can be backed by: Amazon DynamoDB Google Bigtable Apache Cassandra An object store for the Chunk data itself, which can be: Amazon DynamoDB Google Bigtable Apache Cassandra Amazon S3 Google Cloud Storage Internally, the access to the chunks storage relies on a unified interface called \u0026ldquo;chunks store\u0026rdquo;. Unlike other Cortex components, the chunk store is not a separate service, but rather a library embedded in the services that need to access the long-term storage: ingester, querier and ruler.\nThe chunk and index format are versioned, this allows Cortex operators to upgrade the cluster to take advantage of new features and improvements. This strategy enables changes in the storage format without requiring any downtime or complex procedures to rewrite the stored data. A set of schemas are used to map the version while reading and writing time series belonging to a specific period of time.\nThe current schema recommendation is the v10 schema (v11 is still experimental). For more information about the schema, please check out the Schema documentation.\nBlocks storage (experimental) The blocks storage is based on Prometheus TSDB: it stores each tenant\u0026rsquo;s time series into their own TSDB which write out their series to a on-disk Block (defaults to 2h block range periods). Each Block is composed by few files storing the chunks and the block index.\nThe TSDB chunk files contain the samples for multiple series. The series inside the Chunks are then indexed by a per-block index, which indexes metric names and labels to time series in the chunk files.\nThe blocks storage doesn\u0026rsquo;t require a dedicated storage backend for the index. The only requirement is an object store for the Block files, which can be:\n Amazon S3 Google Cloud Storage For more information, please check out the Blocks storage documentation.\nServices Cortex has a service-based architecture, in which the overall system is split up into a variety of components that perform a specific task. These components run separately and in parallel. Cortex can alternatively run in a single process mode, where all components are executed within a single process. The single process mode is particularly handy for local testing and development.\nCortex is, for the most part, a shared-nothing system. Each layer of the system can run multiple instances of each component and they don\u0026rsquo;t coordinate or communicate with each other within that layer.\nThe Cortex services are:\n Distributor Ingester Querier Query frontend (optional) Ruler (optional) Alertmanager (optional) Distributor The distributor service is responsible for handling incoming samples from Prometheus. It\u0026rsquo;s the first stop in the write path for series samples. Once the distributor receives samples from Prometheus, each sample is validated for correctness and to ensure that it is within the configured tenant limits, falling back to default ones in case limits have not been overridden for the specific tenant. Valid samples are then split into batches and sent to multiple ingesters in parallel.\nThe validation done by the distributor includes:\n The metric labels name are formally correct The configured max number of labels per metric is respected The configured max length of a label name and value is respected The timestamp is not older/newer than the configured min/max time range Distributors are stateless and can be scaled up and down as needed.\nHigh Availability Tracker The distributor features a High Availability (HA) Tracker. When enabled, the distributor deduplicates incoming samples from redundant Prometheus servers. This allows you to have multiple HA replicas of the same Prometheus servers, writing the same series to Cortex and then deduplicate these series in the Cortex distributor.\nThe HA Tracker deduplicates incoming samples based on a cluster and replica label. The cluster label uniquely identifies the cluster of redundant Prometheus servers for a given tenant, while the replica label uniquely identifies the replica within the Prometheus cluster. Incoming samples are considered duplicated (and thus dropped) if received by any replica which is not the current primary within a cluster.\nThe HA Tracker requires a key-value (KV) store to coordinate which replica is currently elected. The distributor will only accept samples from the current leader. Samples with one or no labels (of the replica and cluster) are accepted by default and never deduplicated.\nThe supported KV stores for the HA tracker are:\n Consul Etcd For more information, please refer to config for sending HA pairs data to Cortex in the documentation.\nHashing Distributors use consistent hashing, in conjunction with a configurable replication factor, to determine which ingester instance(s) should receive a given series.\nCortex supports two hashing strategies:\n Hash the metric name and tenant ID (default) Hash the metric name, labels and tenant ID (enabled with -distributor.shard-by-all-labels=true) The trade-off associated with the latter is that writes are more balanced across ingesters but each query needs to talk to any ingester since a metric could be spread across multiple ingesters given different label sets.\nThe hash ring A hash ring (stored in a key-value store) is used to achieve consistent hashing for the series sharding and replication across the ingesters. All ingesters register themselves into the hash ring with a set of tokens they own; each token is a random unsigned 32-bit number. Each incoming series is hashed in the distributor and then pushed to the ingester owning the tokens range for the series hash number plus N-1 subsequent ingesters in the ring, where N is the replication factor.\nTo do the hash lookup, distributors find the smallest appropriate token whose value is larger than the hash of the series. When the replication factor is larger than 1, the next subsequent tokens (clockwise in the ring) that belong to different ingesters will also be included in the result.\nThe effect of this hash set up is that each token that an ingester owns is responsible for a range of hashes. If there are three tokens with values 0, 25, and 50, then a hash of 3 would be given to the ingester that owns the token 25; the ingester owning token 25 is responsible for the hash range of 1-25.\nThe supported KV stores for the hash ring are:\n Consul Etcd Gossip memberlist (experimental) Quorum consistency Since all distributors share access to the same hash ring, write requests can be sent to any distributor and you can setup a stateless load balancer in front of it.\nTo ensure consistent query results, Cortex uses Dynamo-style quorum consistency on reads and writes. This means that the distributor will wait for a positive response of at least one half plus one of the ingesters to send the sample to before successfully responding to the Prometheus write request.\nLoad balancing across distributors We recommend randomly load balancing write requests across distributor instances. For example, if you\u0026rsquo;re running Cortex in a Kubernetes cluster, you could run the distributors as a Kubernetes Service.\nIngester The ingester service is responsible for writing incoming series to a long-term storage backend on the write path and returning in-memory series samples for queries on the read path.\nIncoming series are not immediately written to the storage but kept in memory and periodically flushed to the storage (by default, 12 hours for the chunks storage and 2 hours for the experimental blocks storage). For this reason, the queriers may need to fetch samples both from ingesters and long-term storage while executing a query on the read path.\nIngesters contain a lifecycler which manages the lifecycle of an ingester and stores the ingester state in the hash ring. Each ingester could be in one of the following states:\n PENDING is an ingester\u0026rsquo;s state when it just started and is waiting for a hand-over from another ingester that is LEAVING. If no hand-over occurs within the configured timeout period (\u0026ldquo;auto-join timeout\u0026rdquo;, configurable via -ingester.join-after option), the ingester will join the ring with a new set of random tokens (ie. during a scale up). When hand-over process starts, state changes to JOINING.\n JOINING is an ingester\u0026rsquo;s state in two situations. First, ingester will switch to a JOINING state from PENDING state after auto-join timeout. In this case, ingester will generate tokens, store them into the ring, optionally observe the ring for token conflicts and then move to ACTIVE state. Second, ingester will also switch into a JOINING state as a result of another LEAVING ingester initiating a hand-over process with PENDING (which then switches to JOINING state). JOINING ingester then receives series and tokens from LEAVING ingester, and if everything goes well, JOINING ingester switches to ACTIVE state. If hand-over process fails, JOINING ingester will move back to PENDING state and either wait for another hand-over or auto-join timeout.\n ACTIVE is an ingester\u0026rsquo;s state when it is fully initialized. It may receive both write and read requests for tokens it owns.\n LEAVING is an ingester\u0026rsquo;s state when it is shutting down. It cannot receive write requests anymore, while it could still receive read requests for series it has in memory. While in this state, the ingester may look for a PENDING ingester to start a hand-over process with, used to transfer the state from LEAVING ingester to the PENDING one, during a rolling update (PENDING ingester moves to JOINING state during hand-over process). If there is no new ingester to accept hand-over, ingester in LEAVING state will flush data to storage instead.\n UNHEALTHY is an ingester\u0026rsquo;s state when it has failed to heartbeat to the ring\u0026rsquo;s KV Store. While in this state, distributors skip the ingester while building the replication set for incoming series and the ingester does not receive write or read requests.\n For more information about the hand-over process, please check out the Ingester hand-over documentation.\nIngesters are semi-stateful.\nIngesters failure and data loss If an ingester process crashes or exits abruptly, all the in-memory series that have not yet been flushed to the long-term storage will be lost. There are two main ways to mitigate this failure mode:\n Replication Write-ahead log (WAL) The replication is used to hold multiple (typically 3) replicas of each time series in the ingesters. If the Cortex cluster looses an ingester, the in-memory series hold by the lost ingester are also replicated at least to another ingester. In the event of a single ingester failure, no time series samples will be lost while, in the event of multiple ingesters failure, time series may be potentially lost if failure affects all the ingesters holding the replicas of a specific time series.\nThe write-ahead log (WAL) is used to write to a persistent local disk all incoming series samples until they\u0026rsquo;re flushed to the long-term storage. In the event of an ingester failure, a subsequent process restart will replay the WAL and recover the in-memory series samples.\nContrary to the sole replication and given the persistent local disk data is not lost, in the event of multiple ingesters failure each ingester will recover the in-memory series samples from WAL upon subsequent restart. The replication is still recommended in order to ensure no temporary failures on the read path in the event of a single ingester failure.\nThe WAL for the chunks storage is an experimental feature (disabled by default), while it\u0026rsquo;s always enabled for the blocks storage.\nIngesters write de-amplification Ingesters store recently received samples in-memory in order to perform write de-amplification. If the ingesters would immediately write received samples to the long-term storage, the system would be very difficult to scale due to the very high pressure on the storage. For this reason, the ingesters batch and compress samples in-memory and periodically flush them out to the storage.\nWrite de-amplification is the main source of Cortex\u0026rsquo;s low total cost of ownership (TCO).\nQuerier The querier service handles queries using the PromQL query language.\nQueriers fetch series samples both from the ingesters and long-term storage: the ingesters hold the in-memory series which have not yet been flushed to the long-term storage. Because of the replication factor, it is possible that the querier may receive duplicated samples; to resolve this, for a given time series the querier internally deduplicates samples with the same exact timestamp.\nQueriers are stateless and can be scaled up and down as needed.\nQuery frontend The query frontend is an optional service providing the querier\u0026rsquo;s API endpoints and can be used to accelerate the read path. When the query frontend is in place, incoming query requests should be directed to the query frontend instead of the queriers. The querier service will be still required within the cluster, in order to execute the actual queries.\nThe query frontend internally performs some query adjustments and holds queries in an internal queue. In this setup, queriers act as workers which pull jobs from the queue, execute them, and return them to the query-frontend for aggregation. Queriers need to be configured with the query frontend address (via the -querier.frontend-address CLI flag) in order to allow them to connect to the query frontends.\nQuery frontends are stateless. However, due to how the internal queue works, it\u0026rsquo;s recommended to run a few query frontend replicas to reap the benefit of fair scheduling. Two replicas should suffice in most cases.\nQueueing The query frontend queuing mechanism is used to:\n Ensure that large queries, that could cause an out-of-memory (OOM) error in the querier, will be retried on failure. This allows administrators to under-provision memory for queries, or optimistically run more small queries in parallel, which helps to reduce the TCO. Prevent multiple large requests from being convoyed on a single querier by distributing them across all queriers using a first-in/first-out queue (FIFO). Prevent a single tenant from denial-of-service-ing (DOSing) other tenants by fairly scheduling queries between tenants. Splitting The query frontend splits multi-day queries into multiple single-day queries, executing these queries in parallel on downstream queriers and stitching the results back together again. This prevents large (multi-day) queries from causing out of memory issues in a single querier and helps to execute them faster.\nCaching The query frontend supports caching query results and reuses them on subsequent queries. If the cached results are incomplete, the query frontend calculates the required subqueries and executes them in parallel on downstream queriers. The query frontend can optionally align queries with their step parameter to improve the cacheability of the query results. The result cache is compatible with any cortex caching backend (currently memcached, redis, and an in-memory cache).\nRuler The ruler is an optional service executing PromQL queries for recording rules and alerts. The ruler requires a database storing the recording rules and alerts for each tenant.\nRuler can be scaled horizontally.\nAlertmanager The alertmanager is an optional service responsible for accepting alert notifications from the ruler, deduplicating and grouping them, and routing them to the correct notification channel, such as email, PagerDuty or OpsGenie.\nThe Cortex alertmanager is built on top of the Prometheus Alertmanager, adding multi-tenancy support. Like the ruler, the alertmanager requires a database storing the per-tenant configuration.\n","excerpt":"Cortex consists of multiple horizontally scalable microservices. Each microservice uses the most …","ref":"/docs/architecture/","title":"Cortex Architecture"},{"body":" Context You can have more than a single Prometheus monitoring and ingesting the same metrics for redundancy. Cortex already does replication for redundancy and it doesn\u0026rsquo;t make sense to ingest the same data twice. So in Cortex, we made sure we can dedupe the data we receive from HA Pairs of Prometheus. We do this via the following:\nAssume that there are two teams, each running their own Prometheus, monitoring different services. Let\u0026rsquo;s call the Prometheis T1 and T2. Now, if the teams are running HA pairs, let\u0026rsquo;s call the individual Prometheis, T1.a, T1.b and T2.a and T2.b.\nIn Cortex we make sure we only ingest from one of T1.a and T1.b, and only from one of T2.a and T2.b. We do this by electing a leader replica for each cluster of Prometheus. For example, in the case of T1, let it be T1.a. As long as T1.a is the leader, we drop the samples sent by T1.b. And if Cortex sees no new samples from T1.a for a short period (30s by default), it\u0026rsquo;ll switch the leader to be T1.b.\nThis means if T1.a goes down for a few minutes Cortex\u0026rsquo;s HA sample handling will have switched and elected T1.b as the leader. This failover timeout is what enables us to only accept samples from a single replica at a time, but ensure we don\u0026rsquo;t drop too much data in case of issues. Note that with the default scrape period of 15s, and the default timeouts in Cortex, in most cases you\u0026rsquo;ll only lose a single scrape of data in the case of a leader election failover. For any rate queries the rate window should be at least 4x the scrape period to account for any of these failover scenarios, for example with the default scrape period of 15s then you should calculate rates over at least 1m periods.\nNow we do the same leader election process T2.\nConfig Client Side So for Cortex to achieve this, we need 2 identifiers for each process, one identifier for the cluster (T1 or T2, etc) and one identifier to identify the replica in the cluster (a or b). The easiest way to do with is by setting external labels, ideally cluster and replica (note the default is __replica__). For example:\ncluster: prom-team1 replica: replica1 (or pod-name) and\ncluster: prom-team1 replica: replica2 Note: These are external labels and have nothing to do with remote_write config.\nThese two label names are configurable per-tenant within Cortex, and should be set to something sensible. For example, cluster label is already used by some workloads, and you should set the label to be something else but uniquely identifies the cluster. Good examples for this label-name would be team, cluster, prometheus, etc.\nThe replica label should be set so that the value for each prometheus is unique in that cluster. Note: Cortex drops this label when ingesting data, but preserves the cluster label. This way, your timeseries won\u0026rsquo;t change when replicas change.\nServer Side To enable handling of samples, see the distributor flags having ha-tracker in them.\n","excerpt":"Context You can have more than a single Prometheus monitoring and ingesting the same metrics for …","ref":"/docs/guides/ha-pair-handling/","title":"Config for sending HA Pairs data to Cortex"},{"body":"Configuration for running Cortex in single-process mode. This should not be used in production. It is only for getting started and development.\n# Disable the requirement that every request to Cortex has a# X-Scope-OrgID header. `fake` will be substituted in instead.auth_enabled:falseserver:http_listen_port:9009# Configure the server to allow messages up to 100MB.grpc_server_max_recv_msg_size:104857600grpc_server_max_send_msg_size:104857600grpc_server_max_concurrent_streams:1000distributor:shard_by_all_labels:truepool:health_check_ingesters:trueingester_client:grpc_client_config:# Configure the client to allow messages up to 100MB.max_recv_msg_size:104857600max_send_msg_size:104857600use_gzip_compression:trueingester:#chunk_idle_period: 15mlifecycler:# The address to advertise for this ingester. Will be autodiscovered by# looking up address on eth0 or en0; can be specified if this fails.# address: 127.0.0.1# We want to start immediately and flush on shutdown.join_after:0claim_on_rollout:falsefinal_sleep:0snum_tokens:512# Use an in memory ring store, so we don\u0026#39;t need to launch a Consul.ring:kvstore:store:inmemoryreplication_factor:1# Use local storage - BoltDB for the index, and the filesystem# for the chunks.schema:configs:-from:2019-07-29store:boltdbobject_store:filesystemschema:v10index:prefix:index_period:168hstorage:boltdb:directory:/tmp/cortex/indexfilesystem:directory:/tmp/cortex/chunks","excerpt":"Configuration for running Cortex in single-process mode. This should not be used in production. It …","ref":"/docs/configuration/single-process-config/","title":"Single-process"},{"body":" [this is a work in progress]\nRemote API Cortex supports Prometheus\u0026rsquo; remote_read and remote_write APIs. The encoding is Protobuf over http.\nRead is on /api/prom/read and write is on /api/prom/push.\nConfigs API The configs service provides an API-driven multi-tenant approach to handling various configuration files for prometheus. The service hosts an API where users can read and write Prometheus rule files, Alertmanager configuration files, and Alertmanager templates to a database.\nEach tenant will have it\u0026rsquo;s own set of rule files, Alertmanager config, and templates. A POST operation will effectively replace the existing copy with the configs provided in the request body.\nConfigs Format At the current time of writing, the API is part-way through a migration from a single Configs service that handled all three sets of data to a split API (Tracking issue). All APIs take and return all sets of data.\nThe following schema is used both when retrieving the current configs from the API and when setting new configs via the API.\nSchema: { \u0026#34;id\u0026#34;: 99, \u0026#34;rule_format_version\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;alertmanager_config\u0026#34;: \u0026#34;\u0026lt;standard alertmanager.yaml config\u0026gt;\u0026#34;, \u0026#34;rules_files\u0026#34;: { \u0026#34;rules.yaml\u0026#34;: \u0026#34;\u0026lt;standard rules.yaml config\u0026gt;\u0026#34;, \u0026#34;rules2.yaml\u0026#34;: \u0026#34;\u0026lt;standard rules.yaml config\u0026gt;\u0026#34; }, \u0026#34;template_files\u0026#34;: { \u0026#34;templates.tmpl\u0026#34;: \u0026#34;\u0026lt;standard template file\u0026gt;\u0026#34;, \u0026#34;templates2.tmpl\u0026#34;: \u0026#34;\u0026lt;standard template file\u0026gt;\u0026#34; } } } Formatting id - should be incremented every time data is updated; Cortex will use the config with the highest number.\nrule_format_version - allows compatibility for tenants with config in Prometheus V1 format. Pass \u0026ldquo;1\u0026rdquo; or \u0026ldquo;2\u0026rdquo; according to which Prometheus version you want to match.\nconfig.alertmanager_config - The contents of the alertmanager config file should be as described here, encoded as a single string to fit within the overall JSON payload.\nconfig.rules_files - The contents of a rules file should be as described here, encoded as a single string to fit within the overall JSON payload.\nconfig.template_files - The contents of a template file should be as described here, encoded as a single string to fit within the overall JSON payload.\nEndpoints Manage Alertmanager GET /api/prom/configs/alertmanager - Get current Alertmanager config\n Normal Response Codes: OK(200) Error Response Codes: Unauthorized(401), NotFound(404) POST /api/prom/configs/alertmanager - Replace current Alertmanager config\n Normal Response Codes: NoContent(204) Error Response Codes: Unauthorized(401), BadRequest(400) POST /api/prom/configs/alertmanager/validate - Validate Alertmanager config\nNormal Response: OK(200)\n{ \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } Error Response: BadRequest(400)\n{ \u0026#34;status\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;error\u0026#34;: \u0026#34;error message\u0026#34; } Manage Rules GET /api/prom/configs/rules - Get current rule files\n Normal Response Codes: OK(200) Error Response Codes: Unauthorized(400), NotFound(404) POST /api/prom/configs/rules - Replace current rule files\n Normal Response Codes: NoContent(204) Error Response Codes: Unauthorized(401), BadRequest(400) Manage Templates GET /api/prom/configs/templates - Get current templates\n Normal Response Codes: OK(200) Error Response Codes: Unauthorized(401), NotFound(404) POST /api/prom/configs/templates - Replace current templates\n Normal Response Codes: NoContent(204) Error Response Codes: Unauthorized(401), BadRequest(400) Deactivate/Restore Configs DELETE /api/prom/configs/deactivate - Disable configs for a tenant\n Normal Response Codes: OK(200) Error Response Codes: Unauthorized(401), NotFound(404) POST /api/prom/configs/restore - Re-enable configs for a tenant\n Normal Response Codes OK(200) Error Response Codes: Unauthorized(401), NotFound(404) These API endpoints will disable/enable the current Rule and Alertmanager configuration for a tenant.\nNote that setting a new config will effectively \u0026ldquo;re-enable\u0026rdquo; the Rules and Alertmanager configuration for a tenant.\nIngester Shutdown POST /shutdown - Shutdown all operations of an ingester. Shutdown operations performed are similar to when an ingester is gracefully shutting down, including flushing of chunks if no other ingester is in PENDING state. Ingester does not terminate after calling this endpoint.\n Normal Response Codes: NoContent(204) Error Response Codes: Unauthorized(401) ","excerpt":"[this is a work in progress]\nRemote API Cortex supports Prometheus\u0026rsquo; remote_read and …","ref":"/docs/apis/","title":"Cortex APIs"},{"body":"The ingester holds several hours of sample data in memory. When we want to shut down an ingester, either for software version update or to drain a node for maintenance, this data must not be discarded.\nEach ingester goes through different states in its lifecycle. When working normally, the state is ACTIVE.\nOn start-up, an ingester first goes into state PENDING. After a short time, if nothing happens, it adds itself to the ring and goes into state ACTIVE.\nA running ingester is notified to shut down by Unix signal SIGINT. On receipt of this signal it goes into state LEAVING and looks for an ingester in state PENDING. If it finds one, that ingester goes into state JOINING and the leaver transfers all its in-memory data over to the joiner. On successful transfer the leaver removes itself from the ring and exits and the joiner changes to ACTIVE, taking over ownership of the leaver\u0026rsquo;s ring tokens.\nIf a leaving ingester does not find a pending ingester after several attempts, it will flush all of its chunks to the backing database, then remove itself from the ring and exit. This may take tens of minutes to complete.\nDuring hand-over, neither the leaving nor joining ingesters will accept new samples. Distributors are aware of this, and \u0026ldquo;spill\u0026rdquo; the samples to the next ingester in the ring. This creates a set of extra \u0026ldquo;spilled\u0026rdquo; chunks which will idle out and flush after hand-over is complete. The sudden increase in flush queue can be alarming!\nThe following metrics can be used to observe this process:\n cortex_member_ring_tokens_owned - how many tokens each ingester thinks it owns cortex_ring_tokens_owned - how many tokens each ingester is seen to own by other components cortex_ring_member_ownership_percent same as cortex_ring_tokens_owned but expressed as a percentage cortex_ring_members - how many ingesters can be seen in each state, by other components cortex_ingester_sent_chunks - number of chunks sent by leaving ingester cortex_ingester_received_chunks - number of chunks received by joining ingester You can see the current state of the ring via http browser request to /ring on a distributor.\n","excerpt":"The ingester holds several hours of sample data in memory. When we want to shut down an ingester, …","ref":"/docs/guides/ingester-handover/","title":"Ingester Hand-over"},{"body":"","excerpt":"","ref":"/docs/guides/","title":"Guides"},{"body":" Cortex uses Jaeger to implement distributed tracing. We have found Jaeger invaluable for troubleshooting the behavior of Cortex in production.\nDependencies In order to send traces you will need to set up a Jaeger deployment. A deployment includes either the jaeger all-in-one binary, or else a distributed system of agents, collectors, and queriers. If running on Kubernetes, Jaeger Kubernetes is an excellent resource.\nConfiguration In order to configure Cortex to send traces you must do two things: 1. Set the JAEGER_AGENT_HOST environment variable in all components to point to your Jaeger agent. This defaults to localhost. 1. Enable sampling in the appropriate components: * The Ingester and Ruler self-initiate traces and should have sampling explicitly enabled. * Sampling for the Distributor and Query Frontend can be enabled in Cortex or in an upstream service such as your frontdoor.\nTo enable sampling in Cortex components you can specify either JAEGER_SAMPLER_MANAGER_HOST_PORT for remote sampling, or JAEGER_SAMPLER_TYPE and JAEGER_SAMPLER_PARAM to manually set sampling configuration. See the Jaeger Client Go documentation for the full list of environment variables you can configure.\nNote that you must specify one of JAEGER_AGENT_HOST or JAEGER_SAMPLER_MANAGER_HOST_PORT in each component for Jaeger to be enabled, even if you plan to use the default values.\n","excerpt":"Cortex uses Jaeger to implement distributed tracing. We have found Jaeger invaluable for …","ref":"/docs/guides/tracing/","title":"Tracing"},{"body":"","excerpt":"","ref":"/docs/configuration/","title":"Configuration"},{"body":"","excerpt":"","ref":"/docs/operations/","title":"Operating Cortex"},{"body":" This guide covers how to run a single local Cortex instance - with the chunks storage engine - storing time series chunks and index in Cassandra.\nIn this guide we\u0026rsquo;re going to:\n Setup a locally running Cassandra Configure Cortex to store chunks and index on Cassandra Configure Prometheus to send series to Cortex Configure Grafana to visualise metrics Setup a locally running Cassandra Run Cassandra with the following command:\ndocker run -d --name cassandra --rm -p 9042:9042 cassandra:3.11 Use Docker to execute the Cassandra Query Language (CQL) shell in the container:\ndocker exec -it \u0026lt;container_id\u0026gt; cqlsh Create a new Cassandra keyspace for Cortex metrics:\nA keyspace is an object that is used to hold column families, user defined types. A keyspace is like RDBMS database which contains column families, indexes, user defined types.\nCREATE KEYSPACE cortex WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1}; Configure Cortex to store chunks and index on Cassandra Now, we have to configure Cortex to store the chunks and index in Cassandra. Create a config file called single-process-config.yaml, then add the content below. Make sure to replace the following placeholders: - LOCALHOST: Addresses of your Cassandra instance. This can accept multiple addresses by passing them as comma separated values. - KEYSPACE: The name of the Cassandra keyspace used to store the metrics.\nsingle-process-config.yaml\n# Configuration for running Cortex in single-process mode. # This should not be used in production. It is only for getting started # and development. # Disable the requirement that every request to Cortex has a # X-Scope-OrgID header. `fake` will be substituted in instead. auth_enabled: false server: http_listen_port: 9009 # Configure the server to allow messages up to 100MB. grpc_server_max_recv_msg_size: 104857600 grpc_server_max_send_msg_size: 104857600 grpc_server_max_concurrent_streams: 1000 distributor: shard_by_all_labels: true pool: health_check_ingesters: true ingester_client: grpc_client_config: # Configure the client to allow messages up to 100MB. max_recv_msg_size: 104857600 max_send_msg_size: 104857600 use_gzip_compression: true ingester: lifecycler: # The address to advertise for this ingester. Will be autodiscovered by # looking up address on eth0 or en0; can be specified if this fails. address: 127.0.0.1 # We want to start immediately and flush on shutdown. join_after: 0 claim_on_rollout: false final_sleep: 0s num_tokens: 512 # Use an in memory ring store, so we don't need to launch a Consul. ring: kvstore: store: inmemory replication_factor: 1 # Use cassandra as storage -for both index store and chunks store. schema: configs: - from: 2019-07-29 store: cassandra object_store: cassandra schema: v10 index: prefix: index_ period: 168h storage: cassandra: addresses: LOCALHOST # configure cassandra addresses here. keyspace: KEYSPACE # configure desired keyspace here. Run Cortex using the latest stable version:\ndocker run -d --name=cortex -v $(pwd)/single-process-config.yaml:/etc/single-process-config.yaml -p 9009:9009 quay.io/cortexproject/cortex -config.file=/etc/single-process-config.yaml In case you prefer to run the master version, please follow this documentation on how to build Cortex from source.\nConfigure Prometheus to send series to Cortex Now that Cortex is up, it should be running on http://localhost:9009.\nAdd the following section to your Prometheus configuration file. This will configure the remote write to send metrics to Cortex.\nremote_write: - url: http://localhost:9009/api/prom/push Configure Grafana to visualise metrics Run grafana to visualise metrics from Cortex:\ndocker run -d --name=grafana -p 3000:3000 grafana/grafana Add a data source in Grafana by selecting Prometheus as the data source type and use the Cortex URL to query metrics: http://localhost:9009/api/prom.\nFinally, You can monitor Cortex\u0026rsquo;s reads \u0026amp; writes by creating the dashboard. You can follow this documentation to do so.\n","excerpt":"This guide covers how to run a single local Cortex instance - with the chunks storage engine - …","ref":"/docs/guides/cassandra/","title":"Running Cortex with Cassandra"},{"body":"","excerpt":"","ref":"/index.json","title":""},{"body":" master / unreleased [ENHANCEMENT] metric cortex_ingester_flush_reasons gets a new reason value: Spread, when -ingester.spread-flushes option is enabled.\n [CHANGE] Flags changed with transition to upstream Prometheus rules manager:\n ruler.client-timeout is now ruler.configs.client-timeout in order to match ruler.configs.url ruler.group-timeouthas been removed ruler.num-workers has been removed ruler.rule-path has been added to specify where the prometheus rule manager will sync rule files ruler.storage.type has beem added to specify the rule store backend type, currently only the configdb. ruler.poll-interval has been added to specify the interval in which to poll new rule groups. [CHANGE] Use relative links from /ring page to make it work when used behind reverse proxy. #1896\n [CHANGE] Deprecated -distributor.limiter-reload-period flag. #1766\n [CHANGE] Ingesters now write only normalised tokens to the ring, although they can still read denormalised tokens used by other ingesters. -ingester.normalise-tokens is now deprecated, and ignored. If you want to switch back to using denormalised tokens, you need to downgrade to Cortex 0.4.0. Previous versions don\u0026rsquo;t handle claiming tokens from normalised ingesters correctly. #1809\n [CHANGE] Overrides mechanism has been renamed to \u0026ldquo;runtime config\u0026rdquo;, and is now separate from limits. Runtime config is simply a file that is reloaded by Cortex every couple of seconds. Limits and now also multi KV use this mechanism.\nNew arguments were introduced: -runtime-config.file (defaults to empty) and -runtime-config.reload-period (defaults to 10 seconds), which replace previously used -limits.per-user-override-config and -limits.per-user-override-period options. Old options are still used if -runtime-config.file is not specified. This change is also reflected in YAML configuration, where old limits.per_tenant_override_config and limits.per_tenant_override_period fields are replaced with runtime_config.file and runtime_config.period respectively. #1749\n [CHANGE] Changed the default value for -distributor.ha-tracker.prefix from collectors/ to ha-tracker/ in order to not clash with other keys (ie. ring) stored in the same key-value store. #1940\n [FEATURE] The distributor can now drop labels from samples (similar to the removal of the replica label for HA ingestion) per user via the distributor.drop-label flag. #1726\n [FEATURE] Added flag debug.mutex-profile-fraction to enable mutex profiling #1969\n [FEATURE] Added global ingestion rate limiter strategy. Deprecated -distributor.limiter-reload-period flag. #1766\n [FEATURE] Added support for Microsoft Azure blob storage to be used for storing chunk data. #1913\n [FEATURE] Added readiness probe endpoint/ready to queriers. #1934\n [FEATURE] EXPERIMENTAL: Added /series API endpoint support with TSDB blocks storage. #1830\n [FEATURE] Added \u0026ldquo;multi\u0026rdquo; KV store that can interact with two other KV stores, primary one for all reads and writes, and secondary one, which only receives writes. Primary/secondary store can be modified in runtime via runtime-config mechanism (previously \u0026ldquo;overrides\u0026rdquo;). #1749\n [ENHANCEMENT] Added password and enable_tls options to redis cache configuration. Enables usage of Microsoft Azure Cache for Redis service.\n [ENHANCEMENT] Experimental TSDB: Open existing TSDB on startup to prevent ingester from becoming ready before it can accept writes. #1917\n --experimental.tsdb.max-tsdb-opening-concurrency-on-startup [BUGFIX] Fixed unnecessary CAS operations done by the HA tracker when the jitter is enabled. #1861\n [BUGFIX] Fixed #1904 ingesters getting stuck in a LEAVING state after coming up from an ungraceful exit. #1921\n [BUGFIX] TSDB: Fixed handling of out of order/bound samples in ingesters with the experimental TSDB blocks storage. #1864\n [BUGFIX] TSDB: Fixed querying ingesters in LEAVING state with the experimental TSDB blocks storage. #1854\n [BUGFIX] TSDB: Fixed error handling in the series to chunks conversion with the experimental TSDB blocks storage. #1837\n [BUGFIX] TSDB: Fixed TSDB creation conflict with blocks transfer in a JOINING ingester with the experimental TSDB blocks storage. #1818\n [BUGFIX] TSDB: experimental.tsdb.ship-interval of \u0026lt;=0 treated as disabled instead of allowing panic. #1975\n [BUGFIX] TSDB: Fixed cortex_ingester_queried_samples and cortex_ingester_queried_series metrics when using block storage. #1981\n [BUGFIX] TSDB: Fixed cortex_ingester_memory_series and cortex_ingester_memory_users metrics when using with the experimental TSDB blocks storage. #1982\n 0.4.0 / 2019-12-02 [CHANGE] The frontend component has been refactored to be easier to re-use. When upgrading the frontend, cache entries will be discarded and re-created with the new protobuf schema. #1734 [CHANGE] Removed direct DB/API access from the ruler. -ruler.configs.url has been now deprecated. #1579 [CHANGE] Removed Delta encoding. Any old chunks with Delta encoding cannot be read anymore. If ingester.chunk-encoding is set to Delta the ingester will fail to start. #1706 [CHANGE] Setting -ingester.max-transfer-retries to 0 now disables hand-over when ingester is shutting down. Previously, zero meant infinite number of attempts. #1771 [CHANGE] dynamo has been removed as a valid storage name to make it consistent for all components. aws and aws-dynamo remain as valid storage names. [CHANGE/FEATURE] The frontend split and cache intervals can now be configured using the respective flag --querier.split-queries-by-interval and --frontend.cache-split-interval. If --querier.split-queries-by-interval is not provided request splitting is disabled by default. --querier.split-queries-by-day is still accepted for backward compatibility but has been deprecated. You should now use --querier.split-queries-by-interval. We recommend a to use a multiple of 24 hours. [FEATURE] Global limit on the max series per user and metric #1760 -ingester.max-global-series-per-user -ingester.max-global-series-per-metric Requires -distributor.replication-factor and -distributor.shard-by-all-labels set for the ingesters too [FEATURE] Flush chunks with stale markers early with ingester.max-stale-chunk-idle. #1759 [FEATURE] EXPERIMENTAL: Added new KV Store backend based on memberlist library. Components can gossip about tokens and ingester states, instead of using Consul or Etcd. #1721 [FEATURE] EXPERIMENTAL: Use TSDB in the ingesters \u0026amp; flush blocks to S3/GCS ala Thanos. This will let us use an Object Store more efficiently and reduce costs. #1695 [FEATURE] Allow Query Frontend to log slow queries with frontend.log-queries-longer-than. #1744 [FEATURE] Add HTTP handler to trigger ingester flush \u0026amp; shutdown - used when running as a stateful set with the WAL enabled. #1746 [FEATURE] EXPERIMENTAL: Added GCS support to TSDB blocks storage. #1772 [ENHANCEMENT] Reduce memory allocations in the write path. #1706 [ENHANCEMENT] Consul client now follows recommended practices for blocking queries wrt returned Index value. #1708 [ENHANCEMENT] Consul client can optionally rate-limit itself during Watch (used e.g. by ring watchers) and WatchPrefix (used by HA feature) operations. Rate limiting is disabled by default. New flags added: --consul.watch-rate-limit, and --consul.watch-burst-size. #1708 [ENHANCEMENT] Added jitter to HA deduping heartbeats, configure using distributor.ha-tracker.update-timeout-jitter-max #1534 [ENHANCEMENT] Add ability to flush chunks with stale markers early. #1759 [BUGFIX] Stop reporting successful actions as 500 errors in KV store metrics. #1798 [BUGFIX] Fix bug where duplicate labels can be returned through metadata APIs. #1790 [BUGFIX] Fix reading of old, v3 chunk data. #1779 [BUGFIX] Now support IAM roles in service accounts in AWS EKS. #1803 [BUGFIX] Fixed duplicated series returned when querying both ingesters and store with the experimental TSDB blocks storage. #1778 In this release we updated the following dependencies: - gRPC v1.25.0 (resulted in a drop of 30% CPU usage when compression is on) - jaeger-client v2.20.0 - aws-sdk-go to v1.25.22\n0.3.0 / 2019-10-11 This release adds support for Redis as an alternative to Memcached, and also includes many optimisations which reduce CPU and memory usage.\n [CHANGE] Gauge metrics were renamed to drop the _total suffix. #1685 In Alertmanager, alertmanager_configs_total is now alertmanager_configs In Ruler, scheduler_configs_total is now scheduler_configs scheduler_groups_total is now scheduler_groups. [CHANGE] --alertmanager.configs.auto-slack-root flag was dropped as auto Slack root is not supported anymore. #1597 [CHANGE] In table-manager, default DynamoDB capacity was reduced from 3,000 units to 1,000 units. We recommend you do not run with the defaults: find out what figures are needed for your environment and set that via -dynamodb.periodic-table.write-throughput and -dynamodb.chunk-table.write-throughput. [FEATURE] Add Redis support for caching #1612 [FEATURE] Allow spreading chunk writes across multiple S3 buckets #1625 [FEATURE] Added /shutdown endpoint for ingester to shutdown all operations of the ingester. #1746 [ENHANCEMENT] Upgraded Prometheus to 2.12.0 and Alertmanager to 0.19.0. #1597 [ENHANCEMENT] Cortex is now built with Go 1.13 #1675, #1676, #1679 [ENHANCEMENT] Many optimisations, mostly impacting ingester and querier: #1574, #1624, #1638, #1644, #1649, #1654, #1702 Full list of changes: https://github.com/cortexproject/cortex/compare/v0.2.0...v0.3.0\n0.2.0 / 2019-09-05 This release has several exciting features, the most notable of them being setting -ingester.spread-flushes to potentially reduce your storage space by upto 50%.\n [CHANGE] Flags changed due to changes upstream in Prometheus Alertmanager #929: alertmanager.mesh.listen-address is now cluster.listen-address alertmanager.mesh.peer.host and alertmanager.mesh.peer.service can be replaced by cluster.peer alertmanager.mesh.hardware-address, alertmanager.mesh.nickname, alertmanager.mesh.password, and alertmanager.mesh.peer.refresh-interval all disappear. [CHANGE] \u0026ndash;claim-on-rollout flag deprecated; feature is now always on #1566 [CHANGE] Retention period must now be a multiple of periodic table duration #1564 [CHANGE] The value for the name label for the chunks memcache in all cortex_cache_ metrics is now chunksmemcache (before it was memcache) #1569 [FEATURE] Makes the ingester flush each timeseries at a specific point in the max-chunk-age cycle with -ingester.spread-flushes. This means multiple replicas of a chunk are very likely to contain the same contents which cuts chunk storage space by up to 66%. #1578 [FEATURE] Make minimum number of chunk samples configurable per user #1620 [FEATURE] Honor HTTPS for custom S3 URLs #1603 [FEATURE] You can now point the query-frontend at a normal Prometheus for parallelisation and caching #1441 [FEATURE] You can now specify http_config on alert receivers #929 [FEATURE] Add option to use jump hashing to load balance requests to memcached #1554 [FEATURE] Add status page for HA tracker to distributors #1546 [FEATURE] The distributor ring page is now easier to read with alternate rows grayed out #1621 0.1.0 / 2019-08-07 [CHANGE] HA Tracker flags were renamed to provide more clarity #1465 distributor.accept-ha-labels is now distributor.ha-tracker.enable distributor.accept-ha-samples is now distributor.ha-tracker.enable-for-all-users ha-tracker.replica is now distributor.ha-tracker.replica ha-tracker.cluster is now distributor.ha-tracker.cluster [FEATURE] You can specify \u0026ldquo;heap ballast\u0026rdquo; to reduce Go GC Churn #1489 [BUGFIX] HA Tracker no longer always makes a request to Consul/Etcd when a request is not from the active replica #1516 [BUGFIX] Queries are now correctly cancelled by the query-frontend #1508 ","excerpt":"master / unreleased [ENHANCEMENT] metric cortex_ingester_flush_reasons gets a new reason value: …","ref":"/docs/changelog/","title":"Changelog"},{"body":"Cortex follows the CNCF Code of Conduct.\n","excerpt":"Cortex follows the CNCF Code of Conduct.","ref":"/docs/code-of-conduct/","title":"Code of Conduct"},{"body":" Welcome! We\u0026rsquo;re excited that you\u0026rsquo;re interested in contributing. Below are some basic guidelines.\nWorkflow Cortex follows a standard GitHub pull request workflow. If you\u0026rsquo;re unfamiliar with this workflow, read the very helpful Understanding the GitHub flow guide from GitHub.\nYou are welcome to create draft PRs at any stage of readiness - this can be helpful to ask for assistance or to develop an idea. But before a piece of work is finished it should:\n Be organised into one or more commits, each of which has a commit message that describes all changes made in that commit (\u0026lsquo;why\u0026rsquo; more than \u0026lsquo;what\u0026rsquo; - we can read the diffs to see the code that changed). Each commit should build towards the whole - don\u0026rsquo;t leave in back-tracks and mistakes that you later corrected. Have tests for new functionality or tests that would have caught the bug being fixed. Include a CHANGELOG message if users of Cortex need to hear about what you did. Developer Certificates of Origin (DCOs) Before submitting your work in a pull request, make sure that all commits are signed off with a Developer Certificate of Origin (DCO). Here\u0026rsquo;s an example:\ngit commit -s -m \u0026#34;Here is my signed commit\u0026#34; You can find further instructions here.\nBuilding Cortex To build:\nmake (By default, the build runs in a Docker container, using an image built with all the tools required. The source code is mounted from where you run make into the build container as a Docker volume.)\nTo run the test suite:\nmake test Playing in minikube First, start minikube.\nYou may need to load the Docker images into your minikube environment. There is a convenient rule in the Makefile to do this:\nmake prime-minikube Then run Cortex in minikube:\nkubectl apply -f ./k8s (these manifests use latest tags, i.e. this will work if you have just built the images and they are available on the node(s) in your Kubernetes cluster)\nCortex will sit behind an nginx instance exposed on port 30080. A job is deployed to scrape itself. Try it:\nhttp://192.168.99.100:30080/api/prom/api/v1/query?query=up\nIf that doesn\u0026rsquo;t work, your Minikube might be using a different ip address. Check with minikube status.\nDependency management We uses Go modules to manage dependencies on external packages. This requires a working Go environment with version 1.11 or greater, git and bzr installed.\nTo add or update a new dependency, use the go get command:\n# Pick the latest tagged release. go get example.com/some/module/pkg # Pick a specific version. go get example.com/some/module/pkg@vX.Y.Z Tidy up the go.mod and go.sum files:\ngo mod tidy go mod vendor git add go.mod go.sum vendor git commit You have to commit the changes to go.mod and go.sum before submitting the pull request.\n","excerpt":"Welcome! We\u0026rsquo;re excited that you\u0026rsquo;re interested in contributing. Below are some basic …","ref":"/docs/contributing/","title":"Contributing"},{"body":" Horizontally scalable, highly available, multi-tenant, long term Prometheus. Learn More Releases Companies using Cortex\n Long term storage Durably store data for longer than the lifetime of any single machine, and use this data for long term capacity planning. Blazin\u0026rsquo; fast PromQL Cortex makes your PromQL queries blazin' fast through aggressive parallelization and caching. A global view of data Cortex gives you a global view of Prometheus time series data that includes data in long-term storage, greatly expanding the usefulness of PromQL for analytical purposes. Horizontally scalable Cortex runs across multiple machines in a cluster, exceeding the throughput and storage of a single machine. This enables you to send the metrics from multiple Prometheus servers to a single Cortex cluster. We are a Cloud Native Computing Foundation Sandbox project.\n Join the community ! Join users and companies that are using Cortex in production.\n Slack Issues Twitter ","excerpt":"Horizontally scalable, highly available, multi-tenant, long term Prometheus. Learn More Releases …","ref":"/","title":"Cortex"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"}]