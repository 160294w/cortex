<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cortex – Guides</title><link>/docs/guides/</link><description>Recent content in Guides on Cortex</description><generator>Hugo -- gohugo.io</generator><atom:link href="/docs/guides/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Running Cortex chunks storage in Production</title><link>/docs/guides/running-chunks-storage-in-production/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/running-chunks-storage-in-production/</guid><description>
&lt;p>This document builds on the &lt;a href="/docs/getting-started/">getting started guide&lt;/a> and specifies the steps needed to get Cortex &lt;a href="/docs/chunks-storage/">&lt;strong>chunks storage&lt;/strong>&lt;/a> into production.
Ensure you have completed all the steps in the &lt;a href="/docs/getting-started/">getting started guide&lt;/a> and read about &lt;a href="/docs/architecture/">the Cortex architecture&lt;/a> before you start this one.&lt;/p>
&lt;h2 id="1-pick-a-storage-backend">1. Pick a storage backend&lt;/h2>
&lt;p>The getting started guide uses local chunk storage.
Local chunk storage is experimental and shouldn’t be used in production.&lt;/p>
&lt;p>Cortex requires a scalable storage back-end for production systems.
It is recommended you use chunk storage with one of the following back-ends:&lt;/p>
&lt;ul>
&lt;li>DynamoDB/S3 (see &lt;a href="/docs/chunks-storage/aws-tips/">AWS tips&lt;/a>)&lt;/li>
&lt;li>BigTable/GCS&lt;/li>
&lt;li>Cassandra (see &lt;a href="/docs/guides/running-chunks-storage-with-cassandra/">Running chunks storage on Cassandra&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>Commercial cloud options are DynamoDB/S3 and Bigtable/GCS: the advantage is you don&amp;rsquo;t have to know how to manage them, but the downside is they have specific costs.&lt;/p>
&lt;p>Alternatively you can choose Apache Cassandra, which you will have to install and manage.
Cassandra support can also be used with commecial Cassandra-compatible services such as Azure Cosmos DB.&lt;/p>
&lt;p>Cortex has an alternative to chunks storage: &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>. Blocks storage is ready for production use and does not require a separate index store.&lt;/p>
&lt;h2 id="2-deploy-query-frontend">2. Deploy Query Frontend&lt;/h2>
&lt;p>The &lt;strong>Query Frontend&lt;/strong> is the Cortex component which parallelizes the execution of and caches the results of queries.
The &lt;strong>Query Frontend&lt;/strong> is also responsible for retries and multi-tenant QoS.&lt;/p>
&lt;p>For the multi-tenant QoS algorithms to work, you should not run more than two &lt;strong>Query Frontends&lt;/strong>.
The &lt;strong>Query Frontend&lt;/strong> should be deployed behind a load balancer, and should only be sent queries &amp;ndash; writes should go straight to the Distributor component, or to the single-process Cortex.&lt;/p>
&lt;p>The &lt;strong>Querier&lt;/strong> component (or single-process Cortex) “pulls” queries from the queues in the &lt;strong>Query Frontend&lt;/strong>.
&lt;strong>Queriers&lt;/strong> discover the &lt;strong>Query Frontend&lt;/strong> via DNS.
The &lt;strong>Queriers&lt;/strong> should not use the load balancer to access the &lt;strong>Query Frontend&lt;/strong>.
In Kubernetes, you should use a separate headless service.&lt;/p>
&lt;p>To configure the &lt;strong>Queries&lt;/strong> to use the &lt;strong>Query Frontend&lt;/strong>, set the following flag:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh"> -querier.frontend-address string
Address of query frontend service.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>There are other flag you can use to control the behaviour of the frontend - concurrency, retries, etc.
See &lt;a href="/docs/configuration/arguments/#query-frontend">Query Frontend configuration&lt;/a> for more information.&lt;/p>
&lt;p>The &lt;strong>Query Frontend&lt;/strong> can run using an in-process cache, but should be configured with an external Memcached for production workloads.
The next section has more details.&lt;/p>
&lt;h2 id="3-setup-caching">3. Setup Caching&lt;/h2>
&lt;p>Correctly configured caching is important for a production-ready Cortex cluster.
Cortex has many opportunities for using caching to accelerate queries and reduce cost.&lt;/p>
&lt;p>For more information, see the &lt;a href="/docs/chunks-storage/caching/">Caching in Cortex documentation.&lt;/a>&lt;/p>
&lt;h2 id="4-monitoring-and-alerting">4. Monitoring and Alerting&lt;/h2>
&lt;p>Cortex exports metrics in the Prometheus format.
We recommend you install and configure Prometheus server to monitor your Cortex cluster.&lt;/p>
&lt;p>We publish a set of Prometheus alerts and Grafana dashboards as the &lt;a href="https://github.com/grafana/cortex-jsonnet">cortex-mixin&lt;/a>.
We recommend you use these for any production Cortex cluster.&lt;/p>
&lt;h2 id="5-authentication--multitenancy">5. Authentication &amp;amp; Multitenancy&lt;/h2>
&lt;p>If you want to run Cortex as a multi-tenant system, you need to give each
tenant a unique ID - this can be any string.
Managing tenants and allocating IDs must be done outside of Cortex.
See &lt;a href="/docs/guides/auth/">Authentication and Authorisation&lt;/a> for more information.&lt;/p>
&lt;h2 id="6-handling-ha-prometheus-pairs">6. Handling HA Prometheus Pairs&lt;/h2>
&lt;p>You should use a pair of Prometheus servers to monitor your targets and send metrics to Cortex.
This allows your monitoring system to survive the failure of one of these Prometheus instances.
Cortex support deduping the samples on ingestion.
For more information on how to configure Cortex and Prometheus to HA pairs, see &lt;a href="/docs/guides/ha-pair-handling/">Config for sending HA Pairs data to Cortex&lt;/a>.&lt;/p></description></item><item><title>Docs: Running Cortex chunks storage with Cassandra</title><link>/docs/guides/running-chunks-storage-with-cassandra/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/running-chunks-storage-with-cassandra/</guid><description>
&lt;p>This guide covers how to run a single local Cortex instance - with the &lt;a href="/docs/chunks-storage/">&lt;strong>chunks storage&lt;/strong>&lt;/a> engine - storing time series chunks and index in Cassandra.&lt;/p>
&lt;p>In this guide we&amp;rsquo;re going to:&lt;/p>
&lt;ol>
&lt;li>Setup a locally running Cassandra&lt;/li>
&lt;li>Configure Cortex to store chunks and index on Cassandra&lt;/li>
&lt;li>Configure Prometheus to send series to Cortex&lt;/li>
&lt;li>Configure Grafana to visualise metrics&lt;/li>
&lt;/ol>
&lt;h2 id="setup-a-locally-running-cassandra">Setup a locally running Cassandra&lt;/h2>
&lt;p>Run Cassandra with the following command:&lt;/p>
&lt;pre>&lt;code>docker run -d --name cassandra --rm -p 9042:9042 cassandra:3.11
&lt;/code>&lt;/pre>&lt;p>Use Docker to execute the Cassandra Query Language (CQL) shell in the container:&lt;/p>
&lt;pre>&lt;code>docker exec -it &amp;lt;container_id&amp;gt; cqlsh
&lt;/code>&lt;/pre>&lt;p>Create a new Cassandra keyspace for Cortex metrics:&lt;/p>
&lt;p>A keyspace is an object that is used to hold column families, user defined types. A keyspace is like RDBMS database which contains column families, indexes, user defined types.&lt;/p>
&lt;pre>&lt;code>CREATE KEYSPACE cortex WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1};
&lt;/code>&lt;/pre>&lt;h2 id="configure-cortex-to-store-chunks-and-index-on-cassandra">Configure Cortex to store chunks and index on Cassandra&lt;/h2>
&lt;p>Now, we have to configure Cortex to store the chunks and index in Cassandra. Create a config file called &lt;code>single-process-config.yaml&lt;/code>, then add the content below. Make sure to replace the following placeholders:&lt;/p>
&lt;ul>
&lt;li>&lt;code>LOCALHOST&lt;/code>: Addresses of your Cassandra instance. This can accept multiple addresses by passing them as comma separated values.&lt;/li>
&lt;li>&lt;code>KEYSPACE&lt;/code>: The name of the Cassandra keyspace used to store the metrics.&lt;/li>
&lt;/ul>
&lt;p>&lt;code>single-process-config.yaml&lt;/code>&lt;/p>
&lt;pre>&lt;code># Configuration for running Cortex in single-process mode.
# This should not be used in production. It is only for getting started
# and development.
# Disable the requirement that every request to Cortex has a
# X-Scope-OrgID header. `fake` will be substituted in instead.
auth_enabled: false
server:
http_listen_port: 9009
# Configure the server to allow messages up to 100MB.
grpc_server_max_recv_msg_size: 104857600
grpc_server_max_send_msg_size: 104857600
grpc_server_max_concurrent_streams: 1000
distributor:
shard_by_all_labels: true
pool:
health_check_ingesters: true
ingester_client:
grpc_client_config:
# Configure the client to allow messages up to 100MB.
max_recv_msg_size: 104857600
max_send_msg_size: 104857600
grpc_compression: gzip
ingester:
lifecycler:
# The address to advertise for this ingester. Will be autodiscovered by
# looking up address on eth0 or en0; can be specified if this fails.
address: 127.0.0.1
# We want to start immediately and flush on shutdown.
join_after: 0
final_sleep: 0s
num_tokens: 512
# Use an in memory ring store, so we don't need to launch a Consul.
ring:
kvstore:
store: inmemory
replication_factor: 1
# Use cassandra as storage -for both index store and chunks store.
schema:
configs:
- from: 2019-07-29
store: cassandra
object_store: cassandra
schema: v10
index:
prefix: index_
period: 168h
chunks:
prefix: chunk_
period: 168h
storage:
cassandra:
addresses: LOCALHOST # configure cassandra addresses here.
keyspace: KEYSPACE # configure desired keyspace here.
&lt;/code>&lt;/pre>&lt;p>The latest tag is not published for the Cortex docker image. Visit quay.io/repository/cortexproject/cortex
to find the latest stable version tag and use it in the command below (currently it is &lt;code>v1.7.0&lt;/code>).&lt;/p>
&lt;p>Run Cortex using the latest stable version:&lt;/p>
&lt;pre>&lt;code>docker run -d --name=cortex -v $(pwd)/single-process-config.yaml:/etc/single-process-config.yaml -p 9009:9009 quay.io/cortexproject/cortex:v1.7.0 -config.file=/etc/single-process-config.yaml
&lt;/code>&lt;/pre>&lt;p>In case you prefer to run the master version, please follow this &lt;a href="/docs/getting-started/getting-started-chunks-storage/">documentation&lt;/a> on how to build Cortex from source.&lt;/p>
&lt;h3 id="configure-the-index-and-chunk-table-options">Configure the index and chunk table options&lt;/h3>
&lt;p>In order to create index and chunk tables on Cassandra, Cortex will use the default table options of your Cassandra.
If you want to configure the table options, use the &lt;code>storage.cassandra.table_options&lt;/code> property or &lt;code>cassandra.table-options&lt;/code> flag.
This configuration property is just &lt;code>string&lt;/code> type and this value used as plain text on &lt;code>WITH&lt;/code> option of table creation query.
It is recommended to enclose the value of &lt;code>table_options&lt;/code> in double-quotes because you should enclose strings of table options in quotes on Cassandra.&lt;/p>
&lt;p>For example, suppose the name of index(or chunk) table is &amp;lsquo;test_table&amp;rsquo;.
Details about column definitions of the table are omitted.
If no table options configured, then Cortex will generate the query to create a table without a &lt;code>WITH&lt;/code> clause to use default table options:&lt;/p>
&lt;pre>&lt;code>CREATE TABLE IF NOT EXISTS cortex.test_table (...)
&lt;/code>&lt;/pre>&lt;p>If table options configured with &lt;code>table_options&lt;/code> as below:&lt;/p>
&lt;pre>&lt;code>storage:
cassandra:
addresses: 127.0.0.1
keyspace: cortex
table_options: &amp;quot;gc_grace_seocnds = 86400
AND comments = 'this is a test table'
AND COMPACT STORAGE
AND caching = { 'keys': 'ALL', 'rows_per_partition': 1024 }&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Then Cortex will generate the query to create a table with a &lt;code>WITH&lt;/code> clause as below:&lt;/p>
&lt;pre>&lt;code>CREATE TABLE IF NOT EXISTS cortex.test_table (...) WITH gc_grace_seocnds = 86400 AND comments = 'this is a test table' AND COMPACT STORAGE AND caching = { 'keys': 'ALL', 'rows_per_partition': 1024 }
&lt;/code>&lt;/pre>&lt;p>Available settings of the table options on Cassandra depend on Cassandra version or storage which is compatible.
For details about table options, see the official document of storage you are using.&lt;/p>
&lt;p>&lt;strong>WARNING&lt;/strong>: Make sure there are no incorrect options and mistakes. Misconfigured table options may cause a failure in creating a table by &lt;a href="/docs/chunks-storage/table-manager/">table-manager&lt;/a> at runtime and seriously affect your Cortex.&lt;/p>
&lt;h2 id="configure-prometheus-to-send-series-to-cortex">Configure Prometheus to send series to Cortex&lt;/h2>
&lt;p>Now that Cortex is up, it should be running on &lt;code>http://localhost:9009&lt;/code>.&lt;/p>
&lt;p>Add the following section to your Prometheus configuration file. This will configure the remote write to send metrics to Cortex.&lt;/p>
&lt;pre>&lt;code>remote_write:
- url: http://localhost:9009/api/prom/push
&lt;/code>&lt;/pre>&lt;h2 id="configure-grafana-to-visualise-metrics">Configure Grafana to visualise metrics&lt;/h2>
&lt;p>Run grafana to visualise metrics from Cortex:&lt;/p>
&lt;pre>&lt;code>docker run -d --name=grafana -p 3000:3000 grafana/grafana
&lt;/code>&lt;/pre>&lt;p>Add a data source in Grafana by selecting Prometheus as the data source type and use the Cortex URL to query metrics: &lt;code>http://localhost:9009/api/prom&lt;/code>.&lt;/p>
&lt;p>Finally, You can monitor Cortex&amp;rsquo;s reads &amp;amp; writes by creating the dashboard. If you&amp;rsquo;re looking for ready to use dashboards, you can take a look at Grafana&amp;rsquo;s &lt;a href="https://github.com/grafana/cortex-jsonnet/">Cortex dashboards and alerts&lt;/a> (Jsonnet) or Weaveworks&amp;rsquo;s &lt;a href="https://github.com/weaveworks/cortex-dashboards">Cortex dashboards&lt;/a> (Python).&lt;/p></description></item><item><title>Docs: Running Cortex on Kubernetes</title><link>/docs/guides/running-cortex-on-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/running-cortex-on-kubernetes/</guid><description>
&lt;p>Because Cortex is designed to run multiple instances of each component
(ingester, querier, etc.), you probably want to automate the placement
and shepherding of these instances. Most users choose Kubernetes to do
this, but this is not mandatory.&lt;/p>
&lt;h2 id="configuration">Configuration&lt;/h2>
&lt;h3 id="resource-requests">Resource requests&lt;/h3>
&lt;p>If using Kubernetes, each container should specify resource requests
so that the scheduler can place them on a node with sufficient capacity.&lt;/p>
&lt;p>For example an ingester might request:&lt;/p>
&lt;pre>&lt;code> resources:
requests:
cpu: 4
memory: 10Gi
&lt;/code>&lt;/pre>&lt;p>The specific values here should be adjusted based on your own
experiences running Cortex - they are very dependent on rate of data
arriving and other factors such as series churn.&lt;/p>
&lt;h3 id="take-extra-care-with-ingesters">Take extra care with ingesters&lt;/h3>
&lt;p>Ingesters hold hours of timeseries data in memory; you can configure
Cortex to replicate the data but you should take steps to avoid losing
all replicas at once:&lt;/p>
&lt;ul>
&lt;li>Don&amp;rsquo;t run multiple ingesters on the same node.&lt;/li>
&lt;li>Don&amp;rsquo;t run ingesters on preemptible/spot nodes.&lt;/li>
&lt;li>Spread out ingesters across racks / availability zones / whatever
applies in your datacenters.&lt;/li>
&lt;/ul>
&lt;p>You can ask Kubernetes to avoid running on the same node like this:&lt;/p>
&lt;pre>&lt;code> affinity:
podAntiAffinity:
preferredDuringSchedulingIgnoredDuringExecution:
- weight: 100
podAffinityTerm:
labelSelector:
matchExpressions:
- key: name
operator: In
values:
- ingester
topologyKey: &amp;quot;kubernetes.io/hostname&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Give plenty of time for an ingester to hand over or flush data to
store when shutting down; for Kubernetes this looks like:&lt;/p>
&lt;pre>&lt;code> terminationGracePeriodSeconds: 2400
&lt;/code>&lt;/pre>&lt;p>Ask Kubernetes to limit rolling updates to one ingester at a time, and
signal the old one to stop before the new one is ready:&lt;/p>
&lt;pre>&lt;code> strategy:
rollingUpdate:
maxSurge: 0
maxUnavailable: 1
&lt;/code>&lt;/pre>&lt;p>Ingesters provide an HTTP hook to signal readiness when all is well;
this is valuable because it stops a rolling update at the first
problem:&lt;/p>
&lt;pre>&lt;code> readinessProbe:
httpGet:
path: /ready
port: 80
&lt;/code>&lt;/pre>&lt;p>We do not recommend configuring a liveness probe on ingesters -
killing them is a last resort and should not be left to a machine.&lt;/p></description></item><item><title>Docs: Authentication and Authorisation</title><link>/docs/guides/auth/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/auth/</guid><description>
&lt;p>All Cortex components take the tenant ID from a header &lt;code>X-Scope-OrgID&lt;/code>
on each request. A tenant (also called &amp;ldquo;user&amp;rdquo; or &amp;ldquo;org&amp;rdquo;) is the owner of
a set of series written to and queried from Cortex. All Cortex components
trust this value completely: if you need to protect your Cortex installation
from accidental or malicious calls then you must add an additional layer
of protection.&lt;/p>
&lt;p>Typically this means you run Cortex behind a reverse proxy, and you must
ensure that all callers, both machines sending data over the &lt;code>remote_write&lt;/code>
interface and humans sending queries from GUIs, supply credentials
which identify them and confirm they are authorised.&lt;/p>
&lt;p>When configuring the &lt;code>remote_write&lt;/code> API in Prometheus there is no way to
add extra headers. The user and password fields of http Basic auth, or
Bearer token, can be used to convey the tenant ID and/or credentials.
See the &lt;strong>Cortex-Tenant&lt;/strong> section below for one way to solve this.&lt;/p>
&lt;p>To disable the multi-tenant functionality, you can pass the argument
&lt;code>-auth.enabled=false&lt;/code> to every Cortex component, which will set the OrgID
to the string &lt;code>fake&lt;/code> for every request.&lt;/p>
&lt;p>Note that the tenant ID that is used to write the series to the datastore
should be the same as the one you use to query the data. If they don&amp;rsquo;t match
you won&amp;rsquo;t see any data. As of now, you can&amp;rsquo;t see series from other tenants.&lt;/p>
&lt;p>For more information regarding the tenant ID limits, refer to: &lt;a href="/docs/guides/limitations/#tenant-id-naming">Tenant ID limitations&lt;/a>&lt;/p>
&lt;h3 id="cortex-tenant">Cortex-Tenant&lt;/h3>
&lt;p>One way to add &lt;code>X-Scope-OrgID&lt;/code> to Prometheus requests is to use a &lt;a href="https://github.com/blind-oracle/cortex-tenant">cortex-tenant&lt;/a>
proxy which is able to extract the tenant ID from Prometheus labels.&lt;/p>
&lt;p>It can be placed between Prometheus and Cortex and will search for a predefined
label and use its value as &lt;code>X-Scope-OrgID&lt;/code> header when proxying the timeseries to Cortex.&lt;/p>
&lt;p>This can help to run Cortex in a trusted environment where you want to separate your metrics
into distinct namespaces by some criteria (e.g. teams, applications, etc).&lt;/p>
&lt;p>Be advised that &lt;strong>cortex-tenant&lt;/strong> is a third-party community project and it&amp;rsquo;s not maintained by Cortex team.&lt;/p></description></item><item><title>Docs: Capacity Planning</title><link>/docs/guides/capacity-planning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/capacity-planning/</guid><description>
&lt;p>You will want to estimate how many nodes are required, how many of
each component to run, and how much storage space will be required.
In practice, these will vary greatly depending on the metrics being
sent to Cortex.&lt;/p>
&lt;p>Some key parameters are:&lt;/p>
&lt;ol>
&lt;li>The number of active series. If you have Prometheus already you
can query &lt;code>prometheus_tsdb_head_series&lt;/code> to see this number.&lt;/li>
&lt;li>Sampling rate, e.g. a new sample for each series every minute
(the default Prometheus &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/">scrape_interval&lt;/a>).
Multiply this by the number of active series to get the
total rate at which samples will arrive at Cortex.&lt;/li>
&lt;li>The rate at which series are added and removed. This can be very
high if you monitor objects that come and go - for example if you run
thousands of batch jobs lasting a minute or so and capture metrics
with a unique ID for each one. &lt;a href="https://www.robustperception.io/using-tsdb-analyze-to-investigate-churn-and-cardinality">Read how to analyse this on
Prometheus&lt;/a>.&lt;/li>
&lt;li>How compressible the time-series data are. If a metric stays at
the same value constantly, then Cortex can compress it very well, so
12 hours of data sampled every 15 seconds would be around 2KB. On
the other hand if the value jumps around a lot it might take 10KB.
There are not currently any tools available to analyse this.&lt;/li>
&lt;li>How long you want to retain data for, e.g. 1 month or 2 years.&lt;/li>
&lt;/ol>
&lt;p>Other parameters which can become important if you have particularly
high values:&lt;/p>
&lt;ol start="6">
&lt;li>Number of different series under one metric name.&lt;/li>
&lt;li>Number of labels per series.&lt;/li>
&lt;li>Rate and complexity of queries.&lt;/li>
&lt;/ol>
&lt;p>Now, some rules of thumb:&lt;/p>
&lt;ol>
&lt;li>Each million series in an ingester takes 15GB of RAM. Total number
of series in ingesters is number of active series times the
replication factor. This is with the default of 12-hour chunks - RAM
required will reduce if you set &lt;code>-ingester.max-chunk-age&lt;/code> lower
(trading off more back-end database IO)&lt;/li>
&lt;li>Each million series (including churn) consumes 15GB of chunk
storage and 4GB of index, per day (so multiply by the retention
period).&lt;/li>
&lt;li>Each 100,000 samples/sec arriving takes 1 CPU in distributors.
Distributors don&amp;rsquo;t need much RAM.&lt;/li>
&lt;/ol>
&lt;p>If you turn on compression between distributors and ingesters (for
example to save on inter-zone bandwidth charges at AWS/GCP) they will use
significantly more CPU (approx 100% more for distributor and 50% more
for ingester).&lt;/p></description></item><item><title>Docs: Config for horizontally scaling the Ruler</title><link>/docs/guides/ruler-sharding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/ruler-sharding/</guid><description>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>One option to scale the ruler is by scaling it horizontally. However, with multiple ruler instances running they will need to coordinate to determine which instance will evaluate which rule. Similar to the ingesters, the rulers establish a hash ring to divide up the responsibilities of evaluating rules.&lt;/p>
&lt;h2 id="config">Config&lt;/h2>
&lt;p>In order to enable sharding in the ruler the following flag needs to be set:&lt;/p>
&lt;pre>&lt;code> -ruler.enable-sharding=true
&lt;/code>&lt;/pre>&lt;p>In addition the ruler requires it&amp;rsquo;s own ring to be configured, for instance:&lt;/p>
&lt;pre>&lt;code> -ruler.ring.consul.hostname=consul.dev.svc.cluster.local:8500
&lt;/code>&lt;/pre>&lt;p>The only configuration that is required is to enable sharding and configure a key value store. From there the rulers will shard and handle the division of rules automatically.&lt;/p>
&lt;p>Unlike ingesters, rulers do not hand over responsibility: all rules are re-sharded randomly every time a ruler is added to or removed from the ring.&lt;/p>
&lt;h2 id="ruler-storage">Ruler Storage&lt;/h2>
&lt;p>The ruler supports six kinds of storage (configdb, azure, gcs, s3, swift, local). Most kinds of storage work with the sharded ruler configuration in an obvious way. i.e. configure all rulers to use the same backend.&lt;/p>
&lt;p>The local implementation reads &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/">Prometheus recording rules&lt;/a> off of the local filesystem. This is a read only backend that does not support the creation and deletion of rules through &lt;a href="https://cortexmetrics.io/docs/apis/#ruler">the API&lt;/a>. Despite the fact that it reads the local filesystem this method can still be used in a sharded ruler configuration if the operator takes care to load the same rules to every ruler. For instance this could be accomplished by mounting a &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/">Kubernetes ConfigMap&lt;/a> onto every ruler pod.&lt;/p>
&lt;p>A typical local config may look something like:&lt;/p>
&lt;pre>&lt;code> -ruler.storage.type=local
-ruler.storage.local.directory=/tmp/cortex/rules
&lt;/code>&lt;/pre>&lt;p>With the above configuration the ruler would expect the following layout:&lt;/p>
&lt;pre>&lt;code>/tmp/cortex/rules/&amp;lt;tenant id&amp;gt;/rules1.yaml
/rules2.yaml
&lt;/code>&lt;/pre>&lt;p>Yaml files are expected to be in the &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/#recording-rules">Prometheus format&lt;/a>.&lt;/p></description></item><item><title>Docs: Config for sending HA Pairs data to Cortex</title><link>/docs/guides/ha-pair-handling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/ha-pair-handling/</guid><description>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>You can have more than a single Prometheus monitoring and ingesting the same metrics for redundancy. Cortex already does replication for redundancy and it doesn&amp;rsquo;t make sense to ingest the same data twice. So in Cortex, we made sure we can dedupe the data we receive from HA Pairs of Prometheus. We do this via the following:&lt;/p>
&lt;p>Assume that there are two teams, each running their own Prometheus, monitoring different services. Let&amp;rsquo;s call the Prometheis T1 and T2. Now, if the teams are running HA pairs, let&amp;rsquo;s call the individual Prometheis, T1.a, T1.b and T2.a and T2.b.&lt;/p>
&lt;p>In Cortex we make sure we only ingest from one of T1.a and T1.b, and only from one of T2.a and T2.b. We do this by electing a leader replica for each cluster of Prometheus. For example, in the case of T1, let it be T1.a. As long as T1.a is the leader, we drop the samples sent by T1.b. And if Cortex sees no new samples from T1.a for a short period (30s by default), it&amp;rsquo;ll switch the leader to be T1.b.&lt;/p>
&lt;p>This means if T1.a goes down for a few minutes Cortex&amp;rsquo;s HA sample handling will have switched and elected T1.b as the leader. This failover timeout is what enables us to only accept samples from a single replica at a time, but ensure we don&amp;rsquo;t drop too much data in case of issues. Note that with the default scrape period of 15s, and the default timeouts in Cortex, in most cases you&amp;rsquo;ll only lose a single scrape of data in the case of a leader election failover. For any rate queries the rate window should be at least 4x the scrape period to account for any of these failover scenarios, for example with the default scrape period of 15s then you should calculate rates over at least 1m periods.&lt;/p>
&lt;p>Now we do the same leader election process T2.&lt;/p>
&lt;h2 id="config">Config&lt;/h2>
&lt;h3 id="client-side">Client Side&lt;/h3>
&lt;p>So for Cortex to achieve this, we need 2 identifiers for each process, one identifier for the cluster (T1 or T2, etc) and one identifier to identify the replica in the cluster (a or b). The easiest way to do with is by setting external labels, the default labels are &lt;code>cluster&lt;/code> and &lt;code>__replica__&lt;/code>. For example:&lt;/p>
&lt;pre>&lt;code>cluster: prom-team1
__replica__: replica1 (or pod-name)
&lt;/code>&lt;/pre>&lt;p>and&lt;/p>
&lt;pre>&lt;code>cluster: prom-team1
__replica__: replica2
&lt;/code>&lt;/pre>&lt;p>Note: These are external labels and have nothing to do with remote_write config.&lt;/p>
&lt;p>These two label names are configurable per-tenant within Cortex, and should be set to something sensible. For example, cluster label is already used by some workloads, and you should set the label to be something else but uniquely identifies the cluster. Good examples for this label-name would be &lt;code>team&lt;/code>, &lt;code>cluster&lt;/code>, &lt;code>prometheus&lt;/code>, etc.&lt;/p>
&lt;p>The replica label should be set so that the value for each prometheus is unique in that cluster. Note: Cortex drops this label when ingesting data, but preserves the cluster label. This way, your timeseries won&amp;rsquo;t change when replicas change.&lt;/p>
&lt;h3 id="server-side">Server Side&lt;/h3>
&lt;p>The minimal configuration requires:&lt;/p>
&lt;ul>
&lt;li>Enabling the HA tracker via &lt;code>-distributor.ha-tracker.enable=true&lt;/code> CLI flag (or its YAML config option)&lt;/li>
&lt;li>Configuring the KV store for the ring (See: &lt;a href="/docs/configuration/arguments/#ringha-tracker-store">Ring/HA Tracker Store&lt;/a>). Only Consul and etcd are currently supported. Multi should be used for migration purposes only.&lt;/li>
&lt;li>Setting the limits configuration to accept samples via &lt;code>-distributor.ha-tracker.enable-for-all-users&lt;/code> (or its YAML config option)&lt;/li>
&lt;/ul>
&lt;p>The following configuration snippet shows an example of the HA tracker config via YAML config file:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#204a87;font-weight:bold">limits&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>...&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">accept_ha_samples&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>...&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">distributor&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>...&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">ha_tracker&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enable_ha_tracker&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>...&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">kvstore&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">[store&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;string&amp;gt;&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>|&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>default&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>=&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#34;consul&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">[consul | etcd&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;config&amp;gt;&lt;span style="color:#000;font-weight:bold">]&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>...&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>...&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For further configuration file documentation, see the &lt;a href="/docs/configuration/configuration-file/#distributor_config">distributor section&lt;/a> and &lt;a href="/docs/configuration/arguments/#ringha-tracker-store">Ring/HA Tracker Store&lt;/a>.&lt;/p>
&lt;p>For flag configuration, see the &lt;a href="/docs/configuration/arguments/#ha-tracker">distributor flags&lt;/a> having &lt;code>ha-tracker&lt;/code> in them.&lt;/p></description></item><item><title>Docs: Deleting Series</title><link>/docs/guides/deleting-series/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/deleting-series/</guid><description>
&lt;p>&lt;em>This feature is currently experimental and is only supported for Chunks storage.&lt;/em>&lt;/p>
&lt;p>Cortex supports deletion of series using &lt;a href="https://prometheus.io/docs/prometheus/latest/querying/api/#delete-series">Prometheus compatible API&lt;/a>.
It however does not support &lt;a href="https://prometheus.io/docs/prometheus/latest/querying/api/#clean-tombstones">Prometheuses Clean Tombstones&lt;/a> API because Cortex uses a different mechanism to manage deletions.&lt;/p>
&lt;h3 id="how-it-works">How it works&lt;/h3>
&lt;p>A new service called &lt;code>purger&lt;/code> is added which exposes deletion APIs and does the processing of the requests.
To store the requests, and some additional information while performing deletions, the purger requires configuring an index and object store respectively for it.
For more information about the &lt;code>purger&lt;/code> configuration, please refer to the &lt;a href="/docs/configuration/configuration-file/#purger_config">config file reference&lt;/a> documentation.&lt;/p>
&lt;p>All the requests specified below needs to be sent to &lt;code>purger&lt;/code>.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> If you have enabled multi-tenancy in your Cortex cluster then deletion APIs requests require to have the &lt;code>X-Scope-OrgID&lt;/code> header set like for any other Cortex API.&lt;/p>
&lt;h4 id="requesting-deletion">Requesting Deletion&lt;/h4>
&lt;p>By calling the &lt;code>/api/v1/admin/tsdb/delete_series&lt;/code> API like how it is done in &lt;a href="https://prometheus.io/docs/prometheus/latest/querying/api/#delete-series">Prometheus&lt;/a>, you can request the deletion of series.
Delete Series requests are immediately honored by eliminating series requested for deletion from query responses without actually deleting them from storage.
The actual data is not deleted from storage until period configured for &lt;code>-purger.delete-request-cancel-period&lt;/code> CLI flag or its respective YAML config option which helps operators take informed decision about continuing with the deletion or cancelling the request.&lt;/p>
&lt;p>Cortex would keep eliminating series requested for deletion until the &lt;code>purger&lt;/code> is done processing the delete request or the delete request gets cancelled.&lt;/p>
&lt;p>&lt;em>Sample cURL command:&lt;/em>&lt;/p>
&lt;pre>&lt;code>curl -X POST \
'&amp;lt;purger_addr&amp;gt;/api/v1/admin/tsdb/delete_series?match%5B%5D=up&amp;amp;start=1591616227&amp;amp;end=1591619692' \
-H 'x-scope-orgid: &amp;lt;tenant-id&amp;gt;'
&lt;/code>&lt;/pre>&lt;h4 id="cancellation-of-delete-request">Cancellation of Delete Request&lt;/h4>
&lt;p>Cortex allows cancellation of delete requests until they are not picked up for processing, which is controlled by the &lt;code>-purger.delete-request-cancel-period&lt;/code> CLI flag or its respective YAML config option.
Since Cortex does query time filtering of data request for deletion until it is actually deleted, you can take an informed decision to cancel the delete request by calling the API defined below:&lt;/p>
&lt;pre>&lt;code>POST /api/v1/admin/tsdb/cancel_delete_request?request_id=&amp;lt;request_id&amp;gt;
PUT /api/v1/admin/tsdb/cancel_delete_request?request_id=&amp;lt;request_id&amp;gt;
&lt;/code>&lt;/pre>&lt;p>&lt;em>Sample cURL command:&lt;/em>&lt;/p>
&lt;pre>&lt;code>curl -X POST \
'&amp;lt;purger_addr&amp;gt;/api/v1/admin/tsdb/cancel_delete_request?request_id=&amp;lt;request_id&amp;gt;' \
-H 'x-scope-orgid: &amp;lt;tenant-id&amp;gt;'
&lt;/code>&lt;/pre>&lt;p>You can find the id of the request that you want to cancel by using the GET &lt;code>delete_series&lt;/code> API defined below.&lt;/p>
&lt;h4 id="listing-delete-requests">Listing Delete Requests&lt;/h4>
&lt;p>You can list the created delete requests using following API:&lt;/p>
&lt;pre>&lt;code>GET /api/v1/admin/tsdb/delete_series
&lt;/code>&lt;/pre>&lt;p>&lt;em>Sample cURL command:&lt;/em>&lt;/p>
&lt;pre>&lt;code>curl -X GET \
&amp;lt;purger_addr&amp;gt;/api/v1/admin/tsdb/delete_series \
-H 'x-scope-orgid: &amp;lt;orgid&amp;gt;'
&lt;/code>&lt;/pre>&lt;p>&lt;strong>NOTE:&lt;/strong> List API returns both processed and un-processed requests except the cancelled ones since they are removed from the store.&lt;/p></description></item><item><title>Docs: Encryption at Rest</title><link>/docs/guides/encryption-at-rest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/encryption-at-rest/</guid><description>
&lt;p>Cortex supports data encryption at rest for some storage backends.&lt;/p>
&lt;h2 id="s3">S3&lt;/h2>
&lt;p>The Cortex S3 client supports the following server-side encryption (SSE) modes:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html">SSE-S3&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html">SSE-KMS&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="blocks-storage">Blocks storage&lt;/h3>
&lt;p>The &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> S3 server-side encryption can be configured as follows.&lt;/p>
&lt;h3 id="s3_sse_config">&lt;code>s3_sse_config&lt;/code>&lt;/h3>
&lt;p>The &lt;code>s3_sse_config&lt;/code> configures the S3 server-side encryption.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#204a87;font-weight:bold">sse&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#8f5902;font-style:italic"># Enable AWS Server Side Encryption. Supported values: SSE-KMS, SSE-S3.&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#8f5902;font-style:italic"># CLI flag: -s3.sse.type&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">[type&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;string&amp;gt;&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>|&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>default&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>=&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#8f5902;font-style:italic"># KMS Key ID used to encrypt objects in S3&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#8f5902;font-style:italic"># CLI flag: -s3.sse.kms-key-id&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">[kms_key_id&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;string&amp;gt;&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>|&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>default&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>=&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#8f5902;font-style:italic"># KMS Encryption Context used for object encryption. It expects JSON formatted&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#8f5902;font-style:italic"># string.&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#8f5902;font-style:italic"># CLI flag: -s3.sse.kms-encryption-context&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">[kms_encryption_context&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;string&amp;gt;&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>|&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>default&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>=&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="chunks-storage">Chunks storage&lt;/h3>
&lt;p>The &lt;a href="/docs/chunks-storage/">chunks storage&lt;/a> S3 server-side encryption can be configured similarly to the blocks storage, but &lt;strong>per-tenant overrides are not supported&lt;/strong>.&lt;/p>
&lt;h3 id="ruler">Ruler&lt;/h3>
&lt;p>The ruler S3 server-side encryption can be configured similarly to the blocks storage. The per-tenant overrides are supported when using the storage backend configurable the &lt;code>-ruler-storage.&lt;/code> flag prefix (or their respective YAML config options).&lt;/p>
&lt;h3 id="per-tenant-config-overrides">Per-tenant config overrides&lt;/h3>
&lt;p>The S3 client used by the blocks storage and ruler supports S3 SSE config overrides on a per-tenant basis, using the &lt;a href="/docs/configuration/arguments/#runtime-configuration-file">runtime configuration file&lt;/a>.
The following settings can ben overridden for each tenant:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;code>s3_sse_type&lt;/code>&lt;/strong>&lt;br />
S3 server-side encryption type. It must be set to enable the SSE config override for a given tenant.&lt;/li>
&lt;li>&lt;strong>&lt;code>s3_sse_kms_key_id&lt;/code>&lt;/strong>&lt;br />
S3 server-side encryption KMS Key ID. Ignored if the SSE type override is not set or the type is not &lt;code>SSE-KMS&lt;/code>.&lt;/li>
&lt;li>&lt;strong>&lt;code>s3_sse_kms_encryption_context&lt;/code>&lt;/strong>&lt;br />
S3 server-side encryption KMS encryption context. If unset and the key ID override is set, the encryption context will not be provided to S3. Ignored if the SSE type override is not set or the type is not &lt;code>SSE-KMS&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="other-storages">Other storages&lt;/h2>
&lt;p>Other storage backends may support encryption at rest configuring it directly at the storage level.&lt;/p></description></item><item><title>Docs: gRPC storage plugin</title><link>/docs/guides/grpc-based-plugin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/grpc-based-plugin/</guid><description>
&lt;p>&lt;em>This feature is currently experimental and is only supported for Chunks storage.&lt;/em>&lt;/p>
&lt;p>Cortex chunks storage supports a &lt;strong>gRPC-based plugin system&lt;/strong> to use alternative backends for the index and chunks store.
A store plugin is a gRPC-based server which implements the methods required by the index and chunks store. Cortex chunks storage schema is then configured to use the plugin as backend system and gRPC will be used to communicate between Cortex and the plugin.
For example, if you&amp;rsquo;re deploying your Cortex cluster on Kubernetes, the plugin would run as a sidecar container of your Cortex pods and the Cortex&amp;rsquo;s &lt;code>-grpc-store.server-address&lt;/code> should be configured to the endpoint exposed by the sidecar plugin (eg. &lt;code>localhost:&amp;lt;port&amp;gt;&lt;/code>).&lt;/p>
&lt;h3 id="how-it-works">How it works&lt;/h3>
&lt;p>In the cortex configuration file, add &lt;code>store&lt;/code> and &lt;code>object_store&lt;/code> as &lt;code>grpc-store&lt;/code> and configure storage with plugin server endpoint (ie. the address to the gRPC server which implements the cortex chunk store methods).&lt;/p>
&lt;pre>&lt;code>schema:
configs:
- from: 2019-07-29
store: grpc-store
object_store: grpc-store
schema: v10
index:
prefix: index_
period: 168h
chunks:
prefix: chunk_
period: 168h
storage:
grpc_store:
# gRPC server address
server_address: localhost:6666
&lt;/code>&lt;/pre>&lt;h2 id="community-plugins">Community plugins&lt;/h2>
&lt;p>The following list shows Cortex storage plugins built and shared by the community:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://github.com/VineethReddy02/cortex-mongo-store">gRPC based Cortex chunk store for Mongo&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/VineethReddy02/cortex-mysql-store">gRPC based Cortex chunk store for Mysql&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Docs: Ingesters rolling updates</title><link>/docs/guides/ingesters-rolling-updates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/ingesters-rolling-updates/</guid><description>
&lt;p>Cortex &lt;a href="/docs/architecture/#ingester">ingesters&lt;/a> are semi-stateful.
A running ingester holds several hours of time series data in memory, before they&amp;rsquo;re flushed to the long-term storage.
When an ingester shutdowns, because of a rolling update or maintenance, the in-memory data must not be discarded in order to avoid any data loss.&lt;/p>
&lt;p>In this document we describe the techniques employed to safely handle rolling updates, based on different setups:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#blocks-storage">Blocks storage&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chunks-storage-with-wal-enabled">Chunks storage with WAL enabled&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chunks-storage-with-wal-disabled-hand-over">Chunks storage with WAL disabled&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>If you&amp;rsquo;re looking how to scale up / down ingesters, please refer to the &lt;a href="/docs/guides/ingesters-scaling-up-and-down/">dedicated guide&lt;/a>.&lt;/em>&lt;/p>
&lt;h2 id="blocks-storage">Blocks storage&lt;/h2>
&lt;p>The Cortex &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> requires ingesters to run with a persistent disk where the TSDB WAL and blocks are stored (eg. a StatefulSet when deployed on Kubernetes).&lt;/p>
&lt;p>During a rolling update, the leaving ingester closes the open TSDBs, synchronize the data to disk (&lt;code>fsync&lt;/code>) and releases the disk resources.
The new ingester, which is expected to reuse the same disk of the leaving one, will replay the TSDB WAL on startup in order to load back in memory the time series that have not been compacted into a block yet.&lt;/p>
&lt;p>&lt;em>The blocks storage doesn&amp;rsquo;t support the series &lt;a href="#chunks-storage-with-wal-disabled-hand-over">hand-over&lt;/a>.&lt;/em>&lt;/p>
&lt;h2 id="chunks-storage">Chunks storage&lt;/h2>
&lt;p>The Cortex chunks storage optionally supports a write-ahead log (WAL).
The rolling update procedure for a Cortex cluster running the chunks storage depends whether the WAL is enabled or not.&lt;/p>
&lt;h3 id="chunks-storage-with-wal-enabled">Chunks storage with WAL enabled&lt;/h3>
&lt;p>Similarly to the blocks storage, when Cortex is running the &lt;a href="/docs/chunks-storage/">chunks storage&lt;/a> with WAL enabled, it requires ingesters to run with a persistent disk where the WAL is stored (eg. a StatefulSet when deployed on Kubernetes).&lt;/p>
&lt;p>During a rolling update, the leaving ingester closes the WAL, synchronize the data to disk (&lt;code>fsync&lt;/code>) and releases the disk resources.
The new ingester, which is expected to reuse the same disk of the leaving one, will replay the WAL on startup in order to load back in memory the time series data.&lt;/p>
&lt;p>&lt;em>For more information about the WAL, please refer to &lt;a href="/docs/chunks-storage/ingesters-with-wal/">Ingesters with WAL&lt;/a>.&lt;/em>&lt;/p>
&lt;h3 id="chunks-storage-with-wal-disabled-hand-over">Chunks storage with WAL disabled (hand-over)&lt;/h3>
&lt;p>When Cortex is running the &lt;a href="/docs/chunks-storage/">chunks storage&lt;/a> with WAL disabled, Cortex supports on-the-fly series hand-over between a leaving ingester and a joining one.&lt;/p>
&lt;p>The hand-over is based on the ingesters state stored in the ring. Each ingester could be in one of the following &lt;strong>states&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>PENDING&lt;/code>&lt;/li>
&lt;li>&lt;code>JOINING&lt;/code>&lt;/li>
&lt;li>&lt;code>ACTIVE&lt;/code>&lt;/li>
&lt;li>&lt;code>LEAVING&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>On startup, an ingester goes into the &lt;strong>&lt;code>PENDING&lt;/code>&lt;/strong> state.
In this state, the ingester is waiting for a hand-over from another ingester that is &lt;code>LEAVING&lt;/code>.
If no hand-over occurs within the configured timeout period (&amp;ldquo;auto-join timeout&amp;rdquo;, configurable via &lt;code>-ingester.join-after&lt;/code> option), the ingester will join the ring with a new set of random tokens (eg. during a scale up) and will switch its state to &lt;code>ACTIVE&lt;/code>.&lt;/p>
&lt;p>When a running ingester in the &lt;strong>&lt;code>ACTIVE&lt;/code>&lt;/strong> state is notified to shutdown via &lt;code>SIGINT&lt;/code> or &lt;code>SIGTERM&lt;/code> Unix signal, the ingester switches to &lt;code>LEAVING&lt;/code> state. In this state it cannot receive write requests anymore, but it can still receive read requests for series it has in memory.&lt;/p>
&lt;p>A &lt;strong>&lt;code>LEAVING&lt;/code>&lt;/strong> ingester looks for a &lt;code>PENDING&lt;/code> ingester to start a hand-over process with.
If it finds one, that ingester goes into the &lt;code>JOINING&lt;/code> state and the leaver transfers all its in-memory data over to the joiner.
On successful transfer the leaver removes itself from the ring and exits, while the joiner changes its state to &lt;code>ACTIVE&lt;/code>, taking over ownership of the leaver&amp;rsquo;s &lt;a href="/docs/architecture/#hashing">ring tokens&lt;/a>. As soon as the joiner switches it state to &lt;code>ACTIVE&lt;/code>, it will start receive both write requests from distributors and queries from queriers.&lt;/p>
&lt;p>If the &lt;code>LEAVING&lt;/code> ingester does not find a &lt;code>PENDING&lt;/code> ingester after &lt;code>-ingester.max-transfer-retries&lt;/code> retries, it will flush all of its chunks to the long-term storage, then removes itself from the ring and exits. The chunks flushing to the storage may take several minutes to complete.&lt;/p>
&lt;h4 id="higher-number-of-series--chunks-during-rolling-updates">Higher number of series / chunks during rolling updates&lt;/h4>
&lt;p>During hand-over, neither the leaving nor joining ingesters will
accept new samples. Distributors are aware of this, and &amp;ldquo;spill&amp;rdquo; the
samples to the next ingester in the ring. This creates a set of extra
&amp;ldquo;spilled&amp;rdquo; series and chunks which will idle out and flush after hand-over is
complete.&lt;/p>
&lt;h4 id="observability">Observability&lt;/h4>
&lt;p>The following metrics can be used to observe this process:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;code>cortex_member_ring_tokens_owned&lt;/code>&lt;/strong>&lt;br />
How many tokens each ingester thinks it owns.&lt;/li>
&lt;li>&lt;strong>&lt;code>cortex_ring_tokens_owned&lt;/code>&lt;/strong>&lt;br />
How many tokens each ingester is seen to own by other components.&lt;/li>
&lt;li>&lt;strong>&lt;code>cortex_ring_member_ownership_percent&lt;/code>&lt;/strong>&lt;br />
Same as &lt;code>cortex_ring_tokens_owned&lt;/code> but expressed as a percentage.&lt;/li>
&lt;li>&lt;strong>&lt;code>cortex_ring_members&lt;/code>&lt;/strong>&lt;br />
How many ingesters can be seen in each state, by other components.&lt;/li>
&lt;li>&lt;strong>&lt;code>cortex_ingester_sent_chunks&lt;/code>&lt;/strong>&lt;br />
Number of chunks sent by leaving ingester.&lt;/li>
&lt;li>&lt;strong>&lt;code>cortex_ingester_received_chunks&lt;/code>&lt;/strong>&lt;br />
Number of chunks received by joining ingester.&lt;/li>
&lt;/ul>
&lt;p>You can see the current state of the ring via http browser request to
&lt;code>/ring&lt;/code> on a distributor.&lt;/p></description></item><item><title>Docs: Ingesters scaling up and down</title><link>/docs/guides/ingesters-scaling-up-and-down/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/ingesters-scaling-up-and-down/</guid><description>
&lt;p>This guide explains how to scale up and down ingesters.&lt;/p>
&lt;p>&lt;em>If you&amp;rsquo;re looking how to run ingesters rolling updates, please refer to the &lt;a href="/docs/guides/ingesters-rolling-updates/">dedicated guide&lt;/a>.&lt;/em>&lt;/p>
&lt;h2 id="scaling-up">Scaling up&lt;/h2>
&lt;p>Adding more ingesters to a Cortex cluster is considered a safe operation. When a new ingester starts, it will register to the &lt;a href="/docs/architecture/#the-hash-ring">hash ring&lt;/a> and the distributors will reshard received series accordingly.&lt;/p>
&lt;p>No special care is required to take when scaling up ingesters.&lt;/p>
&lt;h2 id="scaling-down">Scaling down&lt;/h2>
&lt;p>A running ingester holds several hours of time series data in memory, before they&amp;rsquo;re flushed to the long-term storage. When an ingester shuts down, because of a scale down operation, the in-memory data must not be discarded in order to avoid any data loss.&lt;/p>
&lt;p>The procedure to adopt when scaling down ingesters depends on your Cortex setup:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#blocks-storage">Blocks storage&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chunks-storage-with-wal-enabled">Chunks storage with WAL enabled&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chunks-storage-with-wal-disabled-hand-over">Chunks storage with WAL disabled&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="blocks-storage">Blocks storage&lt;/h3>
&lt;p>When Cortex is running the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>, ingesters don&amp;rsquo;t flush series to blocks at shutdown by default. However, Cortex ingesters expose an API endpoint &lt;a href="/docs/api/#shutdown">&lt;code>/shutdown&lt;/code>&lt;/a> that can be called to flush series to blocks and upload blocks to the long-term storage before the ingester terminates.&lt;/p>
&lt;p>Even if ingester blocks are compacted and shipped to the storage at shutdown, it takes some time for queriers and store-gateways to discover the newly uploaded blocks. This is due to the fact that the blocks storage runs a periodic scanning of the storage bucket to discover blocks. If two or more ingesters are scaled down in a short period of time, queriers may miss some data at query time due to series that were stored in the terminated ingesters but their blocks haven&amp;rsquo;t been discovered yet.&lt;/p>
&lt;p>The ingesters scale down is deemed an infrequent operation and no automation is currently provided. However, if you need to scale down ingesters, please be aware of the following:&lt;/p>
&lt;ul>
&lt;li>Configure queriers and rulers to always query the storage
&lt;ul>
&lt;li>&lt;code>-querier.query-store-after=0s&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Frequently scan the storage bucket
&lt;ul>
&lt;li>&lt;code>-blocks-storage.bucket-store.sync-interval=5m&lt;/code>&lt;/li>
&lt;li>&lt;code>-compactor.cleanup-interval=5m&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Lower bucket scanning cache TTLs
&lt;ul>
&lt;li>&lt;code>-blocks-storage.bucket-store.metadata-cache.bucket-index-content-ttl=1m&lt;/code>&lt;/li>
&lt;li>&lt;code>-blocks-storage.bucket-store.metadata-cache.tenant-blocks-list-ttl=1m&lt;/code>&lt;/li>
&lt;li>&lt;code>-blocks-storage.bucket-store.metadata-cache.metafile-doesnt-exist-ttl=1m&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ingesters should be scaled down one by one:
&lt;ol>
&lt;li>Call &lt;code>/shutdown&lt;/code> endpoint on the ingester to shutdown&lt;/li>
&lt;li>Wait until the HTTP call returns successfully or &amp;ldquo;finished flushing and shipping TSDB blocks&amp;rdquo; is logged&lt;/li>
&lt;li>Terminate the ingester process (the &lt;code>/shutdown&lt;/code> will not do it)&lt;/li>
&lt;li>Before proceeding to the next ingester, wait 2x the maximum between &lt;code>-blocks-storage.bucket-store.sync-interval&lt;/code> and &lt;code>-compactor.cleanup-interval&lt;/code>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="chunks-storage-with-wal-enabled">Chunks storage with WAL enabled&lt;/h3>
&lt;p>When Cortex is running the &lt;a href="/docs/chunks-storage/">chunks storage&lt;/a> with WAL enabled, ingesters don&amp;rsquo;t flush series chunks to storage at shutdown by default. However, Cortex ingesters expose an API endpoint &lt;a href="/docs/api/#shutdown">&lt;code>/shutdown&lt;/code>&lt;/a> that can be called to flush chunks to the long-term storage before the ingester terminates.&lt;/p>
&lt;p>The procedure to scale down ingesters &amp;ndash; one by one &amp;ndash; should be:&lt;/p>
&lt;ol>
&lt;li>Call &lt;code>/shutdown&lt;/code> endpoint on the ingester to shutdown&lt;/li>
&lt;li>Wait until the HTTP call returns successfully or &amp;ldquo;flushing of chunks complete&amp;rdquo; is logged&lt;/li>
&lt;li>Terminate the ingester process (the &lt;code>/shutdown&lt;/code> will not do it)&lt;/li>
&lt;/ol>
&lt;p>&lt;em>For more information about the chunks storage WAL, please refer to &lt;a href="/docs/chunks-storage/ingesters-with-wal/">Ingesters with WAL&lt;/a>.&lt;/em>&lt;/p>
&lt;h3 id="chunks-storage-with-wal-disabled">Chunks storage with WAL disabled&lt;/h3>
&lt;p>When Cortex is running the chunks storage with WAL disabled, ingesters flush series chunks to the storage at shutdown if no &lt;code>PENDING&lt;/code> ingester (to transfer series to) is found. Because of this, it&amp;rsquo;s safe to scale down ingesters with no special care in this setup.&lt;/p></description></item><item><title>Docs: Overrides Exporter</title><link>/docs/guides/overrides-exporter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/overrides-exporter/</guid><description>
&lt;p>Since Cortex is a multi-tenant system, it supports applying limits to each tenant to prevent
any single one from using too many resources. In order to help operators understand how close
to their limits tenants are, the &lt;code>overrides-exporter&lt;/code> module can expose limits as Prometheus metrics.&lt;/p>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>To update configuration without restarting, Cortex allows operators to supply a &lt;code>runtime_config&lt;/code>
file that will be periodically reloaded. This file can be specified under the &lt;code>runtime_config&lt;/code> section
of the main &lt;a href="/docs/configuration/arguments/#runtime-configuration-file">configuration file&lt;/a> or using the &lt;code>-runtime-config.file&lt;/code>
command line flag. This file is used to apply tenant-specific limits.&lt;/p>
&lt;h2 id="example">Example&lt;/h2>
&lt;p>The &lt;code>overrides-exporter&lt;/code> is not enabled by default, it must be explicitly enabled. We recommend
only running a single instance of it in your cluster due to the cardinality of the metrics
emitted.&lt;/p>
&lt;p>With a &lt;code>runtime.yaml&lt;/code> file given below&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#8f5902;font-style:italic"># file: runtime.yaml&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># In this example, we&amp;#39;re overriding ingestion limits for a single tenant.&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">overrides&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#34;user1&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">ingestion_burst_size&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">350000&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">ingestion_rate&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">350000&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">max_global_series_per_metric&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">300000&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">max_global_series_per_user&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">300000&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">max_series_per_metric&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">max_series_per_user&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">max_samples_per_query&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">100000&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">max_series_per_query&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">100000&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>overrides-exporter&lt;/code> is configured to run as follows&lt;/p>
&lt;pre>&lt;code>cortex -target overrides-exporter -runtime-config.file runtime.yaml -server.http-listen-port=8080
&lt;/code>&lt;/pre>&lt;p>After the &lt;code>overrides-exporter&lt;/code> starts, you can to use &lt;code>curl&lt;/code> to inspect the tenant overrides.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">curl -s http://localhost:8080/metrics | grep cortex_overrides
# HELP cortex_overrides Resource limit overrides applied to tenants
# TYPE cortex_overrides gauge
cortex_overrides{limit_name=&amp;#34;ingestion_burst_size&amp;#34;,user=&amp;#34;user1&amp;#34;} 350000
cortex_overrides{limit_name=&amp;#34;ingestion_rate&amp;#34;,user=&amp;#34;user1&amp;#34;} 350000
cortex_overrides{limit_name=&amp;#34;max_global_series_per_metric&amp;#34;,user=&amp;#34;user1&amp;#34;} 300000
cortex_overrides{limit_name=&amp;#34;max_global_series_per_user&amp;#34;,user=&amp;#34;user1&amp;#34;} 300000
cortex_overrides{limit_name=&amp;#34;max_local_series_per_metric&amp;#34;,user=&amp;#34;user1&amp;#34;} 0
cortex_overrides{limit_name=&amp;#34;max_local_series_per_user&amp;#34;,user=&amp;#34;user1&amp;#34;} 0
cortex_overrides{limit_name=&amp;#34;max_samples_per_query&amp;#34;,user=&amp;#34;user1&amp;#34;} 100000
cortex_overrides{limit_name=&amp;#34;max_series_per_query&amp;#34;,user=&amp;#34;user1&amp;#34;} 100000
&lt;/code>&lt;/pre>&lt;/div>&lt;p>With these metrics, you can set up alerts to know when tenants are close to hitting their limits
before they exceed them.&lt;/p></description></item><item><title>Docs: Securing communication between Cortex components with TLS</title><link>/docs/guides/tls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/tls/</guid><description>
&lt;p>Cortex is a distributed system with significant traffic between its services.
To allow for secure communication, Cortex supports TLS between all its
components. This guide describes the process of setting up TLS.&lt;/p>
&lt;h3 id="generation-of-certs-to-configure-tls">Generation of certs to configure TLS&lt;/h3>
&lt;p>The first step to securing inter-service communication in Cortex with TLS is
generating certificates. A Certifying Authority (CA) will be used for this
purpose which should be private to the organization, as any certificates signed
by this CA will have permissions to communicate with the cluster.&lt;/p>
&lt;p>We will use the following script to generate self signed certs for the cluster:&lt;/p>
&lt;pre>&lt;code># keys
openssl genrsa -out root.key
openssl genrsa -out client.key
openssl genrsa -out server.key
# root cert / certifying authority
openssl req -x509 -new -nodes -key root.key -subj &amp;quot;/C=US/ST=KY/O=Org/CN=root&amp;quot; -sha256 -days 100000 -out root.crt
# csrs - certificate signing requests
openssl req -new -sha256 -key client.key -subj &amp;quot;/C=US/ST=KY/O=Org/CN=client&amp;quot; -out client.csr
openssl req -new -sha256 -key server.key -subj &amp;quot;/C=US/ST=KY/O=Org/CN=localhost&amp;quot; -out server.csr
# certificates
openssl x509 -req -in client.csr -CA root.crt -CAkey root.key -CAcreateserial -out client.crt -days 100000 -sha256
openssl x509 -req -in server.csr -CA root.crt -CAkey root.key -CAcreateserial -out server.crt -days 100000 -sha256
&lt;/code>&lt;/pre>&lt;p>Note that the above script generates certificates that are valid for 100000 days.
This can be changed by adjusting the &lt;code>-days&lt;/code> option in the above commands.
It is recommended that the certs be replaced atleast once every 2 years.&lt;/p>
&lt;p>The above script generates keys &lt;code>client.key, server.key&lt;/code> and certs
&lt;code>client.crt, server.crt&lt;/code> for both the client and server. The CA cert is
generated as &lt;code>root.crt&lt;/code>.&lt;/p>
&lt;h3 id="load-certs-into-the-httpgrpc-serverclient">Load certs into the HTTP/GRPC server/client&lt;/h3>
&lt;p>Every HTTP/GRPC link between Cortex components supports TLS configuration
through the following config parameters:&lt;/p>
&lt;h4 id="server-flags">Server flags&lt;/h4>
&lt;pre>&lt;code> # Path to the TLS Cert for the HTTP Server
-server.http-tls-cert-path=/path/to/server.crt
# Path to the TLS Key for the HTTP Server
-server.http-tls-key-path=/path/to/server.key
# Type of Client Auth for the HTTP Server
-server.http-tls-client-auth=&amp;quot;RequireAndVerifyClientCert&amp;quot;
# Path to the Client CA Cert for the HTTP Server
-server.http-tls-ca-path=&amp;quot;/path/to/root.crt&amp;quot;
# Path to the TLS Cert for the GRPC Server
-server.grpc-tls-cert-path=/path/to/server.crt
# Path to the TLS Key for the GRPC Server
-server.grpc-tls-key-path=/path/to/server.key
# Type of Client Auth for the GRPC Server
-server.grpc-tls-client-auth=&amp;quot;RequireAndVerifyClientCert&amp;quot;
# Path to the Client CA Cert for the GRPC Server
-server.grpc-tls-ca-path=/path/to/root.crt
&lt;/code>&lt;/pre>&lt;h4 id="client-flags">Client flags&lt;/h4>
&lt;p>Client flags are component specific.&lt;/p>
&lt;p>For an HTTP client in the Alertmanager:&lt;/p>
&lt;pre>&lt;code> # Path to the TLS Cert for the HTTP Client
-alertmanager.configs.tls-cert-path=/path/to/client.crt
# Path to the TLS Key for the HTTP Client
-alertmanager.configs.tls-key-path=/path/to/client.key
# Path to the TLS CA for the HTTP Client
-alertmanager.configs.tls-ca-path=/path/to/root.crt
&lt;/code>&lt;/pre>&lt;p>For a GRPC client in the Querier:&lt;/p>
&lt;pre>&lt;code> # Path to the TLS Cert for the GRPC Client
-querier.frontend-client.tls-cert-path=/path/to/client.crt
# Path to the TLS Key for the GRPC Client
-querier.frontend-client.tls-key-path=/path/to/client.key
# Path to the TLS CA for the GRPC Client
-querier.frontend-client.tls-ca-path=/path/to/root.crt
&lt;/code>&lt;/pre>&lt;p>Similarly, for the GRPC Ingester Client:&lt;/p>
&lt;pre>&lt;code> # Path to the TLS Cert for the GRPC Client
-ingester.client.tls-cert-path=/path/to/client.crt
# Path to the TLS Key for the GRPC Client
-ingester.client.tls-key-path=/path/to/client.key
# Path to the TLS CA for the GRPC Client
-ingester.client.tls-ca-path=/path/to/root.crt
&lt;/code>&lt;/pre>&lt;p>TLS can be configured in a similar fashion for other HTTP/GRPC clients in Cortex.&lt;/p></description></item><item><title>Docs: Shuffle Sharding</title><link>/docs/guides/shuffle-sharding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/shuffle-sharding/</guid><description>
&lt;p>Cortex leverages on sharding techniques to horizontally scale both single and multi-tenant clusters beyond the capacity of a single node.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>The &lt;strong>default sharding strategy&lt;/strong> employed by Cortex distributes the workload across the entire pool of instances running a given service (eg. ingesters). For example, on the write path each tenant&amp;rsquo;s series are sharded across all ingesters, regardless how many active series the tenant has or how many different tenants are in the cluster.&lt;/p>
&lt;p>The default strategy allows to have a fair balance on the resources consumed by each instance (ie. CPU and memory) and to maximise these resources across the cluster.&lt;/p>
&lt;p>However, in a &lt;strong>multi-tenant&lt;/strong> cluster this approach also introduces some &lt;strong>downsides&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>An outage affects all tenants&lt;/li>
&lt;li>A misbehaving tenant (eg. causing out of memory) could affect all other tenants&lt;/li>
&lt;/ol>
&lt;p>The goal of &lt;strong>shuffle sharding&lt;/strong> is to provide an alternative sharding strategy to reduce the blast radius of an outage and better isolate tenants.&lt;/p>
&lt;h2 id="what-is-shuffle-sharding">What is shuffle sharding&lt;/h2>
&lt;p>Shuffle sharding is a technique used to isolate different tenant&amp;rsquo;s workloads and to give each tenant a single-tenant experience even if they&amp;rsquo;re running in a shared cluster. This technique has been publicly shared and clearly explained by AWS in their &lt;a href="https://aws.amazon.com/builders-library/workload-isolation-using-shuffle-sharding/">builders&amp;rsquo; library&lt;/a> and a reference implementation has been shown in the &lt;a href="https://github.com/awslabs/route53-infima/blob/master/src/main/java/com/amazonaws/services/route53/infima/SimpleSignatureShuffleSharder.java">Route53 Infima library&lt;/a>.&lt;/p>
&lt;p>The idea is to assign each tenant a shard composed by a subset of the Cortex service instances, aiming to minimize the overlapping instances between two different tenants. Shuffle sharding brings the following &lt;strong>benefits&lt;/strong> over the default sharding strategy:&lt;/p>
&lt;ul>
&lt;li>An outage on some Cortex cluster instances/nodes will only affect a subset of tenants.&lt;/li>
&lt;li>A misbehaving tenant will affect only its shard instances. Due to the low overlap of instances between different tenants, it&amp;rsquo;s statistically quite likely that any other tenant will run on different instances or only a subset of instances will match the affected ones.&lt;/li>
&lt;/ul>
&lt;p>Shuffle sharding requires no more resources than the default sharding strategy but instances may be less evenly balanced from time to time.&lt;/p>
&lt;h3 id="low-overlapping-instances-probability">Low overlapping instances probability&lt;/h3>
&lt;p>For example, given a Cortex cluster running &lt;strong>50 ingesters&lt;/strong> and assigning &lt;strong>each tenant 4&lt;/strong> out of 50 ingesters, shuffling instances between each tenant, we get &lt;strong>230K possible combinations&lt;/strong>.&lt;/p>
&lt;p>Randomly picking two different tenants we have the:&lt;/p>
&lt;ul>
&lt;li>71% chance that they will not share any instance&lt;/li>
&lt;li>26% chance that they will share only 1 instance&lt;/li>
&lt;li>2.7% chance that they will share 2 instances&lt;/li>
&lt;li>0.08% chance that they will share 3 instances&lt;/li>
&lt;li>Only a 0.0004% chance that their instances will fully overlap&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="/images/guides/shuffle-sharding-probability.png" alt="Shuffle sharding probability">&lt;/p>
&lt;!-- Chart source at https://docs.google.com/spreadsheets/d/1FXbiWTXi6bdERtamH-IfmpgFq1fNL4GP_KX_yJvbRi4/edit -->
&lt;h2 id="cortex-shuffle-sharding">Cortex shuffle sharding&lt;/h2>
&lt;p>Cortex currently supports shuffle sharding in the following services:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#ingesters-shuffle-sharding">Ingesters&lt;/a>&lt;/li>
&lt;li>&lt;a href="#query-frontend-shuffle-sharding">Query-frontend&lt;/a>&lt;/li>
&lt;li>&lt;a href="#store-gateway-shuffle-sharding">Store-gateway&lt;/a>&lt;/li>
&lt;li>&lt;a href="#ruler-shuffle-sharding">Ruler&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Shuffle sharding is &lt;strong>disabled by default&lt;/strong> and needs to be explicitly enabled in the configuration.&lt;/p>
&lt;h3 id="guaranteed-properties">Guaranteed properties&lt;/h3>
&lt;p>The Cortex shuffle sharding implementation guarantees the following properties:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Stability&lt;/strong>&lt;br />
Given a consistent state of the hash ring, the shuffle sharding algorithm always selects the same instances for a given tenant, even across different machines.&lt;/li>
&lt;li>&lt;strong>Consistency&lt;/strong>&lt;br />
Adding or removing 1 instance from the hash ring leads to only 1 instance changed at most, in each tenant&amp;rsquo;s shard.&lt;/li>
&lt;li>&lt;strong>Shuffling&lt;/strong>&lt;br />
Probabilistically and for a large enough cluster, it ensures that every tenant gets a different set of instances, with a reduced number of overlapping instances between two tenants to improve failure isolation.&lt;/li>
&lt;li>&lt;strong>Zone-awareness&lt;/strong>&lt;br />
When &lt;a href="/docs/guides/zone-aware-replication/">zone-aware replication&lt;/a> is enabled, the subset of instances selected for each tenant contains a balanced number of instances for each availability zone.&lt;/li>
&lt;/ul>
&lt;h3 id="ingesters-shuffle-sharding">Ingesters shuffle sharding&lt;/h3>
&lt;p>By default the Cortex distributor spreads the received series across all running ingesters.&lt;/p>
&lt;p>When shuffle sharding is &lt;strong>enabled&lt;/strong> via &lt;code>-distributor.sharding-strategy=shuffle-sharding&lt;/code> (or its respective YAML config option), the distributor spreads each tenant series across &lt;code>-distributor.ingestion-tenant-shard-size&lt;/code> number of ingesters.&lt;/p>
&lt;p>&lt;em>The shard size can be overridden on a per-tenant basis in the limits overrides configuration.&lt;/em>&lt;/p>
&lt;h3 id="query-frontend-shuffle-sharding">Query-frontend shuffle sharding&lt;/h3>
&lt;p>By default all Cortex queriers can execute received queries for given tenant.&lt;/p>
&lt;p>When shuffle sharding is &lt;strong>enabled&lt;/strong> by setting &lt;code>-frontend.max-queriers-per-tenant&lt;/code> (or its respective YAML config option) to a value higher than 0 and lower than the number of available queriers, only specified number of queriers will execute queries for single tenant. Note that this distribution happens in query-frontend, or query-scheduler if used. When using query-scheduler, &lt;code>-frontend.max-queriers-per-tenant&lt;/code> option must be set for query-scheduler component. When not using query-frontend (with or without scheduler), this option is not available.&lt;/p>
&lt;p>&lt;em>The maximum number of queriers can be overridden on a per-tenant basis in the limits overrides configuration.&lt;/em>&lt;/p>
&lt;h3 id="store-gateway-shuffle-sharding">Store-gateway shuffle sharding&lt;/h3>
&lt;p>The Cortex store-gateway &amp;ndash; used by the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> &amp;ndash; by default spreads each tenant&amp;rsquo;s blocks across all running store-gateways.&lt;/p>
&lt;p>When shuffle sharding is &lt;strong>enabled&lt;/strong> via &lt;code>-store-gateway.sharding-strategy=shuffle-sharding&lt;/code> (or its respective YAML config option), each tenant blocks will be sharded across a subset of &lt;code>-store-gateway.tenant-shard-size&lt;/code> store-gateway instances.&lt;/p>
&lt;p>&lt;em>The shard size can be overridden on a per-tenant basis setting &lt;code>store_gateway_tenant_shard_size&lt;/code> in the limits overrides configuration.&lt;/em>&lt;/p>
&lt;p>&lt;em>Please check out the &lt;a href="/docs/blocks-storage/store-gateway/">store-gateway documentation&lt;/a> for more information about how it works.&lt;/em>&lt;/p>
&lt;h3 id="ruler-shuffle-sharding">Ruler shuffle sharding&lt;/h3>
&lt;p>Cortex ruler can run in three modes:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>No sharding at all.&lt;/strong> This is the most basic mode of the ruler. It is activated by using &lt;code>-ruler.enable-sharding=false&lt;/code> (default) and works correctly only if single ruler is running. In this mode the Ruler loads all rules for all tenants.&lt;/li>
&lt;li>&lt;strong>Default sharding&lt;/strong>, activated by using &lt;code>-ruler.enable-sharding=true&lt;/code> and &lt;code>-ruler.sharding-strategy=default&lt;/code> (default). In this mode rulers register themselves into the ring. Each ruler will then select and evaluate only those rules that it &amp;ldquo;owns&amp;rdquo;.&lt;/li>
&lt;li>&lt;strong>Shuffle sharding&lt;/strong>, activated by using &lt;code>-ruler.enable-sharding=true&lt;/code> and &lt;code>-ruler.sharding-strategy=shuffle-sharding&lt;/code>. Similarly to default sharding, rulers use the ring to distribute workload, but rule groups for each tenant can only be evaluated on limited number of rulers (&lt;code>-ruler.tenant-shard-size&lt;/code>, can also be set per tenant as &lt;code>ruler_tenant_shard_size&lt;/code> in overrides).&lt;/li>
&lt;/ol>
&lt;p>Note that when using sharding strategy, each rule group is evaluated by single ruler only, there is no replication.&lt;/p>
&lt;h2 id="faq">FAQ&lt;/h2>
&lt;h3 id="does-shuffle-sharding-add-additional-overhead-to-the-kv-store">Does shuffle sharding add additional overhead to the KV store?&lt;/h3>
&lt;p>No, shuffle sharding subrings are computed client-side and are not stored in the ring. KV store sizing still depends primarily on the number of replicas (of any component that uses the ring, e.g. ingesters) and tokens per replica.&lt;/p>
&lt;p>However, each tenant&amp;rsquo;s subring is cached in memory on the client-side which may slightly increase the memory footprint of certain components (mostly the distributor).&lt;/p></description></item><item><title>Docs: Tracing</title><link>/docs/guides/tracing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/tracing/</guid><description>
&lt;p>Cortex uses &lt;a href="https://www.jaegertracing.io/">Jaeger&lt;/a> to implement distributed
tracing. We have found Jaeger invaluable for troubleshooting the behavior of
Cortex in production.&lt;/p>
&lt;h2 id="dependencies">Dependencies&lt;/h2>
&lt;p>In order to send traces you will need to set up a Jaeger deployment. A
deployment includes either the jaeger all-in-one binary, or else a distributed
system of agents, collectors, and queriers. If running on Kubernetes, &lt;a href="https://github.com/jaegertracing/jaeger-kubernetes">Jaeger
Kubernetes&lt;/a> is an excellent
resource.&lt;/p>
&lt;h2 id="configuration">Configuration&lt;/h2>
&lt;p>In order to configure Cortex to send traces you must do two things:&lt;/p>
&lt;ol>
&lt;li>Set the &lt;code>JAEGER_AGENT_HOST&lt;/code> environment variable in all components to point
to your Jaeger agent. This defaults to &lt;code>localhost&lt;/code>.&lt;/li>
&lt;li>Enable sampling in the appropriate components:
&lt;ul>
&lt;li>The Ingester and Ruler self-initiate traces and should have sampling
explicitly enabled.&lt;/li>
&lt;li>Sampling for the Distributor and Query Frontend can be enabled in Cortex
or in an upstream service such as your frontdoor.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>To enable sampling in Cortex components you can specify either
&lt;code>JAEGER_SAMPLER_MANAGER_HOST_PORT&lt;/code> for remote sampling, or
&lt;code>JAEGER_SAMPLER_TYPE&lt;/code> and &lt;code>JAEGER_SAMPLER_PARAM&lt;/code> to manually set sampling
configuration. See the &lt;a href="https://github.com/jaegertracing/jaeger-client-go#environment-variables">Jaeger Client Go
documentation&lt;/a>
for the full list of environment variables you can configure.&lt;/p>
&lt;p>Note that you must specify one of &lt;code>JAEGER_AGENT_HOST&lt;/code> or
&lt;code>JAEGER_SAMPLER_MANAGER_HOST_PORT&lt;/code> in each component for Jaeger to be enabled,
even if you plan to use the default values.&lt;/p></description></item><item><title>Docs: Zone Aware Replication</title><link>/docs/guides/zone-aware-replication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/zone-aware-replication/</guid><description>
&lt;p>Cortex supports data replication for different services. By default, data is transparently replicated across the whole pool of service instances, regardless of whether these instances are all running within the same availability zone (or data center, or rack) or in different ones.&lt;/p>
&lt;p>It is completely possible that all the replicas for the given data are held within the same availability zone, even if the Cortex cluster spans multiple zones. Storing multiple replicas for a given data within the same availability zone poses a risk for data loss if there is an outage affecting various nodes within a zone or a full zone outage.&lt;/p>
&lt;p>For this reason, Cortex optionally supports zone-aware replication. When zone-aware replication is &lt;strong>enabled&lt;/strong>, replicas for the given data are guaranteed to span across different availability zones. This requires Cortex cluster to run at least in a number of zones equal to the configured replication factor.&lt;/p>
&lt;p>Reads from a zone-aware replication enabled Cortex Cluster can withstand zone failures as long as there are no more than &lt;code>floor(replication factor / 2)&lt;/code> zones with failing instances.&lt;/p>
&lt;p>The Cortex services supporting &lt;strong>zone-aware replication&lt;/strong> are:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="#distributors-and-ingesters-time-series-replication">Distributors and Ingesters&lt;/a>&lt;/strong>&lt;/li>
&lt;li>&lt;strong>&lt;a href="#store-gateways-blocks-replication">Store-gateways&lt;/a>&lt;/strong> (&lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> only)&lt;/li>
&lt;/ul>
&lt;h2 id="distributors--ingesters-time-series-replication">Distributors / Ingesters: time-series replication&lt;/h2>
&lt;p>The Cortex time-series replication is used to hold multiple (typically 3) replicas of each time series in the &lt;strong>ingesters&lt;/strong>.&lt;/p>
&lt;p>&lt;strong>To enable&lt;/strong> the zone-aware replication for the ingesters you should:&lt;/p>
&lt;ol>
&lt;li>Configure the availability zone for each ingester via the &lt;code>-ingester.availability-zone&lt;/code> CLI flag (or its respective YAML config option)&lt;/li>
&lt;li>Rollout ingesters to apply the configured zone&lt;/li>
&lt;li>Enable time-series zone-aware replication via the &lt;code>-distributor.zone-awareness-enabled&lt;/code> CLI flag (or its respective YAML config option). Please be aware this configuration option should be set to distributors, queriers and rulers.&lt;/li>
&lt;/ol>
&lt;p>The &lt;code>-distributor.shard-by-all-labels&lt;/code> setting has an impact on read availability. When enabled, a metric is sharded across all ingesters and querier needs to fetch series from all ingesters while, when disabled, a metric is sharded only across &lt;code>&amp;lt;replication factor&amp;gt;&lt;/code> ingesters.&lt;/p>
&lt;p>In the event of a large outage impacting ingesters in more than 1 zone, when &lt;code>-distributor.shard-by-all-labels=true&lt;/code> all queries will fail, while when disabled some queries may still succeed if the ingesters holding the required metric are not impacted by the outage.&lt;/p>
&lt;h2 id="store-gateways-blocks-replication">Store-gateways: blocks replication&lt;/h2>
&lt;p>The Cortex &lt;a href="/docs/blocks-storage/store-gateway/">store-gateway&lt;/a> (used only when Cortex is running with the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>) supports blocks sharding, used to horizontally scale blocks in a large cluster without hitting any vertical scalability limit.&lt;/p>
&lt;p>To enable the zone-aware replication for the store-gateways, please refer to the &lt;a href="/docs/blocks-storage/store-gateway/#zone-awareness">store-gateway&lt;/a> documentation.&lt;/p>
&lt;h2 id="minimum-number-of-zones">Minimum number of zones&lt;/h2>
&lt;p>For Cortex to function correctly, there must be at least the same number of availability zones as the replication factor. For example, if the replication factor is configured to 3 (default for time-series replication), the Cortex cluster should be spread at least over 3 availability zones.&lt;/p>
&lt;p>It is safe to have more zones than the replication factor, but it cannot be less. Having fewer availability zones than replication factor causes a replica write to be missed, and in some cases, the write fails if the availability zones count is too low.&lt;/p>
&lt;h2 id="impact-on-unbalanced-zones">Impact on unbalanced zones&lt;/h2>
&lt;p>&lt;strong>Cortex requires that each zone runs the same number of instances&lt;/strong> of a given service for which the zone-aware replication is enabled. This guarantees a fair split of the workload across zones.&lt;/p>
&lt;p>On the contrary, if zones are unbalanced, the zones with a lower number of instances would have an higher pressure on resources utilization (eg. CPU and memory) compared to zones with an higher number of instances.&lt;/p>
&lt;h2 id="impact-on-costs">Impact on costs&lt;/h2>
&lt;p>Depending on the underlying infrastructure being used, deploying Cortex across multiple availability zones may cause an increase in running costs as most cloud providers charge for inter availability zone networking. The most significant change would be for a Cortex cluster currently running in a single zone.&lt;/p></description></item><item><title>Docs: Limitations</title><link>/docs/guides/limitations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/limitations/</guid><description>
&lt;h2 id="tenant-id-naming">Tenant ID naming&lt;/h2>
&lt;p>The tenant ID (also called &amp;ldquo;user ID&amp;rdquo; or &amp;ldquo;org ID&amp;rdquo;) is the unique identifier of a tenant within a Cortex cluster. The tenant ID is an opaque information to Cortex, which doesn&amp;rsquo;t make any assumption on its format/content, but its naming has two limitations:&lt;/p>
&lt;ol>
&lt;li>Supported characters&lt;/li>
&lt;li>Length&lt;/li>
&lt;/ol>
&lt;h3 id="supported-characters">Supported characters&lt;/h3>
&lt;p>The following character sets are generally &lt;strong>safe for use in the tenant ID&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Alphanumeric characters
&lt;ul>
&lt;li>&lt;code>0-9&lt;/code>&lt;/li>
&lt;li>&lt;code>a-z&lt;/code>&lt;/li>
&lt;li>&lt;code>A-Z&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Special characters
&lt;ul>
&lt;li>Exclamation point (&lt;code>!&lt;/code>)&lt;/li>
&lt;li>Hyphen (&lt;code>-&lt;/code>)&lt;/li>
&lt;li>Underscore (&lt;code>_&lt;/code>)&lt;/li>
&lt;li>Period (&lt;code>.&lt;/code>)&lt;/li>
&lt;li>Asterisk (&lt;code>*&lt;/code>)&lt;/li>
&lt;li>Single quote (&lt;code>'&lt;/code>)&lt;/li>
&lt;li>Open parenthesis (&lt;code>(&lt;/code>)&lt;/li>
&lt;li>Close parenthesis (&lt;code>)&lt;/code>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>All other characters are not safe to use. In particular, slashes &lt;code>/&lt;/code> and whitespaces (&lt;code> &lt;/code>) are &lt;strong>not supported&lt;/strong>.&lt;/p>
&lt;h3 id="length">Length&lt;/h3>
&lt;p>The tenant ID length should not exceed 150 bytes/characters.&lt;/p>
&lt;h2 id="query-without-metric-name">Query without metric name&lt;/h2>
&lt;p>The Cortex chunks storage doesn&amp;rsquo;t support queries without a metric name, like &lt;code>count({__name__=~&amp;quot;.+&amp;quot;})&lt;/code>. On the contrary, the Cortex &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> supports it.&lt;/p>
&lt;h2 id="query-series-and-labels">Query series and labels&lt;/h2>
&lt;p>When running queries to the &lt;code>/api/v1/series&lt;/code>, &lt;code>/api/v1/labels&lt;/code> and &lt;code>/api/v1/label/{name}/values&lt;/code> endpoints, query&amp;rsquo;s time range is ignored and the data is always fetched from ingesters. There is experimental support to query the long-term store with the &lt;em>blocks&lt;/em> storage engine when &lt;code>-querier.query-store-for-labels-enabled&lt;/code> is set.&lt;/p></description></item><item><title>Docs: Glossary</title><link>/docs/guides/glossary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/glossary/</guid><description>
&lt;h3 id="blocks-storage">Blocks storage&lt;/h3>
&lt;p>The blocks storage is a Cortex storage engine based on Prometheus TSDB, which only requires an object store (eg. AWS S3, Google GCS, &amp;hellip;) as backend storage.&lt;/p>
&lt;p>For more information, please refer to the &lt;a href="/docs/blocks-storage/">Cortex blocks storage&lt;/a> documentation.&lt;/p>
&lt;h3 id="chunks-storage">Chunks storage&lt;/h3>
&lt;p>The chunks storage is a Cortex storage engine which requires both an index store (eg. AWS DynamoDB, Google BigTable, Cassandra, &amp;hellip;) and an object store (eg. AWS S3, Google GCS, &amp;hellip;) as backend storage.&lt;/p>
&lt;h3 id="chunk">Chunk&lt;/h3>
&lt;p>A chunk is an object containing compressed timestamp-value pairs.&lt;/p>
&lt;p>When running Cortex with the chunks storage, a single chunk object contains timestamp-value pairs for a single series, while when running Cortex with the &lt;a href="#blocks-storage">blocks storage&lt;/a> a single chunk contains timestamp-value pairs for several series.&lt;/p>
&lt;h3 id="churn">Churn&lt;/h3>
&lt;p>Churn is the frequency at which series become idle.&lt;/p>
&lt;p>A series become idle once it&amp;rsquo;s not exported anymore by the monitored targets. Typically, series become idle when the monitored target itself disappear (eg. the process or node gets terminated).&lt;/p>
&lt;h3 id="flushing">Flushing&lt;/h3>
&lt;p>Series flushing is the operation run by ingesters to offload time series from memory and store them in the long-term storage.&lt;/p>
&lt;h3 id="ha-tracker">HA Tracker&lt;/h3>
&lt;p>The HA Tracker is a feature of Cortex distributor which is used to deduplicate received series coming from two (or more) Prometheus servers configured in HA pairs.&lt;/p>
&lt;p>For more information, please refer to the guide &amp;ldquo;&lt;a href="/docs/guides/ha-pair-handling/">Config for sending HA Pairs data to Cortex&lt;/a>&amp;rdquo;.&lt;/p>
&lt;h3 id="hand-over">Hand-over&lt;/h3>
&lt;p>Series hand-over is an operation supported by ingesters to transfer their state, on shutdown, to a new ingester in the &lt;code>JOINING&lt;/code> state. Hand-over is typically used during &lt;a href="/docs/guides/ingesters-rolling-updates/">ingesters rollouts&lt;/a> and is only supported by the Cortex chunks storage.&lt;/p>
&lt;p>For more information, please refer to the guide &amp;ldquo;&lt;a href="/docs/guides/ingesters-rolling-updates/">Ingesters rolling updates&lt;/a>&amp;rdquo;.&lt;/p>
&lt;h3 id="hash-ring">Hash ring&lt;/h3>
&lt;p>The hash ring is a distributed data structure used by Cortex for sharding, replication and service discovery. The hash ring data structure gets shared across Cortex replicas via gossip or a key-value store.&lt;/p>
&lt;p>For more information, please refer to the &lt;a href="/docs/architecture/#the-hash-ring">Architecture&lt;/a> documentation.&lt;/p>
&lt;h3 id="org">Org&lt;/h3>
&lt;p>&lt;em>See &lt;a href="#tenant">Tenant&lt;/a>.&lt;/em>&lt;/p>
&lt;h3 id="ring">Ring&lt;/h3>
&lt;p>&lt;em>See &lt;a href="#hash-ring">Hash ring&lt;/a>.&lt;/em>&lt;/p>
&lt;h3 id="sample">Sample&lt;/h3>
&lt;p>A sample is a single timestamped value in a time series.&lt;/p>
&lt;p>For example, given the series &lt;code>node_cpu_seconds_total{instance=&amp;quot;10.0.0.1&amp;quot;,mode=&amp;quot;system&amp;quot;}&lt;/code> its stream of values (samples) could be:&lt;/p>
&lt;pre>&lt;code># Display format: &amp;lt;value&amp;gt; @&amp;lt;timestamp&amp;gt;
11775 @1603812134
11790 @1603812149
11805 @1603812164
11819 @1603812179
11834 @1603812194
&lt;/code>&lt;/pre>&lt;h3 id="schema-config">Schema config&lt;/h3>
&lt;p>The schema (or schema config) is a configuration file used by the Cortex chunks storage to configure the backend index and chunks store, and manage storage version upgrades. The schema config is &lt;strong>not&lt;/strong> used by the Cortex &lt;a href="#blocks-storage">blocks storage&lt;/a>.&lt;/p>
&lt;p>For more information, please refer to the &lt;a href="/docs/chunks-storage/schema-configuration/">Schema config reference&lt;/a>.&lt;/p>
&lt;h3 id="series">Series&lt;/h3>
&lt;p>In the Prometheus ecosystem, a series (or time series) is a single stream of timestamped values belonging to the same metric, with the same set of label key-value pairs.&lt;/p>
&lt;p>For example, given a single metric &lt;code>node_cpu_seconds_total&lt;/code> you may have multiple series, each one uniquely identified by the combination of metric name and unique label key-value pairs:&lt;/p>
&lt;pre>&lt;code>node_cpu_seconds_total{instance=&amp;quot;10.0.0.1&amp;quot;,mode=&amp;quot;system&amp;quot;}
node_cpu_seconds_total{instance=&amp;quot;10.0.0.1&amp;quot;,mode=&amp;quot;user&amp;quot;}
node_cpu_seconds_total{instance=&amp;quot;10.0.0.2&amp;quot;,mode=&amp;quot;system&amp;quot;}
node_cpu_seconds_total{instance=&amp;quot;10.0.0.2&amp;quot;,mode=&amp;quot;user&amp;quot;}
&lt;/code>&lt;/pre>&lt;h3 id="tenant">Tenant&lt;/h3>
&lt;p>A tenant (also called &amp;ldquo;user&amp;rdquo; or &amp;ldquo;org&amp;rdquo;) is the owner of a set of series written to and queried from Cortex. Cortex multi-tenancy support allows you to isolate series belonging to different tenants. For example, if you have two tenants &lt;code>team-A&lt;/code> and &lt;code>team-B&lt;/code>, &lt;code>team-A&lt;/code> series will be isolated from &lt;code>team-B&lt;/code>, and each team will be able to query only their own series.&lt;/p>
&lt;p>For more information, please refer to:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="/docs/api/#authentication">HTTP API authentication&lt;/a>&lt;/li>
&lt;li>&lt;a href="/docs/guides/limitations/#tenant-id-naming">Tenant ID limitations&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="time-series">Time series&lt;/h3>
&lt;p>&lt;em>See &lt;a href="#series">Series&lt;/a>.&lt;/em>&lt;/p>
&lt;h3 id="user">User&lt;/h3>
&lt;p>&lt;em>See &lt;a href="#tenant">Tenant&lt;/a>.&lt;/em>&lt;/p>
&lt;h3 id="wal">WAL&lt;/h3>
&lt;p>The Write-Ahead Log (WAL) is an append only log stored on disk used by ingesters to recover their in-memory state after the process gets restarted, either after a clear shutdown or an abruptly termination. Despite the implementation is different, the WAL is supported both by Cortex chunks and blocks storage engines.&lt;/p>
&lt;p>For more information, please refer to:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="/docs/chunks-storage/ingesters-with-wal/">Ingesters with WAL&lt;/a> when running &lt;strong>chunks storage&lt;/strong>.&lt;/li>
&lt;li>&lt;a href="/docs/blocks-storage/#the-write-path">Ingesters with WAL&lt;/a> when running &lt;strong>blocks storage&lt;/strong>.&lt;/li>
&lt;/ul></description></item></channel></rss>