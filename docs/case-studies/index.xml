<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cortex – Case Studies</title><link>/docs/case-studies/</link><description>Recent content in Case Studies on Cortex</description><generator>Hugo -- gohugo.io</generator><atom:link href="/docs/case-studies/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: How Gojek Is Leveraging Cortex to Keep Up with Its Ever-Growing Scale</title><link>/docs/case-studies/gojek/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/case-studies/gojek/</guid><description>
&lt;p>&lt;a href="https://www.gojek.com/">Gojek&lt;/a> launched in 2010 as a call center for booking motorcycle taxi rides in Indonesia. Today, the startup is a decacorn serving millions of users across Southeast Asia with its mobile wallet, GoPay, and 20+ products on its super app. Want to order dinner? Book a massage? Buy movie tickets? You can do all of that with the Gojek app.&lt;/p>
&lt;p>The company’s mission is to solve everyday challenges with technology innovation. To achieve that across multiple markets the systems team at Gojek focused on building an infrastructure for speed, reliability, and scale. By 2019, the team realized it needed a new monitoring system that could keep up with Gojek’s ever-growing technology organization, which led them to &lt;a href="https://github.com/cortexproject/cortex">Cortex&lt;/a>, the horizontally scalable &lt;a href="https://prometheus.io/">Prometheus&lt;/a> implementation.&lt;/p>
&lt;p>“We were using InfluxDB for metrics storage. Developers configured alerts by committing kapacitor scripts in git repos. To achieve high availability, we had a relay setup with two InfluxDBs. Since we could not horizontally scale Influx unless we paid for an enterprise license, we ended up having many InfluxDB clusters with relay setup,” says Product Engineer Ankit Goel.&lt;/p>
&lt;p>Though the team had introduced automation for setup, managing all those Influx instances became a pain point for operations. Additionally, some of the Gojek engineering teams needed far greater scale. “Some of our teams generate more than a million active time series,” says Goel. Another common requirement from customers was long-term storage of metrics. With InfluxDB, Gojek only had 2 weeks’ retention, and increasing it would mean provisioning bigger instances.&lt;/p>
&lt;p>Gojek was in search of a better monitoring solution that would meet the following requirements:&lt;/p>
&lt;ul>
&lt;li>Kubernetes native.&lt;/li>
&lt;li>Horizontally scalable.&lt;/li>
&lt;li>Highly available out of the box.&lt;/li>
&lt;li>High reliability.&lt;/li>
&lt;li>Low operations overhead so a small team can manage it.&lt;/li>
&lt;/ul>
&lt;p>Cortex met all of these requirements, and also had the following features that the Gojek team could leverage:&lt;/p>
&lt;ul>
&lt;li>Multi-tenancy.&lt;/li>
&lt;li>Customizable and modifiable, so it could be integrated with Gojek’s existing tooling.&lt;/li>
&lt;li>Support for remote_write.&lt;/li>
&lt;/ul>
&lt;p>Because it supports remote_write, Cortex enabled one of Gojek’s key needs: the ability to offer monitoring as a service. “With Thanos, we would have had to deploy a Thanos sidecar on every Prometheus that would have been deployed,” says Goel. “So essentially, there would be a substantial part of infrastructure on the client side that we would need to manage. We preferred Cortex because people could simply push their metrics to us, and we would have all the metrics in a single place.”&lt;/p>
&lt;p>The implementation started in January 2019. The team developed a few tools: a simple service for token-based authentication, and another for storing team information, such as notification channels and PagerDuty policies. Once all this was done, they leveraged &lt;a href="https://github.com/achilles42/telegraf/tree/prometheus-remote-write">InfluxData Telegraf’s remote_write plugin&lt;/a> to write to Cortex. This allowed them to have all the metrics being sent to InfluxDB to be sent to Cortex as well. “So moving from InfluxDB to tenants would not be that complicated. Because Cortex was multi-tenant, we could directly map each of our InfluxDB servers to our tenants,” says Goel. They’ve developed an internal helm chart to deploy and manage Cortex. After the customizations were completed in about two months, “we had our setup up and running, and we onboarded one team on Cortex,” he says.&lt;/p>
&lt;p>In the initial version, GitOps was the only interface for developers to apply alerts and create dashboards. The team built tools like &lt;a href="https://github.com/lahsivjar/grafonnet-playground">grafonnet-playground&lt;/a> to make it easy for developers to create dashboards. Developers are also allowed to create dashboards using the UI, since Grafana maintains version history for dashboards.&lt;/p>
&lt;p>&lt;img src="/images/case-studies/gojek-jsonnet-playground.png" alt="grafonnet-playground">&lt;/p>
&lt;p>“We needed metrics like ‘the number of alerts triggered for each team,’ ‘how long did it take to resolve these alerts,’ ‘how many were actionable and how many were ignored,’ etc.,” says Goel. “For measuring these metrics, the team only had to create a simple dashboard, since the ruler component exposes the per-tenant alert metrics. Both business and developers have found these metrics to be very useful.”&lt;/p>
&lt;p>&lt;img src="/images/case-studies/gojek-alerting-analytics.png" alt="alert analytics">&lt;/p>
&lt;p>The team built a CLI tool to improve user experience for applying alerts without having to dig into PromQL. “You can write a command and say &lt;code>lens attach alert&lt;/code>, and you tell it what kind of alert you want to attach, such as a CPU alert or Postgres alerts, and then you give it a service name,” says Goel. “There are some challenges to this approach for applying alerts, but we would like to move to such a model in the future.”&lt;/p>
&lt;p>One of the challenges the team faces is developer education. But “we always knew if we are going to move to either Thanos or Cortex, developers would have to learn PromQL,” Goel says. The monitoring team paired with developers to help them understand PromQL and migrate their graphs and alerts.&lt;/p>
&lt;p>The monitoring team has faced issues with Cortex from time to time, but “we always reached out to the Cortex community with our issues through the Cortex slack channel,” says Goel, and “active members of the Cortex community have always helped us with our problems.”&lt;/p>
&lt;p>Today, Gojek’s Lens monitoring system has 40+ tenants, for which Cortex handles about &lt;strong>1.2 million samples per second&lt;/strong>. Adoption is growing organically by word of mouth. Gojek is currently migrating to Kubernetes, and the teams that moved to Kubernetes have found Prometheus to be a better fit than InfluxDB. Seeing that success, other teams on Kubernetes have onboarded themselves to Lens.&lt;/p>
&lt;p>&lt;img src="/images/case-studies/gojek-throughput.png" alt="samples per second">&lt;/p>
&lt;p>Ultimately, Goel says, “where Cortex has really helped us is to integrate the monitoring system with our existing tools. We have a lot of internal tooling, and in certain places, we needed really tight integrations with the monitoring system. So the goal is to make sure that whenever a new service or team is created, they automatically get onboarded to the monitoring platform. After developers deploy, some of their system metrics and all the other standard metrics that are available for a service are automatically sent to the platform.” The team plans to spend the next six months bringing everyone over to Lens.&lt;/p>
&lt;p>Looking ahead, Goel and his team have the long-term vision of growing from a monitoring team to a full-fledged observability team. “We also want to take care of logging and tracing in Gojek,” he says. “&lt;a href="https://github.com/grafana/loki">Loki&lt;/a> would be easy to fit with Cortex, so in the future we want to explore Loki for logging.”&lt;/p></description></item><item><title>Docs: How Cortex helped REWE digital ensure stability while scaling services during the Covid-19 pandemic</title><link>/docs/case-studies/rewe-digital/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/case-studies/rewe-digital/</guid><description>
&lt;p>&lt;a href="https://www.rewe-digital.com/">REWE digital&lt;/a> builds the technology that drives the e-commerce, app, and food pickup and delivery services for one of Germany’s largest grocery chains, REWE. Like other companies involved in the food supply chain, REWE has seen demand spike during the Covid-19 pandemic. Thanks to its adoption of Cortex last year, the monitoring team has been able to ensure stability at growing scale.&lt;/p>
&lt;p>The REWE digital subsidiary was started in 2014 to advance its parent company’s digital transformation, so “we have a rather modern tech stack,” says Cloud Platform Engineer Martin Schneppenheim. A lot of the platform is run on Kubernetes using GKE while some is still running on-prem using Nomad. Still, there were some challenges that arose from the Spotify tribe model that the organization adopted. Each of the four tribes at REWE digital &amp;ndash; ECOM, FULFILLMENT, CONTENT and PLATFORM, and while the tribes mostly converged on the same technologies, he says, “each platform team has its own solution for, say, Kafka, for Prometheus, for Grafana.”&lt;/p>
&lt;p>Plus, over the past six years, the company has grown from 30 employees to around 600.&lt;/p>
&lt;p>With this rapid growth, they realized by the end of 2018 that they needed a new solution. The tipping point came when they experienced some out-of-memory issues with Prometheus, he says, “and we had no idea why.”&lt;/p>
&lt;p>Each tribe had one Prometheus HA pair that was used by all the teams in the tribe. One of the tribes used one Prometheus pair which required 30-60 gigabytes of RAM per instance. “We still saw some random out-of-memory kills, and we believe it was because some queries were loading too many samples,” Schneppenheim says. “We had several platform teams doing basically the same thing, and we wanted to tackle such issues organization-wide.”&lt;/p>
&lt;p>&lt;img src="/images/case-studies/rewe-workplace.jpg" alt="Grafana powered by Cortex">&lt;/p>
&lt;h3 id="searching-for-a-scalable-monitoring-solution">Searching for a scalable monitoring solution&lt;/h3>
&lt;p>The solution needed to support the Prometheus format, since all of REWE’s microservices had a Prometheus end point. And the team wanted to have trust in the project’s longevity. After considering M3, Victoria Metrics, and Thanos, the REWE digital team decided to go with Cortex. “Cortex had just been released as a CNCF project, and there were several developers from different companies,” he says. “That was another plus point for us.”
The key selling point, he adds, was Cortex’s multi-tenant support, which also involves the different protection mechanisms built into Cortex to limit a tenant’s usage so that a single tenant doesn’t affect the performance for other tenants. “Every platform team was providing one Prometheus for their tribe,” Schneppenheim says, “and we wanted to move to something like a software-as-a-service approach, with just one team that provides Cortex, which can be used by all the teams within the company.”&lt;/p>
&lt;h3 id="implementation">Implementation&lt;/h3>
&lt;p>Implementation began with the Big Data tribe, which has since merged with the other tribes and has been the smallest in the company. “We already had Prometheus set up, and we just switched the data source from Prometheus to Cortex,” Schneppenheim says. “In the beginning it was just one dashboard where we switched the data sources, and later on we switched the data source for the whole tribe so that all dashboards used the Cortex data source by default, and the Prometheus deployment basically acted as remote writing Prometheus. We always had the chance to just switch back to the Prometheus, in case there were any failures, so there was not a big risk.”&lt;/p>
&lt;p>In fact, things went smoothly, and a few months later, the ECOM tribe started writing metrics to Cortex. At the same time, the platform tribe decided to create one Grafana instance and use organizations to offer multi-tenancy. After that second migration, the tribe’s teams were able to migrate dashboards to the new Grafana instance, and then start querying against that data. By the end of the year, all the tribes will have migrated to Cortex and the Grafana instance.&lt;/p>
&lt;p>REWE digital adopted Cortex at “a very early stage,” Schneppenheim says. At first, “sometimes we had to read the code, because there was little documentation, but we were still confident that we took the right decision, because we got lots of support [from the community] in debugging some problems, which were usually misconfigurations.”&lt;/p>
&lt;p>He points out that configuration has become simpler over the past year, with default values set in v1.0, and more documentation: “Things definitely became better.”&lt;/p>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>Cortex’s horizontal scaling has proven to be crucial during the Covid-19 pandemic, when REWE’s grocery and food delivery services have seen extremely high demand. “Our primary focus was to ensure stability, so we had to scale, and we deployed more containers,” he says. “That meant we had way more metrics than before, on the one hand, and on the other hand, I believe our developers were watching our dashboards more closely, so we had way more queries as well.”&lt;/p>
&lt;p>Schneppenheim estimates that over the past two months, reads and writes have increased significantly, and the platform was able to handle the added load. Plus, “it was quite easy deploying another set of queriers,” he says.&lt;/p>
&lt;p>Aside from that, Schneppenheim says, “the biggest advantage for our company is that we now have a team that can offer one thing as an internal service.” While the tribes’ ops teams still have to manage their own Prometheus servers, they have a much more stable and scalable system. The challenges are unpredictable resource usage on querying and some queries that can load too much data causing Prometheus to OOM, but with Cortex handling all the queries, this is no longer a problem. And while Schneppenheim’s team is still just two people (&lt;a href="https://www.rewe-digital.com/en/job-overview.html#categories=34">they’re hiring!&lt;/a>), he adds, “we can spend more time actually learning how to run it, and become an expert within the company for Cortex and the things that come along with it, like high cardinality metric series, which we see every two weeks or so. We are the contact for all the monitoring now.”&lt;/p>
&lt;p>There have been other technical advantages, too: “We have no gaps anymore in our Prometheus and Grafana,” he says. “In case a Prometheus instance fails or if it needs to be restarted, we automatically switch over to the replica with the HA tracker, which is a great thing.”&lt;/p>
&lt;p>&lt;img src="/images/case-studies/rewe-cortex-reads.png" alt="Cortex Reads">&lt;/p>
&lt;p>With Cortex’s query results cache, the queries are cached. The REWE digital team has found that this feature makes dashboards “super fast, because the query is likely already cached and it just has to load the new 30 seconds or so, since the last refresh,” says Schneppenheim. “Preloaded dashboards load or refresh really, really fast.”&lt;/p>
&lt;p>Plus, there is a higher retention with Cortex. “We now have 60 days’ retention; we used to have seven days only,” he says.&lt;/p>
&lt;p>The benefits are also clear as the infrastructure grows. REWE digital has added a few more small Kubernetes clusters, which “obviously have the same monitoring/alerting needs as our biggest clusters,” he says. Previously, the team would have to deploy Prometheus and a separate Grafana instance (along with NGINX and DNS setup).&lt;/p>
&lt;p>“With our new SaaS approach, making monitoring available for these is as easy as adding a Prometheus pair, which sends metrics to our Cortex cluster, and adding this new tenant in our Grafana organization,” he says.&lt;/p>
&lt;p>With Cortex, they’ve also been able to solve two use cases (Kubernetes clusters that had been split for technical reasons, and cloud migration) that required metrics from two different clusters to be available with the same tenant. “The dev teams had the need to aggregate metrics across these two clusters, which was easily possible, because we just ingested them under the same tenant ID,” says Schneppenheim.&lt;/p>
&lt;p>And those out-of-memory issues? “We are constantly growing, not only on the query side, but also on the ingesting metrics side as we onboard teams and tribes,” he says. “But we have fine-tuned it quite well, and there are not as many OOM kills, and if there are, we don&amp;rsquo;t see them in Grafana. That&amp;rsquo;s important to us, that our developers have a smooth experience.” (Most tribes use Grafana alerting; one uses Prometheus Alertmanager.)&lt;/p>
&lt;h3 id="looking-ahead">Looking ahead&lt;/h3>
&lt;p>REWE digital’s main focus right now is to onboard the rest of the teams to Cortex. But looking ahead, the team is exploring &lt;a href="https://grafana.com/blog/2020/03/18/introducing-grafana-cloud-agent-a-remote_write-focused-prometheus-agent-that-can-save-40-on-memory-usage/">Grafana Cloud Agent&lt;/a> for the tribes that aren’t using Prometheus Alertmanager. “They don&amp;rsquo;t need Prometheus; we only use Prometheus to scrape the targets and send samples to our Cortex,” he says, “so that could definitely be interesting, especially given the performance improvements. Its sole purpose is to send metrics to Cortex as our remote-write backend, so maybe there will be other advantages in the future, like a more close monitoring.”&lt;/p>
&lt;p>The in-development Cortex Blocks storage engine is also interesting to the team as a solution for the bottleneck it has around a small BigTable cluster. “We just run three BigTable nodes, and the BigTable read latency sometimes peaks at one second, which is also the upper bucket limit in the histogram,” he says. “This happens if users open Grafana dashboards querying a long time range with many panels. Our hope is that switching from BigTable to the new storage engine would fix this as the object store (GCS) scales on-demand.”&lt;/p>
&lt;p>The REWE digital team has built and open sourced its own &lt;a href="https://github.com/rewe-digital/cortex-gateway">Cortex gateway&lt;/a>, which is on the project roadmap. “This might be a chance for us to contribute,” says Schneppenheim.&lt;/p>
&lt;p>Schneppenheim is also hopeful that the positive results REWE digital has seen with Cortex may lead to its further adoption throughout the greater REWE Group organization. “We&amp;rsquo;re just a small company within the REWE group,” he says, but “we might offer it as internal software as a service for other parts of the Group. They can trust our solution.”&lt;/p></description></item><item><title>Docs: Buoyant Cloud and Cortex: Standing on the shoulders of Linkerd and Prometheus</title><link>/docs/case-studies/buoyant-cloud/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/case-studies/buoyant-cloud/</guid><description>
&lt;p>&lt;a href="https://buoyant.io">Buoyant&lt;/a>, the creator of &lt;a href="https://linkerd.io">Linkerd&lt;/a>, has had a close relationship with Prometheus and related technologies for years. As of today, that relationship now includes &lt;a href="https://cortexmetrics.io/">Cortex&lt;/a>.&lt;/p>
&lt;p>Linkerd is an open source service mesh for Kubernetes, and part of the CNCF, along with Prometheus and Cortex. Linkerd provides three key classes of features: observability, reliability, and security—all without requiring any changes to your code. That first pillar, observability, has motivated deep Prometheus integration, and is the focus of this case study.&lt;/p>
&lt;h2 id="linkerd-x-prometheus">Linkerd x Prometheus&lt;/h2>
&lt;p>In 2016, Linkerd 1.0 shipped with a &lt;a href="https://linkerd.io/2016/10/04/a-service-mesh-for-kubernetes-part-i-top-line-service-metrics/#step-3-install-linkerd-viz">visualization module&lt;/a> built with Prometheus and Grafana. With the release of &lt;a href="https://linkerd.io/2018/09/18/announcing-linkerd-2-0/">Linkerd 2.0 in 2018&lt;/a>, Prometheus and Grafana were promoted to first-class components. That evolution was documented in a blog post entitled &lt;a href="https://linkerd.io/2018/05/17/prometheus-the-right-way-lessons-learned-evolving-conduits-prometheus-integration/">Prometheus the Right Way&lt;/a>, and a KubeCon North America 2018 talk entitled &lt;a href="https://www.youtube.com/watch?v=bnDWApsH36Y">Linkerd 2.0, Now with Extra Prometheus&lt;/a>, both co-authored by &lt;a href="https://twitter.com/fredbrancz">Frederic Branczyk&lt;/a> of Polar Signals and &lt;a href="https://twitter.com/siggy">Andrew Seigner&lt;/a> of Buoyant.&lt;/p>
&lt;p>This deep integration in Linkerd 2.0 provided out-of-the-box golden metrics, that is, success rate, request rate, and latency, across all your Kubernetes workloads.&lt;/p>
&lt;p>&lt;img src="/images/case-studies/bcloud-linkerd-grafana.png" alt="Linkerd Grafana">
&lt;em>Linkerd 2.0 with integrated Prometheus and Grafana, out-of-the-box golden metrics&lt;/em>&lt;/p>
&lt;h2 id="buoyant-x-cortex">Buoyant x Cortex&lt;/h2>
&lt;p>Building on the success of Linkerd, Buoyant has created &lt;a href="https://buoyant.cloud/">Buoyant Cloud&lt;/a>, a global platform health dashboard for Kubernetes. Leveraging Linkerd&amp;rsquo;s integration with Prometheus, Buoyant Cloud rolls up Linkerd metrics across all Kubernetes clusters to provide global platform-wide observability, including advanced features such as cross-cluster Service Level Objectives for success rate and latency.&lt;/p>
&lt;p>&lt;img src="/images/case-studies/bcloud-screenshot.jpg" alt="Buoyant Cloud Screenshot">
&lt;em>Service Level Objectives in Buoyant Cloud&lt;/em>&lt;/p>
&lt;h3 id="buoyant-cloud-prototyping-with-prometheus">Buoyant Cloud Prototyping with Prometheus&lt;/h3>
&lt;p>To enable observability in Buoyant Cloud, customers install an agent into their Kubernetes clusters. That agent gathers metrics from Linkerd and sends them up to Buoyant Cloud. Early prototypes of Buoyant Cloud received these metrics, wrote them to &lt;a href="https://github.com/prometheus/pushgateway">pushgateway&lt;/a>, and then our own larger Prometheus would scrape metrics from there. While this had the nice property that Linkerd metrics looked the same in Buoyant Cloud as they did in individual Linkerd clusters, gathering metrics across all customers into a single Prometheus instance created an inherent scaling limitation, and a single-point-of-failure.&lt;/p>
&lt;h3 id="observability-requirements">Observability Requirements&lt;/h3>
&lt;p>Thinking beyond our early prototypes, we came up with four core requirements for our observability system:&lt;/p>
&lt;ol>
&lt;li>Scalable - To support all Kubernetes clusters across our growing customer base, we needed a system that could scale as-needed. Bonus points if we could scale our reads and writes independently.&lt;/li>
&lt;li>Reliable - Users of Linkerd expect nothing less.&lt;/li>
&lt;li>Multi-tenant - To provide an extra layer of security and performance isolation between customers.&lt;/li>
&lt;li>Prometheus-compatible - To give our customers a familiar API, and to allow a drop-in replacement for our prototype Prometheus instance.&lt;/li>
&lt;/ol>
&lt;p>Those first two requirements in particular are not compatible with a single instance of anything. Even Prometheus, which does exceptionally well as a single instance, is intentionally not designed as a distributed, scalable system.&lt;/p>
&lt;p>Take a look at those requirements again, then go to the &lt;a href="https://cortexmetrics.io/">Cortex website&lt;/a>:&lt;/p>
&lt;p>&lt;img src="/images/case-studies/bcloud-cortex-homepage.png" alt="Cortex Homepage">
&lt;em>&lt;a href="https://cortexmetrics.io/">https://cortexmetrics.io/&lt;/a>&lt;/em>&lt;/p>
&lt;p>This was not intentional, but it was a fortuitous coincidence. Cortex&amp;rsquo;s one-line description of itself literally satisfied all four of our requirements for Buoyant Cloud.&lt;/p>
&lt;h3 id="buoyant-cloud-production-ready-with-cortex">Buoyant Cloud Production-Ready with Cortex&lt;/h3>
&lt;p>We set about building a proof of concept to validate whether Cortex could be a viable replacement for Buoyant Cloud&amp;rsquo;s prototype Prometheus-based observability.&lt;/p>
&lt;h4 id="development-environment">Development environment&lt;/h4>
&lt;p>The very first attempt at Cortex integration in our development environment was surprisingly smooth, thanks largely to Cortex&amp;rsquo;s &lt;a href="https://cortexmetrics.io/docs/getting-started/getting-started-blocks-storage/#single-instance-single-process">single instance, single process&lt;/a> mode. This enabled our developers to operate all of Cortex&amp;rsquo;s services as a single process in our docker-compose development environment, with no dependencies.&lt;/p>
&lt;h4 id="almost-a-drop-in-replacement">(Almost) a drop-in replacement&lt;/h4>
&lt;p>Cortex provides a Prometheus-compatible API. To enable multi-tenancy, you must set a &lt;a href="https://cortexmetrics.io/docs/guides/auth/">X-Scope-OrgID&lt;/a> header on every request with a unique identifier for the tenant. We already had a unique public identifier for each Buoyant Cloud customer, so that was a natural fit for this header. Modifying all reads and writes to Prometheus to set this header was a relatively straightforward code change, considering we were about to completely swap out a back-end storage system.&lt;/p>
&lt;h4 id="blocks-storage">Blocks Storage&lt;/h4>
&lt;p>Being the creators of Linkerd, we care a lot about operational complexity. Thus, it was no surprise that we wanted to minimize operational complexity and cost for ourselves in maintaining Buoyant Cloud. Cortex&amp;rsquo;s &lt;a href="https://cortexmetrics.io/docs/getting-started/getting-started-chunks-storage/">Chunk Storage&lt;/a> was a concern here. Operating our own Cassandra cluster would incur developer time, and paying for DynamoDB, BigTable, or Azure Cosmos DB would incur cost. Cortex&amp;rsquo;s &lt;a href="https://cortexmetrics.io/docs/blocks-storage/">Blocks Storage&lt;/a> removed this complexity and cost by relying on a simple and cheap object store, such as S3, GCS, or Azure Storage. At the time, Blocks Storage was still marked experimental. That&amp;rsquo;s when we hopped into a Q&amp;amp;A at a KubeCon EU 2020 talk, &lt;a href="https://www.youtube.com/watch?v=Z5OJzRogAS4">Scaling Prometheus: How We Got Some Thanos Into Cortex&lt;/a> by &lt;a href="https://twitter.com/thor4hansen">Thor Hansen&lt;/a> of HashiCorp and &lt;a href="https://twitter.com/pracucci">Marco Pracucci&lt;/a> of Grafana Labs. We asked the Cortex maintainers how close to General Availability they felt Blocks Storage was. While wisely not guaranteeing a date, they hinted it may be ready by the next Cortex release (and that hint turned out to be true, Blocks Storage was marked stable in &lt;a href="https://github.com/cortexproject/cortex/releases/tag/v1.4.0">Cortex 1.4.0&lt;/a>!). This gave us enough confidence to build our proof of concept around Blocks Storage.&lt;/p>
&lt;h4 id="recording-rules">Recording Rules&lt;/h4>
&lt;p>Buoyant Cloud uses recording rules for important success rate and latency metrics. One unexpected challenge was that Cortex&amp;rsquo;s multi-tenancy applies to recording rules. This meant that for every customer, we needed to push an identical set of recording rules to the &lt;a href="https://cortexmetrics.io/docs/api/#ruler">Cortex Ruler API&lt;/a>. We ended up writing a small background process that continually checks our database for new customers, and pushes recording rules when they appear.&lt;/p>
&lt;h4 id="cortex-in-prod-writes-then-reads">Cortex in Prod: Writes, then reads&lt;/h4>
&lt;p>Once all the pieces were in place, we enabled production writes to our new Cortex cluster. We continued writing to and reading from our existing Prometheus. Writing to Cortex and Prometheus simultaneously enabled validation of three things. We could 1) Evaluate Cortex&amp;rsquo;s read performance under full production write load, 2) ensure Cortex query results matched Prometheus, and 3) gather enough historical metrics in Cortex to minimize data loss for our customers.&lt;/p>
&lt;p>When we were satisfied with all three of these validation steps, we switched all reads to Cortex. This was the smallest code change in the entire migration. We simply swapped out a few command-line flags pointing to &lt;code>prometheus:9009&lt;/code> with &lt;code>query-frontend.cortex.svc.cluster.local:9009/api/prom&lt;/code>. Boom! It all worked!&lt;/p>
&lt;p>&lt;img src="/images/case-studies/bcloud-cortex-launch.png" alt="Cortex Launch">
&lt;em>The moment we turned on Cortex reads in production (numbers do not reflect full production load).&lt;/em>&lt;/p>
&lt;h2 id="looking-ahead">Looking Ahead&lt;/h2>
&lt;p>Our mission with Buoyant Cloud is to enable our customers to build observable, scalable, and secure Kubernetes platforms, without the complexity of stitching together lots of components.&lt;/p>
&lt;p>Users of Linkerd expect their observability to &amp;ldquo;just work&amp;rdquo;, and expect it to be something they are already familiar with and have tools to integrate with. For Linkerd running on a single Kubernetes cluster, Prometheus fits the bill perfectly. For Buoyant Cloud, we believe Cortex can deliver that same familiarity and integration story for all Kubernetes clusters across all customers.&lt;/p></description></item></channel></rss>